Using device: cpu

Iteration 1/70
Collected 100 trajectories. Average reward: 20.53
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.3056
Reward model training epoch 2/5, Loss: 0.2739
Reward model training epoch 3/5, Loss: 0.2473
Reward model training epoch 4/5, Loss: 0.2209
Reward model training epoch 5/5, Loss: 0.2049
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 0.9618
Policy update: Rollout 10/100, Loss: 0.0826
Policy update: Rollout 15/100, Loss: 0.0221
Policy update: Rollout 20/100, Loss: -0.0320
Policy update: Rollout 25/100, Loss: 0.2192
Policy update: Rollout 30/100, Loss: -0.3143
Policy update: Rollout 35/100, Loss: -0.0011
Policy update: Rollout 40/100, Loss: 0.0302
Policy update: Rollout 45/100, Loss: -0.0037
Policy update: Rollout 50/100, Loss: 0.3412
Policy update: Rollout 55/100, Loss: 0.5410
Policy update: Rollout 60/100, Loss: 0.1799
Policy update: Rollout 65/100, Loss: 0.2217
Policy update: Rollout 70/100, Loss: -0.0882
Policy update: Rollout 75/100, Loss: 0.1181
Policy update: Rollout 80/100, Loss: -0.0998
Policy update: Rollout 85/100, Loss: 0.8949
Policy update: Rollout 90/100, Loss: -0.1014
Policy update: Rollout 95/100, Loss: 0.0010
Policy update: Rollout 100/100, Loss: -0.1054
Evaluation: Average reward = 21.20

Iteration 2/70
Collected 100 trajectories. Average reward: 21.56
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.2137
Reward model training epoch 2/5, Loss: 0.2086
Reward model training epoch 3/5, Loss: 0.2057
Reward model training epoch 4/5, Loss: 0.2044
Reward model training epoch 5/5, Loss: 0.2036
Policy update: Rollout 5/100, Loss: -0.1323
Policy update: Rollout 10/100, Loss: 0.4026
Policy update: Rollout 15/100, Loss: -0.0136
Policy update: Rollout 20/100, Loss: -0.1543
Policy update: Rollout 25/100, Loss: -0.4296
Policy update: Rollout 30/100, Loss: 0.0400
Policy update: Rollout 35/100, Loss: 0.0279
Policy update: Rollout 40/100, Loss: -0.0542
Policy update: Rollout 45/100, Loss: 0.0069
Policy update: Rollout 50/100, Loss: -0.0640
Policy update: Rollout 55/100, Loss: -0.0862
Policy update: Rollout 60/100, Loss: -0.0608
Policy update: Rollout 65/100, Loss: 0.1608
Policy update: Rollout 70/100, Loss: -0.0524
Policy update: Rollout 75/100, Loss: -0.0700
Policy update: Rollout 80/100, Loss: -0.1785
Policy update: Rollout 85/100, Loss: -0.1034
Policy update: Rollout 90/100, Loss: 0.4482
Policy update: Rollout 95/100, Loss: -0.1813
Policy update: Rollout 100/100, Loss: -0.0178
Evaluation: Average reward = 18.40

Iteration 3/70
Collected 100 trajectories. Average reward: 23.17
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1721
Reward model training epoch 2/5, Loss: 0.1712
Reward model training epoch 3/5, Loss: 0.1705
Reward model training epoch 4/5, Loss: 0.1701
Reward model training epoch 5/5, Loss: 0.1699
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -0.0248
Policy update: Rollout 10/100, Loss: -0.0164
Policy update: Rollout 15/100, Loss: 0.2886
Policy update: Rollout 20/100, Loss: 0.1203
Policy update: Rollout 25/100, Loss: 0.0807
Policy update: Rollout 30/100, Loss: -0.0229
Policy update: Rollout 35/100, Loss: 0.0879
Policy update: Rollout 40/100, Loss: 0.2011
Policy update: Rollout 45/100, Loss: -0.7100
Policy update: Rollout 50/100, Loss: -0.0784
Policy update: Rollout 55/100, Loss: -0.2389
Policy update: Rollout 60/100, Loss: -0.0676
Policy update: Rollout 65/100, Loss: -0.1037
Policy update: Rollout 70/100, Loss: -0.1099
Policy update: Rollout 75/100, Loss: -0.2443
Policy update: Rollout 80/100, Loss: -0.1085
Policy update: Rollout 85/100, Loss: -0.0342
Policy update: Rollout 90/100, Loss: 0.1488
Policy update: Rollout 95/100, Loss: -0.0757
Policy update: Rollout 100/100, Loss: 0.0423
Evaluation: Average reward = 25.60

Iteration 4/70
Collected 100 trajectories. Average reward: 24.38
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1745
Reward model training epoch 2/5, Loss: 0.1744
Reward model training epoch 3/5, Loss: 0.1743
Reward model training epoch 4/5, Loss: 0.1743
Reward model training epoch 5/5, Loss: 0.1742
Policy update: Rollout 5/100, Loss: -0.0458
Policy update: Rollout 10/100, Loss: -0.1179
Policy update: Rollout 15/100, Loss: -0.0794
Policy update: Rollout 20/100, Loss: 0.0198
Policy update: Rollout 25/100, Loss: 0.2640
Policy update: Rollout 30/100, Loss: -0.0654
Policy update: Rollout 35/100, Loss: 0.0561
Policy update: Rollout 40/100, Loss: -0.0729
Policy update: Rollout 45/100, Loss: -0.6756
Policy update: Rollout 50/100, Loss: -0.6935
Policy update: Rollout 55/100, Loss: -0.1765
Policy update: Rollout 60/100, Loss: 0.3105
Policy update: Rollout 65/100, Loss: -0.0901
Policy update: Rollout 70/100, Loss: -0.2186
Policy update: Rollout 75/100, Loss: -0.3535
Policy update: Rollout 80/100, Loss: -0.0807
Policy update: Rollout 85/100, Loss: 0.5045
Policy update: Rollout 90/100, Loss: -0.1105
Policy update: Rollout 95/100, Loss: -0.1039
Policy update: Rollout 100/100, Loss: 0.0160
Evaluation: Average reward = 21.20

Iteration 5/70
Collected 100 trajectories. Average reward: 23.69
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1739
Reward model training epoch 2/5, Loss: 0.1739
Reward model training epoch 3/5, Loss: 0.1738
Reward model training epoch 4/5, Loss: 0.1738
Reward model training epoch 5/5, Loss: 0.1738
Preference prediction accuracy: 0.97
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 0.1576
Policy update: Rollout 10/100, Loss: -0.2135
Policy update: Rollout 15/100, Loss: -0.1730
Policy update: Rollout 20/100, Loss: -0.0847
Policy update: Rollout 25/100, Loss: -0.1932
Policy update: Rollout 30/100, Loss: 0.1010
Policy update: Rollout 35/100, Loss: -0.4921
Policy update: Rollout 40/100, Loss: -0.3372
Policy update: Rollout 45/100, Loss: 0.2862
Policy update: Rollout 50/100, Loss: -0.3935
Policy update: Rollout 55/100, Loss: -0.3806
Policy update: Rollout 60/100, Loss: -0.6222
Policy update: Rollout 65/100, Loss: 0.6585
Policy update: Rollout 70/100, Loss: -0.3582
Policy update: Rollout 75/100, Loss: 0.2499
Policy update: Rollout 80/100, Loss: -0.0071
Policy update: Rollout 85/100, Loss: -0.0614
Policy update: Rollout 90/100, Loss: -0.2748
Policy update: Rollout 95/100, Loss: 0.6738
Policy update: Rollout 100/100, Loss: -0.0226
Evaluation: Average reward = 23.80

Iteration 6/70
Collected 100 trajectories. Average reward: 23.99
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1687
Reward model training epoch 2/5, Loss: 0.1687
Reward model training epoch 3/5, Loss: 0.1687
Reward model training epoch 4/5, Loss: 0.1687
Reward model training epoch 5/5, Loss: 0.1686
Policy update: Rollout 5/100, Loss: -0.0926
Policy update: Rollout 10/100, Loss: -0.3390
Policy update: Rollout 15/100, Loss: -0.7477
Policy update: Rollout 20/100, Loss: 0.2883
Policy update: Rollout 25/100, Loss: -0.2783
Policy update: Rollout 30/100, Loss: -0.0746
Policy update: Rollout 35/100, Loss: 0.5404
Policy update: Rollout 40/100, Loss: -0.5694
Policy update: Rollout 45/100, Loss: -0.7543
Policy update: Rollout 50/100, Loss: 0.1099
Policy update: Rollout 55/100, Loss: 0.0564
Policy update: Rollout 60/100, Loss: -0.4435
Policy update: Rollout 65/100, Loss: -0.1931
Policy update: Rollout 70/100, Loss: -0.5737
Policy update: Rollout 75/100, Loss: -2.0730
Policy update: Rollout 80/100, Loss: -0.4872
Policy update: Rollout 85/100, Loss: 0.1892
Policy update: Rollout 90/100, Loss: -0.3468
Policy update: Rollout 95/100, Loss: 0.1400
Policy update: Rollout 100/100, Loss: 0.8878
Evaluation: Average reward = 37.60

Iteration 7/70
Collected 100 trajectories. Average reward: 26.83
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1490
Reward model training epoch 2/5, Loss: 0.1490
Reward model training epoch 3/5, Loss: 0.1489
Reward model training epoch 4/5, Loss: 0.1489
Reward model training epoch 5/5, Loss: 0.1489
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -0.3515
Policy update: Rollout 10/100, Loss: -0.4617
Policy update: Rollout 15/100, Loss: 0.0732
Policy update: Rollout 20/100, Loss: -0.2029
Policy update: Rollout 25/100, Loss: -0.2739
Policy update: Rollout 30/100, Loss: -0.0724
Policy update: Rollout 35/100, Loss: 0.1998
Policy update: Rollout 40/100, Loss: -0.3966
Policy update: Rollout 45/100, Loss: -0.6878
Policy update: Rollout 50/100, Loss: -0.5503
Policy update: Rollout 55/100, Loss: -0.1511
Policy update: Rollout 60/100, Loss: -0.4565
Policy update: Rollout 65/100, Loss: -0.4319
Policy update: Rollout 70/100, Loss: 0.3291
Policy update: Rollout 75/100, Loss: -0.0448
Policy update: Rollout 80/100, Loss: 0.0608
Policy update: Rollout 85/100, Loss: 0.1651
Policy update: Rollout 90/100, Loss: -0.4666
Policy update: Rollout 95/100, Loss: -1.6365
Policy update: Rollout 100/100, Loss: -0.0946
Evaluation: Average reward = 15.00

Iteration 8/70
Collected 100 trajectories. Average reward: 28.37
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1285
Reward model training epoch 2/5, Loss: 0.1285
Reward model training epoch 3/5, Loss: 0.1285
Reward model training epoch 4/5, Loss: 0.1285
Reward model training epoch 5/5, Loss: 0.1285
Policy update: Rollout 5/100, Loss: -0.6240
Policy update: Rollout 10/100, Loss: 0.1530
Policy update: Rollout 15/100, Loss: -0.0542
Policy update: Rollout 20/100, Loss: 0.5079
Policy update: Rollout 25/100, Loss: -0.1126
Policy update: Rollout 30/100, Loss: 0.0573
Policy update: Rollout 35/100, Loss: 0.2303
Policy update: Rollout 40/100, Loss: -1.4450
Policy update: Rollout 45/100, Loss: -0.2158
Policy update: Rollout 50/100, Loss: -0.2209
Policy update: Rollout 55/100, Loss: -0.2988
Policy update: Rollout 60/100, Loss: -0.6364
Policy update: Rollout 65/100, Loss: -0.2099
Policy update: Rollout 70/100, Loss: 0.4670
Policy update: Rollout 75/100, Loss: 0.2604
Policy update: Rollout 80/100, Loss: -0.4061
Policy update: Rollout 85/100, Loss: 0.2323
Policy update: Rollout 90/100, Loss: 0.2364
Policy update: Rollout 95/100, Loss: -0.3818
Policy update: Rollout 100/100, Loss: 0.2055
Evaluation: Average reward = 33.20

Iteration 9/70
Collected 100 trajectories. Average reward: 26.81
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1185
Reward model training epoch 2/5, Loss: 0.1185
Reward model training epoch 3/5, Loss: 0.1185
Reward model training epoch 4/5, Loss: 0.1185
Reward model training epoch 5/5, Loss: 0.1185
Preference prediction accuracy: 0.97
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 0.2718
Policy update: Rollout 10/100, Loss: -0.1938
Policy update: Rollout 15/100, Loss: -1.3611
Policy update: Rollout 20/100, Loss: -0.4965
Policy update: Rollout 25/100, Loss: 0.0938
Policy update: Rollout 30/100, Loss: 0.3297
Policy update: Rollout 35/100, Loss: 0.5733
Policy update: Rollout 40/100, Loss: -1.5978
Policy update: Rollout 45/100, Loss: -0.6539
Policy update: Rollout 50/100, Loss: -0.0927
Policy update: Rollout 55/100, Loss: -0.5412
Policy update: Rollout 60/100, Loss: -0.1173
Policy update: Rollout 65/100, Loss: 0.2776
Policy update: Rollout 70/100, Loss: 0.7216
Policy update: Rollout 75/100, Loss: 0.5700
Policy update: Rollout 80/100, Loss: 0.5808
Policy update: Rollout 85/100, Loss: 0.2309
Policy update: Rollout 90/100, Loss: -0.2872
Policy update: Rollout 95/100, Loss: -1.6959
Policy update: Rollout 100/100, Loss: -0.3895
Evaluation: Average reward = 23.40

Iteration 10/70
Collected 100 trajectories. Average reward: 29.35
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1545
Reward model training epoch 2/5, Loss: 0.1545
Reward model training epoch 3/5, Loss: 0.1545
Reward model training epoch 4/5, Loss: 0.1545
Reward model training epoch 5/5, Loss: 0.1545
Policy update: Rollout 5/100, Loss: 0.2707
Policy update: Rollout 10/100, Loss: -0.3861
Policy update: Rollout 15/100, Loss: -0.1037
Policy update: Rollout 20/100, Loss: -2.0328
Policy update: Rollout 25/100, Loss: -0.5824
Policy update: Rollout 30/100, Loss: -0.1601
Policy update: Rollout 35/100, Loss: 0.1418
Policy update: Rollout 40/100, Loss: -1.1839
Policy update: Rollout 45/100, Loss: -1.2687
Policy update: Rollout 50/100, Loss: -0.4716
Policy update: Rollout 55/100, Loss: -1.3742
Policy update: Rollout 60/100, Loss: 0.4794
Policy update: Rollout 65/100, Loss: -0.6672
Policy update: Rollout 70/100, Loss: -0.8364
Policy update: Rollout 75/100, Loss: -0.5514
Policy update: Rollout 80/100, Loss: -0.9784
Policy update: Rollout 85/100, Loss: 0.5976
Policy update: Rollout 90/100, Loss: 0.6186
Policy update: Rollout 95/100, Loss: 0.0275
Policy update: Rollout 100/100, Loss: -1.3221
Evaluation: Average reward = 27.40

Iteration 11/70
Collected 100 trajectories. Average reward: 28.68
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1294
Reward model training epoch 2/5, Loss: 0.1294
Reward model training epoch 3/5, Loss: 0.1294
Reward model training epoch 4/5, Loss: 0.1294
Reward model training epoch 5/5, Loss: 0.1294
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -1.2708
Policy update: Rollout 10/100, Loss: -3.6204
Policy update: Rollout 15/100, Loss: 0.1551
Policy update: Rollout 20/100, Loss: -1.3197
Policy update: Rollout 25/100, Loss: 0.4070
Policy update: Rollout 30/100, Loss: -0.5696
Policy update: Rollout 35/100, Loss: 0.2092
Policy update: Rollout 40/100, Loss: -0.6487
Policy update: Rollout 45/100, Loss: 1.0022
Policy update: Rollout 50/100, Loss: 0.6170
Policy update: Rollout 55/100, Loss: 0.3966
Policy update: Rollout 60/100, Loss: -0.4058
Policy update: Rollout 65/100, Loss: -0.3916
Policy update: Rollout 70/100, Loss: -0.5121
Policy update: Rollout 75/100, Loss: 0.5325
Policy update: Rollout 80/100, Loss: -1.0395
Policy update: Rollout 85/100, Loss: 0.0434
Policy update: Rollout 90/100, Loss: -2.0624
Policy update: Rollout 95/100, Loss: -1.0059
Policy update: Rollout 100/100, Loss: -0.8862
Evaluation: Average reward = 38.40

Iteration 12/70
Collected 100 trajectories. Average reward: 38.21
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0911
Reward model training epoch 2/5, Loss: 0.0911
Reward model training epoch 3/5, Loss: 0.0911
Reward model training epoch 4/5, Loss: 0.0911
Reward model training epoch 5/5, Loss: 0.0911
Policy update: Rollout 5/100, Loss: -0.7275
Policy update: Rollout 10/100, Loss: 0.4535
Policy update: Rollout 15/100, Loss: 0.2711
Policy update: Rollout 20/100, Loss: -1.4898
Policy update: Rollout 25/100, Loss: -0.2756
Policy update: Rollout 30/100, Loss: -0.0861
Policy update: Rollout 35/100, Loss: -0.2089
Policy update: Rollout 40/100, Loss: -1.0990
Policy update: Rollout 45/100, Loss: -0.1771
Policy update: Rollout 50/100, Loss: -2.7222
Policy update: Rollout 55/100, Loss: 0.1108
Policy update: Rollout 60/100, Loss: 0.3175
Policy update: Rollout 65/100, Loss: 0.3360
Policy update: Rollout 70/100, Loss: -1.1564
Policy update: Rollout 75/100, Loss: 0.8869
Policy update: Rollout 80/100, Loss: -1.6183
Policy update: Rollout 85/100, Loss: 0.5019
Policy update: Rollout 90/100, Loss: -0.3244
Policy update: Rollout 95/100, Loss: -1.6422
Policy update: Rollout 100/100, Loss: -2.3538
Evaluation: Average reward = 25.40

Iteration 13/70
Collected 100 trajectories. Average reward: 32.24
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0980
Reward model training epoch 2/5, Loss: 0.0980
Reward model training epoch 3/5, Loss: 0.0980
Reward model training epoch 4/5, Loss: 0.0980
Reward model training epoch 5/5, Loss: 0.0980
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -0.6464
Policy update: Rollout 10/100, Loss: -1.8768
Policy update: Rollout 15/100, Loss: -2.0364
Policy update: Rollout 20/100, Loss: 0.1498
Policy update: Rollout 25/100, Loss: -4.3042
Policy update: Rollout 30/100, Loss: -1.9050
Policy update: Rollout 35/100, Loss: 1.7771
Policy update: Rollout 40/100, Loss: 0.8163
Policy update: Rollout 45/100, Loss: 0.1969
Policy update: Rollout 50/100, Loss: 0.3014
Policy update: Rollout 55/100, Loss: 0.3188
Policy update: Rollout 60/100, Loss: -0.7824
Policy update: Rollout 65/100, Loss: 0.4534
Policy update: Rollout 70/100, Loss: 0.0305
Policy update: Rollout 75/100, Loss: -1.8259
Policy update: Rollout 80/100, Loss: -0.2704
Policy update: Rollout 85/100, Loss: 0.4131
Policy update: Rollout 90/100, Loss: -0.8878
Policy update: Rollout 95/100, Loss: 1.9403
Policy update: Rollout 100/100, Loss: 0.3573
Evaluation: Average reward = 29.00

Iteration 14/70
Collected 100 trajectories. Average reward: 34.54
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1072
Reward model training epoch 2/5, Loss: 0.1072
Reward model training epoch 3/5, Loss: 0.1072
Reward model training epoch 4/5, Loss: 0.1072
Reward model training epoch 5/5, Loss: 0.1072
Policy update: Rollout 5/100, Loss: 0.9625
Policy update: Rollout 10/100, Loss: -0.6518
Policy update: Rollout 15/100, Loss: -1.3091
Policy update: Rollout 20/100, Loss: -0.2599
Policy update: Rollout 25/100, Loss: 0.2260
Policy update: Rollout 30/100, Loss: -0.2410
Policy update: Rollout 35/100, Loss: 0.0514
Policy update: Rollout 40/100, Loss: -2.0600
Policy update: Rollout 45/100, Loss: 0.8740
Policy update: Rollout 50/100, Loss: -1.4177
Policy update: Rollout 55/100, Loss: -0.1496
Policy update: Rollout 60/100, Loss: 0.2117
Policy update: Rollout 65/100, Loss: 1.4821
Policy update: Rollout 70/100, Loss: 0.5752
Policy update: Rollout 75/100, Loss: 0.7969
Policy update: Rollout 80/100, Loss: 1.0835
Policy update: Rollout 85/100, Loss: -1.1894
Policy update: Rollout 90/100, Loss: 0.1926
Policy update: Rollout 95/100, Loss: 0.7211
Policy update: Rollout 100/100, Loss: 1.1350
Evaluation: Average reward = 54.20

Iteration 15/70
Collected 100 trajectories. Average reward: 40.20
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0750
Reward model training epoch 2/5, Loss: 0.0750
Reward model training epoch 3/5, Loss: 0.0750
Reward model training epoch 4/5, Loss: 0.0750
Reward model training epoch 5/5, Loss: 0.0750
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -2.1690
Policy update: Rollout 10/100, Loss: 0.3965
Policy update: Rollout 15/100, Loss: -1.0755
Policy update: Rollout 20/100, Loss: 0.3139
Policy update: Rollout 25/100, Loss: 0.1556
Policy update: Rollout 30/100, Loss: -1.0081
Policy update: Rollout 35/100, Loss: -2.6498
Policy update: Rollout 40/100, Loss: -0.2579
Policy update: Rollout 45/100, Loss: -0.4423
Policy update: Rollout 50/100, Loss: -3.3393
Policy update: Rollout 55/100, Loss: -0.7516
Policy update: Rollout 60/100, Loss: -1.3259
Policy update: Rollout 65/100, Loss: 0.3006
Policy update: Rollout 70/100, Loss: -0.7260
Policy update: Rollout 75/100, Loss: 0.8857
Policy update: Rollout 80/100, Loss: 1.0749
Policy update: Rollout 85/100, Loss: 0.2741
Policy update: Rollout 90/100, Loss: 0.9257
Policy update: Rollout 95/100, Loss: 0.4648
Policy update: Rollout 100/100, Loss: -1.3442
Evaluation: Average reward = 27.60

Iteration 16/70
Collected 100 trajectories. Average reward: 38.60
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0752
Reward model training epoch 2/5, Loss: 0.0752
Reward model training epoch 3/5, Loss: 0.0752
Reward model training epoch 4/5, Loss: 0.0752
Reward model training epoch 5/5, Loss: 0.0752
Policy update: Rollout 5/100, Loss: -1.8590
Policy update: Rollout 10/100, Loss: -0.2301
Policy update: Rollout 15/100, Loss: -0.2953
Policy update: Rollout 20/100, Loss: 0.7634
Policy update: Rollout 25/100, Loss: 0.6926
Policy update: Rollout 30/100, Loss: 0.2810
Policy update: Rollout 35/100, Loss: 0.1423
Policy update: Rollout 40/100, Loss: -2.7126
Policy update: Rollout 45/100, Loss: -0.7816
Policy update: Rollout 50/100, Loss: -0.7194
Policy update: Rollout 55/100, Loss: -1.4873
Policy update: Rollout 60/100, Loss: -0.5153
Policy update: Rollout 65/100, Loss: -1.5990
Policy update: Rollout 70/100, Loss: 0.8639
Policy update: Rollout 75/100, Loss: -0.1467
Policy update: Rollout 80/100, Loss: 0.1149
Policy update: Rollout 85/100, Loss: -3.0160
Policy update: Rollout 90/100, Loss: -0.1533
Policy update: Rollout 95/100, Loss: 0.4808
Policy update: Rollout 100/100, Loss: 0.3468
Evaluation: Average reward = 43.00

Iteration 17/70
Collected 100 trajectories. Average reward: 43.50
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0843
Reward model training epoch 2/5, Loss: 0.0843
Reward model training epoch 3/5, Loss: 0.0843
Reward model training epoch 4/5, Loss: 0.0843
Reward model training epoch 5/5, Loss: 0.0843
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -2.3758
Policy update: Rollout 10/100, Loss: -0.2937
Policy update: Rollout 15/100, Loss: -0.8162
Policy update: Rollout 20/100, Loss: 0.6353
Policy update: Rollout 25/100, Loss: -1.4185
Policy update: Rollout 30/100, Loss: 0.6138
Policy update: Rollout 35/100, Loss: 0.7345
Policy update: Rollout 40/100, Loss: 0.5895
Policy update: Rollout 45/100, Loss: -1.0451
Policy update: Rollout 50/100, Loss: 0.8895
Policy update: Rollout 55/100, Loss: 0.0244
Policy update: Rollout 60/100, Loss: -0.1404
Policy update: Rollout 65/100, Loss: 0.6011
Policy update: Rollout 70/100, Loss: -0.0127
Policy update: Rollout 75/100, Loss: 0.2245
Policy update: Rollout 80/100, Loss: 0.6341
Policy update: Rollout 85/100, Loss: -0.2074
Policy update: Rollout 90/100, Loss: -0.5007
Policy update: Rollout 95/100, Loss: -1.1131
Policy update: Rollout 100/100, Loss: -4.2165
Evaluation: Average reward = 40.80

Iteration 18/70
Collected 100 trajectories. Average reward: 46.38
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0691
Reward model training epoch 2/5, Loss: 0.0691
Reward model training epoch 3/5, Loss: 0.0691
Reward model training epoch 4/5, Loss: 0.0691
Reward model training epoch 5/5, Loss: 0.0691
Policy update: Rollout 5/100, Loss: 0.6307
Policy update: Rollout 10/100, Loss: 0.4126
Policy update: Rollout 15/100, Loss: -0.0776
Policy update: Rollout 20/100, Loss: -0.8799
Policy update: Rollout 25/100, Loss: -1.6471
Policy update: Rollout 30/100, Loss: 1.1886
Policy update: Rollout 35/100, Loss: -0.8781
Policy update: Rollout 40/100, Loss: 1.1842
Policy update: Rollout 45/100, Loss: -2.7122
Policy update: Rollout 50/100, Loss: -5.0585
Policy update: Rollout 55/100, Loss: -1.3695
Policy update: Rollout 60/100, Loss: 0.8221
Policy update: Rollout 65/100, Loss: -0.2864
Policy update: Rollout 70/100, Loss: -0.2925
Policy update: Rollout 75/100, Loss: 0.5849
Policy update: Rollout 80/100, Loss: 0.6981
Policy update: Rollout 85/100, Loss: -0.3779
Policy update: Rollout 90/100, Loss: 0.3885
Policy update: Rollout 95/100, Loss: -0.0021
Policy update: Rollout 100/100, Loss: 0.1687
Evaluation: Average reward = 40.20

Iteration 19/70
Collected 100 trajectories. Average reward: 47.49
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0456
Reward model training epoch 2/5, Loss: 0.0456
Reward model training epoch 3/5, Loss: 0.0456
Reward model training epoch 4/5, Loss: 0.0456
Reward model training epoch 5/5, Loss: 0.0456
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: 2.0973
Policy update: Rollout 10/100, Loss: 1.3934
Policy update: Rollout 15/100, Loss: -0.4544
Policy update: Rollout 20/100, Loss: -0.1784
Policy update: Rollout 25/100, Loss: 0.2413
Policy update: Rollout 30/100, Loss: 0.2961
Policy update: Rollout 35/100, Loss: 0.4041
Policy update: Rollout 40/100, Loss: -2.3330
Policy update: Rollout 45/100, Loss: -0.2559
Policy update: Rollout 50/100, Loss: -2.1837
Policy update: Rollout 55/100, Loss: -1.2360
Policy update: Rollout 60/100, Loss: 1.1093
Policy update: Rollout 65/100, Loss: 0.5790
Policy update: Rollout 70/100, Loss: 0.1816
Policy update: Rollout 75/100, Loss: -0.6711
Policy update: Rollout 80/100, Loss: -0.6904
Policy update: Rollout 85/100, Loss: -1.9781
Policy update: Rollout 90/100, Loss: 0.1231
Policy update: Rollout 95/100, Loss: 0.3125
Policy update: Rollout 100/100, Loss: -4.2648
Evaluation: Average reward = 54.80

Iteration 20/70
Collected 100 trajectories. Average reward: 48.43
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0583
Reward model training epoch 2/5, Loss: 0.0583
Reward model training epoch 3/5, Loss: 0.0583
Reward model training epoch 4/5, Loss: 0.0583
Reward model training epoch 5/5, Loss: 0.0583
Policy update: Rollout 5/100, Loss: -9.5205
Policy update: Rollout 10/100, Loss: 0.7708
Policy update: Rollout 15/100, Loss: 2.0369
Policy update: Rollout 20/100, Loss: 0.0887
Policy update: Rollout 25/100, Loss: -1.8095
Policy update: Rollout 30/100, Loss: 0.0586
Policy update: Rollout 35/100, Loss: -0.0444
Policy update: Rollout 40/100, Loss: -1.1301
Policy update: Rollout 45/100, Loss: 0.8082
Policy update: Rollout 50/100, Loss: -1.3156
Policy update: Rollout 55/100, Loss: -1.6902
Policy update: Rollout 60/100, Loss: -1.6166
Policy update: Rollout 65/100, Loss: -0.6450
Policy update: Rollout 70/100, Loss: -1.1081
Policy update: Rollout 75/100, Loss: -0.6195
Policy update: Rollout 80/100, Loss: 0.8182
Policy update: Rollout 85/100, Loss: -2.7548
Policy update: Rollout 90/100, Loss: -0.7329
Policy update: Rollout 95/100, Loss: 0.1971
Policy update: Rollout 100/100, Loss: 0.0725
Evaluation: Average reward = 24.00

Iteration 21/70
Collected 100 trajectories. Average reward: 50.78
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0622
Reward model training epoch 2/5, Loss: 0.0622
Reward model training epoch 3/5, Loss: 0.0622
Reward model training epoch 4/5, Loss: 0.0621
Reward model training epoch 5/5, Loss: 0.0621
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 0.4253
Policy update: Rollout 10/100, Loss: -1.1026
Policy update: Rollout 15/100, Loss: 1.0628
Policy update: Rollout 20/100, Loss: -5.4964
Policy update: Rollout 25/100, Loss: 0.3807
Policy update: Rollout 30/100, Loss: 0.3946
Policy update: Rollout 35/100, Loss: 0.5941
Policy update: Rollout 40/100, Loss: -0.3017
Policy update: Rollout 45/100, Loss: 0.6631
Policy update: Rollout 50/100, Loss: 0.3406
Policy update: Rollout 55/100, Loss: 0.3962
Policy update: Rollout 60/100, Loss: -0.6474
Policy update: Rollout 65/100, Loss: 0.6409
Policy update: Rollout 70/100, Loss: -2.2717
Policy update: Rollout 75/100, Loss: -3.4139
Policy update: Rollout 80/100, Loss: -1.2155
Policy update: Rollout 85/100, Loss: 1.4220
Policy update: Rollout 90/100, Loss: -3.3673
Policy update: Rollout 95/100, Loss: 0.3498
Policy update: Rollout 100/100, Loss: 1.3658
Evaluation: Average reward = 77.00

Iteration 22/70
Collected 100 trajectories. Average reward: 55.90
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0480
Reward model training epoch 2/5, Loss: 0.0480
Reward model training epoch 3/5, Loss: 0.0480
Reward model training epoch 4/5, Loss: 0.0480
Reward model training epoch 5/5, Loss: 0.0480
Policy update: Rollout 5/100, Loss: 0.5248
Policy update: Rollout 10/100, Loss: -1.5953
Policy update: Rollout 15/100, Loss: 1.1491
Policy update: Rollout 20/100, Loss: 0.2354
Policy update: Rollout 25/100, Loss: -0.8309
Policy update: Rollout 30/100, Loss: 1.0798
Policy update: Rollout 35/100, Loss: -0.4603
Policy update: Rollout 40/100, Loss: -1.4927
Policy update: Rollout 45/100, Loss: -0.8788
Policy update: Rollout 50/100, Loss: -0.7703
Policy update: Rollout 55/100, Loss: -2.1664
Policy update: Rollout 60/100, Loss: 1.5761
Policy update: Rollout 65/100, Loss: 0.7409
Policy update: Rollout 70/100, Loss: 1.0775
Policy update: Rollout 75/100, Loss: -1.2007
Policy update: Rollout 80/100, Loss: -2.5493
Policy update: Rollout 85/100, Loss: 0.2901
Policy update: Rollout 90/100, Loss: -0.8101
Policy update: Rollout 95/100, Loss: 0.4757
Policy update: Rollout 100/100, Loss: -0.4137
Evaluation: Average reward = 53.40

Iteration 23/70
Collected 100 trajectories. Average reward: 55.41
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0356
Reward model training epoch 2/5, Loss: 0.0356
Reward model training epoch 3/5, Loss: 0.0356
Reward model training epoch 4/5, Loss: 0.0356
Reward model training epoch 5/5, Loss: 0.0356
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -0.9361
Policy update: Rollout 10/100, Loss: 1.3802
Policy update: Rollout 15/100, Loss: 0.4386
Policy update: Rollout 20/100, Loss: -1.9914
Policy update: Rollout 25/100, Loss: 1.4067
Policy update: Rollout 30/100, Loss: 0.0471
Policy update: Rollout 35/100, Loss: -0.6423
Policy update: Rollout 40/100, Loss: -0.6826
Policy update: Rollout 45/100, Loss: 0.3646
Policy update: Rollout 50/100, Loss: -3.2775
Policy update: Rollout 55/100, Loss: -0.7623
Policy update: Rollout 60/100, Loss: -0.8187
Policy update: Rollout 65/100, Loss: 0.8426
Policy update: Rollout 70/100, Loss: 1.2334
Policy update: Rollout 75/100, Loss: -3.1335
Policy update: Rollout 80/100, Loss: 0.7405
Policy update: Rollout 85/100, Loss: 0.7700
Policy update: Rollout 90/100, Loss: 1.0000
Policy update: Rollout 95/100, Loss: -0.1098
Policy update: Rollout 100/100, Loss: -6.0086
Evaluation: Average reward = 43.00

Iteration 24/70
Collected 100 trajectories. Average reward: 55.29
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0654
Reward model training epoch 2/5, Loss: 0.0654
Reward model training epoch 3/5, Loss: 0.0654
Reward model training epoch 4/5, Loss: 0.0654
Reward model training epoch 5/5, Loss: 0.0654
Policy update: Rollout 5/100, Loss: -2.8102
Policy update: Rollout 10/100, Loss: -2.3571
Policy update: Rollout 15/100, Loss: 0.5816
Policy update: Rollout 20/100, Loss: -2.8757
Policy update: Rollout 25/100, Loss: -1.9928
Policy update: Rollout 30/100, Loss: -1.2091
Policy update: Rollout 35/100, Loss: -0.5107
Policy update: Rollout 40/100, Loss: -5.4435
Policy update: Rollout 45/100, Loss: -2.0918
Policy update: Rollout 50/100, Loss: 0.5853
Policy update: Rollout 55/100, Loss: -3.1672
Policy update: Rollout 60/100, Loss: -6.6935
Policy update: Rollout 65/100, Loss: 1.6371
Policy update: Rollout 70/100, Loss: 0.7951
Policy update: Rollout 75/100, Loss: 1.2501
Policy update: Rollout 80/100, Loss: -0.0984
Policy update: Rollout 85/100, Loss: -0.7859
Policy update: Rollout 90/100, Loss: -4.2064
Policy update: Rollout 95/100, Loss: -0.3836
Policy update: Rollout 100/100, Loss: -1.9772
Evaluation: Average reward = 63.40

Iteration 25/70
Collected 100 trajectories. Average reward: 63.84
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0542
Reward model training epoch 2/5, Loss: 0.0542
Reward model training epoch 3/5, Loss: 0.0542
Reward model training epoch 4/5, Loss: 0.0542
Reward model training epoch 5/5, Loss: 0.0542
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -1.8882
Policy update: Rollout 10/100, Loss: -0.5666
Policy update: Rollout 15/100, Loss: 0.2940
Policy update: Rollout 20/100, Loss: -2.1945
Policy update: Rollout 25/100, Loss: -6.1244
Policy update: Rollout 30/100, Loss: 0.5323
Policy update: Rollout 35/100, Loss: -0.4696
Policy update: Rollout 40/100, Loss: -0.1528
Policy update: Rollout 45/100, Loss: -1.0430
Policy update: Rollout 50/100, Loss: -0.0742
Policy update: Rollout 55/100, Loss: -1.1886
Policy update: Rollout 60/100, Loss: 0.3268
Policy update: Rollout 65/100, Loss: 0.7621
Policy update: Rollout 70/100, Loss: -1.2730
Policy update: Rollout 75/100, Loss: -1.8447
Policy update: Rollout 80/100, Loss: 1.4853
Policy update: Rollout 85/100, Loss: -2.3214
Policy update: Rollout 90/100, Loss: -1.9089
Policy update: Rollout 95/100, Loss: -1.2152
Policy update: Rollout 100/100, Loss: -2.3907
Evaluation: Average reward = 60.60

Iteration 26/70
Collected 100 trajectories. Average reward: 69.90
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0366
Reward model training epoch 2/5, Loss: 0.0366
Reward model training epoch 3/5, Loss: 0.0366
Reward model training epoch 4/5, Loss: 0.0366
Reward model training epoch 5/5, Loss: 0.0366
Policy update: Rollout 5/100, Loss: 0.3241
Policy update: Rollout 10/100, Loss: -0.2794
Policy update: Rollout 15/100, Loss: -1.1324
Policy update: Rollout 20/100, Loss: 0.3203
Policy update: Rollout 25/100, Loss: -0.4246
Policy update: Rollout 30/100, Loss: 2.2481
Policy update: Rollout 35/100, Loss: -0.3396
Policy update: Rollout 40/100, Loss: -0.4706
Policy update: Rollout 45/100, Loss: -1.5015
Policy update: Rollout 50/100, Loss: 2.3494
Policy update: Rollout 55/100, Loss: -4.9191
Policy update: Rollout 60/100, Loss: -7.2795
Policy update: Rollout 65/100, Loss: 0.9153
Policy update: Rollout 70/100, Loss: -1.2910
Policy update: Rollout 75/100, Loss: 1.7477
Policy update: Rollout 80/100, Loss: 0.4978
Policy update: Rollout 85/100, Loss: -1.7704
Policy update: Rollout 90/100, Loss: -4.3529
Policy update: Rollout 95/100, Loss: 0.6304
Policy update: Rollout 100/100, Loss: -0.2021
Evaluation: Average reward = 62.40

Iteration 27/70
Collected 100 trajectories. Average reward: 70.20
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0371
Reward model training epoch 2/5, Loss: 0.0371
Reward model training epoch 3/5, Loss: 0.0371
Reward model training epoch 4/5, Loss: 0.0371
Reward model training epoch 5/5, Loss: 0.0371
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -3.4012
Policy update: Rollout 10/100, Loss: -3.7466
Policy update: Rollout 15/100, Loss: -0.2316
Policy update: Rollout 20/100, Loss: -1.6878
Policy update: Rollout 25/100, Loss: 0.9907
Policy update: Rollout 30/100, Loss: -2.2225
Policy update: Rollout 35/100, Loss: -1.0502
Policy update: Rollout 40/100, Loss: -0.7259
Policy update: Rollout 45/100, Loss: 0.2476
Policy update: Rollout 50/100, Loss: -0.6893
Policy update: Rollout 55/100, Loss: 0.3819
Policy update: Rollout 60/100, Loss: 0.5478
Policy update: Rollout 65/100, Loss: -2.2566
Policy update: Rollout 70/100, Loss: -4.5674
Policy update: Rollout 75/100, Loss: -3.6131
Policy update: Rollout 80/100, Loss: -2.1001
Policy update: Rollout 85/100, Loss: 0.3958
Policy update: Rollout 90/100, Loss: 0.6909
Policy update: Rollout 95/100, Loss: -2.5635
Policy update: Rollout 100/100, Loss: 0.3889
Evaluation: Average reward = 75.20

Iteration 28/70
Collected 100 trajectories. Average reward: 75.33
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0352
Reward model training epoch 2/5, Loss: 0.0352
Reward model training epoch 3/5, Loss: 0.0352
Reward model training epoch 4/5, Loss: 0.0352
Reward model training epoch 5/5, Loss: 0.0352
Policy update: Rollout 5/100, Loss: -1.1162
Policy update: Rollout 10/100, Loss: 1.2204
Policy update: Rollout 15/100, Loss: 1.0045
Policy update: Rollout 20/100, Loss: -1.0227
Policy update: Rollout 25/100, Loss: -0.8400
Policy update: Rollout 30/100, Loss: -1.1615
Policy update: Rollout 35/100, Loss: 0.5496
Policy update: Rollout 40/100, Loss: -0.3348
Policy update: Rollout 45/100, Loss: -0.3598
Policy update: Rollout 50/100, Loss: -4.3169
Policy update: Rollout 55/100, Loss: -5.4886
Policy update: Rollout 60/100, Loss: -2.3189
Policy update: Rollout 65/100, Loss: -3.4532
Policy update: Rollout 70/100, Loss: 1.5770
Policy update: Rollout 75/100, Loss: -5.5762
Policy update: Rollout 80/100, Loss: 0.8249
Policy update: Rollout 85/100, Loss: -3.8805
Policy update: Rollout 90/100, Loss: -2.5932
Policy update: Rollout 95/100, Loss: 0.2858
Policy update: Rollout 100/100, Loss: -1.4288
Evaluation: Average reward = 67.20

Iteration 29/70
Collected 100 trajectories. Average reward: 87.93
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0267
Reward model training epoch 2/5, Loss: 0.0267
Reward model training epoch 3/5, Loss: 0.0267
Reward model training epoch 4/5, Loss: 0.0267
Reward model training epoch 5/5, Loss: 0.0267
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -1.4939
Policy update: Rollout 10/100, Loss: -2.7436
Policy update: Rollout 15/100, Loss: -4.8149
Policy update: Rollout 20/100, Loss: -0.9312
Policy update: Rollout 25/100, Loss: -6.6960
Policy update: Rollout 30/100, Loss: -1.4190
Policy update: Rollout 35/100, Loss: 0.4292
Policy update: Rollout 40/100, Loss: -2.3945
Policy update: Rollout 45/100, Loss: 2.4207
Policy update: Rollout 50/100, Loss: -2.0645
Policy update: Rollout 55/100, Loss: -1.4434
Policy update: Rollout 60/100, Loss: -1.8213
Policy update: Rollout 65/100, Loss: -3.8197
Policy update: Rollout 70/100, Loss: -3.0406
Policy update: Rollout 75/100, Loss: -4.5131
Policy update: Rollout 80/100, Loss: 2.4012
Policy update: Rollout 85/100, Loss: -4.0473
Policy update: Rollout 90/100, Loss: 0.2931
Policy update: Rollout 95/100, Loss: 0.2260
Policy update: Rollout 100/100, Loss: -1.6529
Evaluation: Average reward = 73.00

Iteration 30/70
Collected 100 trajectories. Average reward: 97.99
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0420
Reward model training epoch 2/5, Loss: 0.0420
Reward model training epoch 3/5, Loss: 0.0420
Reward model training epoch 4/5, Loss: 0.0420
Reward model training epoch 5/5, Loss: 0.0420
Policy update: Rollout 5/100, Loss: -7.2752
Policy update: Rollout 10/100, Loss: -2.6044
Policy update: Rollout 15/100, Loss: -0.3243
Policy update: Rollout 20/100, Loss: -4.6945
Policy update: Rollout 25/100, Loss: -3.3384
Policy update: Rollout 30/100, Loss: 0.6245
Policy update: Rollout 35/100, Loss: -0.5494
Policy update: Rollout 40/100, Loss: -2.1580
Policy update: Rollout 45/100, Loss: -6.0826
Policy update: Rollout 50/100, Loss: -5.9956
Policy update: Rollout 55/100, Loss: -0.9166
Policy update: Rollout 60/100, Loss: -2.3857
Policy update: Rollout 65/100, Loss: -3.0289
Policy update: Rollout 70/100, Loss: -2.3551
Policy update: Rollout 75/100, Loss: 1.8654
Policy update: Rollout 80/100, Loss: 0.5516
Policy update: Rollout 85/100, Loss: -0.5700
Policy update: Rollout 90/100, Loss: -2.7375
Policy update: Rollout 95/100, Loss: 0.1585
Policy update: Rollout 100/100, Loss: -4.8442
Evaluation: Average reward = 91.40

Iteration 31/70
Collected 100 trajectories. Average reward: 99.26
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0379
Reward model training epoch 2/5, Loss: 0.0379
Reward model training epoch 3/5, Loss: 0.0379
Reward model training epoch 4/5, Loss: 0.0379
Reward model training epoch 5/5, Loss: 0.0379
Preference prediction accuracy: 0.97
Policy update: Rollout 5/100, Loss: -0.4780
Policy update: Rollout 10/100, Loss: -6.7837
Policy update: Rollout 15/100, Loss: -1.2440
Policy update: Rollout 20/100, Loss: -0.6823
Policy update: Rollout 25/100, Loss: -1.4241
Policy update: Rollout 30/100, Loss: 2.1861
Policy update: Rollout 35/100, Loss: -2.3595
Policy update: Rollout 40/100, Loss: -2.7498
Policy update: Rollout 45/100, Loss: -2.6769
Policy update: Rollout 50/100, Loss: -1.9211
Policy update: Rollout 55/100, Loss: -2.2686
Policy update: Rollout 60/100, Loss: -1.6123
Policy update: Rollout 65/100, Loss: -0.3621
Policy update: Rollout 70/100, Loss: -1.6584
Policy update: Rollout 75/100, Loss: -6.2080
Policy update: Rollout 80/100, Loss: -3.4750
Policy update: Rollout 85/100, Loss: -2.8852
Policy update: Rollout 90/100, Loss: -1.3065
Policy update: Rollout 95/100, Loss: -0.7097
Policy update: Rollout 100/100, Loss: -1.3169
Evaluation: Average reward = 79.60

Iteration 32/70
Collected 100 trajectories. Average reward: 107.08
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0330
Reward model training epoch 2/5, Loss: 0.0330
Reward model training epoch 3/5, Loss: 0.0330
Reward model training epoch 4/5, Loss: 0.0330
Reward model training epoch 5/5, Loss: 0.0330
Policy update: Rollout 5/100, Loss: -1.1686
Policy update: Rollout 10/100, Loss: -2.4319
Policy update: Rollout 15/100, Loss: -1.7892
Policy update: Rollout 20/100, Loss: 1.0562
Policy update: Rollout 25/100, Loss: -3.3225
Policy update: Rollout 30/100, Loss: 0.1264
Policy update: Rollout 35/100, Loss: 1.9213
Policy update: Rollout 40/100, Loss: 1.9303
Policy update: Rollout 45/100, Loss: 0.3484
Policy update: Rollout 50/100, Loss: 0.1520
Policy update: Rollout 55/100, Loss: 0.8655
Policy update: Rollout 60/100, Loss: -0.8691
Policy update: Rollout 65/100, Loss: -1.7385
Policy update: Rollout 70/100, Loss: -1.3486
Policy update: Rollout 75/100, Loss: -2.3822
Policy update: Rollout 80/100, Loss: -0.1847
Policy update: Rollout 85/100, Loss: -1.0350
Policy update: Rollout 90/100, Loss: 0.8500
Policy update: Rollout 95/100, Loss: 1.7577
Policy update: Rollout 100/100, Loss: -1.0687
Evaluation: Average reward = 133.80

Iteration 33/70
Collected 100 trajectories. Average reward: 108.23
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0393
Reward model training epoch 2/5, Loss: 0.0393
Reward model training epoch 3/5, Loss: 0.0393
Reward model training epoch 4/5, Loss: 0.0393
Reward model training epoch 5/5, Loss: 0.0393
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -1.3929
Policy update: Rollout 10/100, Loss: -7.5440
Policy update: Rollout 15/100, Loss: -0.6296
Policy update: Rollout 20/100, Loss: -1.2289
Policy update: Rollout 25/100, Loss: -1.4273
Policy update: Rollout 30/100, Loss: -3.4218
Policy update: Rollout 35/100, Loss: -0.0160
Policy update: Rollout 40/100, Loss: -1.2520
Policy update: Rollout 45/100, Loss: -0.1248
Policy update: Rollout 50/100, Loss: -2.2909
Policy update: Rollout 55/100, Loss: -0.2279
Policy update: Rollout 60/100, Loss: 2.0852
Policy update: Rollout 65/100, Loss: 0.4234
Policy update: Rollout 70/100, Loss: -0.1762
Policy update: Rollout 75/100, Loss: -2.7924
Policy update: Rollout 80/100, Loss: -1.6858
Policy update: Rollout 85/100, Loss: -0.2396
Policy update: Rollout 90/100, Loss: 1.5834
Policy update: Rollout 95/100, Loss: -1.5548
Policy update: Rollout 100/100, Loss: 1.9960
Evaluation: Average reward = 102.80

Iteration 34/70
Collected 100 trajectories. Average reward: 113.37
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0369
Reward model training epoch 2/5, Loss: 0.0369
Reward model training epoch 3/5, Loss: 0.0369
Reward model training epoch 4/5, Loss: 0.0369
Reward model training epoch 5/5, Loss: 0.0369
Policy update: Rollout 5/100, Loss: 0.1686
Policy update: Rollout 10/100, Loss: -4.3357
Policy update: Rollout 15/100, Loss: -7.1668
Policy update: Rollout 20/100, Loss: -2.5102
Policy update: Rollout 25/100, Loss: -2.1372
Policy update: Rollout 30/100, Loss: 0.3869
Policy update: Rollout 35/100, Loss: -2.9734
Policy update: Rollout 40/100, Loss: -1.0917
Policy update: Rollout 45/100, Loss: 1.8326
Policy update: Rollout 50/100, Loss: 0.5125
Policy update: Rollout 55/100, Loss: -0.4782
Policy update: Rollout 60/100, Loss: -1.3482
Policy update: Rollout 65/100, Loss: 1.0062
Policy update: Rollout 70/100, Loss: -0.9742
Policy update: Rollout 75/100, Loss: -2.4183
Policy update: Rollout 80/100, Loss: -0.8739
Policy update: Rollout 85/100, Loss: -2.7988
Policy update: Rollout 90/100, Loss: -4.5036
Policy update: Rollout 95/100, Loss: -0.6974
Policy update: Rollout 100/100, Loss: -5.1156
Evaluation: Average reward = 92.00

Iteration 35/70
Collected 100 trajectories. Average reward: 118.91
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0345
Reward model training epoch 2/5, Loss: 0.0345
Reward model training epoch 3/5, Loss: 0.0345
Reward model training epoch 4/5, Loss: 0.0345
Reward model training epoch 5/5, Loss: 0.0345
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -0.5806
Policy update: Rollout 10/100, Loss: -2.1277
Policy update: Rollout 15/100, Loss: -2.7125
Policy update: Rollout 20/100, Loss: -5.2291
Policy update: Rollout 25/100, Loss: -3.7700
Policy update: Rollout 30/100, Loss: -2.9257
Policy update: Rollout 35/100, Loss: -0.3620
Policy update: Rollout 40/100, Loss: -7.0021
Policy update: Rollout 45/100, Loss: 0.2042
Policy update: Rollout 50/100, Loss: -9.2495
Policy update: Rollout 55/100, Loss: -1.3078
Policy update: Rollout 60/100, Loss: 0.1883
Policy update: Rollout 65/100, Loss: -1.1693
Policy update: Rollout 70/100, Loss: -3.6737
Policy update: Rollout 75/100, Loss: -0.6707
Policy update: Rollout 80/100, Loss: -2.6823
Policy update: Rollout 85/100, Loss: -2.2283
Policy update: Rollout 90/100, Loss: -2.7614
Policy update: Rollout 95/100, Loss: 0.3816
Policy update: Rollout 100/100, Loss: -1.1245
Evaluation: Average reward = 142.40

Iteration 36/70
Collected 100 trajectories. Average reward: 118.51
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0636
Reward model training epoch 2/5, Loss: 0.0636
Reward model training epoch 3/5, Loss: 0.0636
Reward model training epoch 4/5, Loss: 0.0636
Reward model training epoch 5/5, Loss: 0.0636
Policy update: Rollout 5/100, Loss: 0.3560
Policy update: Rollout 10/100, Loss: -1.6020
Policy update: Rollout 15/100, Loss: 1.1704
Policy update: Rollout 20/100, Loss: 1.9780
Policy update: Rollout 25/100, Loss: -0.6542
Policy update: Rollout 30/100, Loss: -3.4375
Policy update: Rollout 35/100, Loss: -0.7701
Policy update: Rollout 40/100, Loss: -2.7466
Policy update: Rollout 45/100, Loss: 2.6402
Policy update: Rollout 50/100, Loss: -0.5146
Policy update: Rollout 55/100, Loss: 1.4381
Policy update: Rollout 60/100, Loss: -1.5580
Policy update: Rollout 65/100, Loss: -0.4912
Policy update: Rollout 70/100, Loss: -2.3354
Policy update: Rollout 75/100, Loss: -3.3647
Policy update: Rollout 80/100, Loss: 0.6803
Policy update: Rollout 85/100, Loss: 0.8862
Policy update: Rollout 90/100, Loss: -1.6494
Policy update: Rollout 95/100, Loss: -2.5530
Policy update: Rollout 100/100, Loss: -0.1976
Evaluation: Average reward = 131.60

Iteration 37/70
Collected 100 trajectories. Average reward: 124.58
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0501
Reward model training epoch 2/5, Loss: 0.0501
Reward model training epoch 3/5, Loss: 0.0501
Reward model training epoch 4/5, Loss: 0.0501
Reward model training epoch 5/5, Loss: 0.0501
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 0.8865
Policy update: Rollout 10/100, Loss: -2.7902
Policy update: Rollout 15/100, Loss: -5.1938
Policy update: Rollout 20/100, Loss: -3.2863
Policy update: Rollout 25/100, Loss: 0.0610
Policy update: Rollout 30/100, Loss: -7.4945
Policy update: Rollout 35/100, Loss: 1.6240
Policy update: Rollout 40/100, Loss: -3.1599
Policy update: Rollout 45/100, Loss: -1.4644
Policy update: Rollout 50/100, Loss: 0.1809
Policy update: Rollout 55/100, Loss: -1.3002
Policy update: Rollout 60/100, Loss: -2.7235
Policy update: Rollout 65/100, Loss: -4.8776
Policy update: Rollout 70/100, Loss: -3.1694
Policy update: Rollout 75/100, Loss: -0.2082
Policy update: Rollout 80/100, Loss: -3.1315
Policy update: Rollout 85/100, Loss: -5.9447
Policy update: Rollout 90/100, Loss: 0.0995
Policy update: Rollout 95/100, Loss: -0.4059
Policy update: Rollout 100/100, Loss: -2.6123
Evaluation: Average reward = 141.00

Iteration 38/70
Collected 100 trajectories. Average reward: 131.32
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0979
Reward model training epoch 2/5, Loss: 0.0979
Reward model training epoch 3/5, Loss: 0.0979
Reward model training epoch 4/5, Loss: 0.0979
Reward model training epoch 5/5, Loss: 0.0979
Policy update: Rollout 5/100, Loss: -4.3114
Policy update: Rollout 10/100, Loss: -4.2738
Policy update: Rollout 15/100, Loss: -3.8398
Policy update: Rollout 20/100, Loss: 0.2184
Policy update: Rollout 25/100, Loss: -0.4429
Policy update: Rollout 30/100, Loss: -1.1830
Policy update: Rollout 35/100, Loss: -1.5062
Policy update: Rollout 40/100, Loss: -1.0742
Policy update: Rollout 45/100, Loss: -0.2346
Policy update: Rollout 50/100, Loss: 0.6431
Policy update: Rollout 55/100, Loss: -1.4691
Policy update: Rollout 60/100, Loss: -7.2062
Policy update: Rollout 65/100, Loss: -9.9418
Policy update: Rollout 70/100, Loss: -4.0192
Policy update: Rollout 75/100, Loss: -8.1586
Policy update: Rollout 80/100, Loss: 0.6405
Policy update: Rollout 85/100, Loss: -0.1335
Policy update: Rollout 90/100, Loss: -4.0669
Policy update: Rollout 95/100, Loss: 1.3690
Policy update: Rollout 100/100, Loss: -0.2810
Evaluation: Average reward = 105.20

Iteration 39/70
Collected 100 trajectories. Average reward: 137.29
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0892
Reward model training epoch 2/5, Loss: 0.0892
Reward model training epoch 3/5, Loss: 0.0892
Reward model training epoch 4/5, Loss: 0.0892
Reward model training epoch 5/5, Loss: 0.0892
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -4.8924
Policy update: Rollout 10/100, Loss: -0.5333
Policy update: Rollout 15/100, Loss: 4.5273
Policy update: Rollout 20/100, Loss: -2.5706
Policy update: Rollout 25/100, Loss: 2.6976
Policy update: Rollout 30/100, Loss: -2.2219
Policy update: Rollout 35/100, Loss: -4.8436
Policy update: Rollout 40/100, Loss: -3.0926
Policy update: Rollout 45/100, Loss: -0.0326
Policy update: Rollout 50/100, Loss: 1.5406
Policy update: Rollout 55/100, Loss: -0.8062
Policy update: Rollout 60/100, Loss: -0.7824
Policy update: Rollout 65/100, Loss: -2.0869
Policy update: Rollout 70/100, Loss: 0.7619
Policy update: Rollout 75/100, Loss: -13.3334
Policy update: Rollout 80/100, Loss: -11.1490
Policy update: Rollout 85/100, Loss: -3.0356
Policy update: Rollout 90/100, Loss: -3.2631
Policy update: Rollout 95/100, Loss: 2.7541
Policy update: Rollout 100/100, Loss: -1.2312
Evaluation: Average reward = 140.80

Iteration 40/70
Collected 100 trajectories. Average reward: 145.87
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1393
Reward model training epoch 2/5, Loss: 0.1393
Reward model training epoch 3/5, Loss: 0.1393
Reward model training epoch 4/5, Loss: 0.1393
Reward model training epoch 5/5, Loss: 0.1393
Policy update: Rollout 5/100, Loss: 0.8560
Policy update: Rollout 10/100, Loss: 3.9300
Policy update: Rollout 15/100, Loss: -13.1000
Policy update: Rollout 20/100, Loss: -2.7047
Policy update: Rollout 25/100, Loss: -5.1042
Policy update: Rollout 30/100, Loss: 2.4720
Policy update: Rollout 35/100, Loss: 1.7797
Policy update: Rollout 40/100, Loss: -0.1445
Policy update: Rollout 45/100, Loss: 0.6059
Policy update: Rollout 50/100, Loss: -0.6766
Policy update: Rollout 55/100, Loss: 1.2924
Policy update: Rollout 60/100, Loss: -6.2892
Policy update: Rollout 65/100, Loss: -2.1937
Policy update: Rollout 70/100, Loss: 3.0026
Policy update: Rollout 75/100, Loss: 2.1707
Policy update: Rollout 80/100, Loss: -0.3169
Policy update: Rollout 85/100, Loss: -8.6422
Policy update: Rollout 90/100, Loss: -5.6728
Policy update: Rollout 95/100, Loss: -5.1606
Policy update: Rollout 100/100, Loss: -1.2628
Evaluation: Average reward = 159.20

Iteration 41/70
Collected 100 trajectories. Average reward: 144.06
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1722
Reward model training epoch 2/5, Loss: 0.1722
Reward model training epoch 3/5, Loss: 0.1722
Reward model training epoch 4/5, Loss: 0.1722
Reward model training epoch 5/5, Loss: 0.1722
Preference prediction accuracy: 0.93
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -1.8903
Policy update: Rollout 10/100, Loss: -9.4630
Policy update: Rollout 15/100, Loss: -1.3564
Policy update: Rollout 20/100, Loss: 1.2619
Policy update: Rollout 25/100, Loss: -0.5250
Policy update: Rollout 30/100, Loss: -3.5413
Policy update: Rollout 35/100, Loss: 1.9807
Policy update: Rollout 40/100, Loss: 0.8091
Policy update: Rollout 45/100, Loss: -6.2100
Policy update: Rollout 50/100, Loss: -2.7003
Policy update: Rollout 55/100, Loss: -3.1395
Policy update: Rollout 60/100, Loss: 0.7289
Policy update: Rollout 65/100, Loss: -1.6958
Policy update: Rollout 70/100, Loss: -4.3927
Policy update: Rollout 75/100, Loss: -7.5126
Policy update: Rollout 80/100, Loss: -1.3734
Policy update: Rollout 85/100, Loss: 0.2540
Policy update: Rollout 90/100, Loss: 0.0132
Policy update: Rollout 95/100, Loss: -9.3634
Policy update: Rollout 100/100, Loss: 0.3048
Evaluation: Average reward = 168.20

Iteration 42/70
Collected 100 trajectories. Average reward: 157.24
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.2204
Reward model training epoch 2/5, Loss: 0.2204
Reward model training epoch 3/5, Loss: 0.2204
Reward model training epoch 4/5, Loss: 0.2204
Reward model training epoch 5/5, Loss: 0.2204
Policy update: Rollout 5/100, Loss: -2.8324
Policy update: Rollout 10/100, Loss: -1.6789
Policy update: Rollout 15/100, Loss: -2.0122
Policy update: Rollout 20/100, Loss: -6.8435
Policy update: Rollout 25/100, Loss: 2.2358
Policy update: Rollout 30/100, Loss: -1.3412
Policy update: Rollout 35/100, Loss: -3.1760
Policy update: Rollout 40/100, Loss: -6.6370
Policy update: Rollout 45/100, Loss: -3.1000
Policy update: Rollout 50/100, Loss: -0.0707
Policy update: Rollout 55/100, Loss: -1.6998
Policy update: Rollout 60/100, Loss: -1.3455
Policy update: Rollout 65/100, Loss: -8.8186
Policy update: Rollout 70/100, Loss: -5.3169
Policy update: Rollout 75/100, Loss: 1.7631
Policy update: Rollout 80/100, Loss: 1.0122
Policy update: Rollout 85/100, Loss: 1.7301
Policy update: Rollout 90/100, Loss: -1.9568
Policy update: Rollout 95/100, Loss: -1.9112
Policy update: Rollout 100/100, Loss: -9.7994
Evaluation: Average reward = 149.20

Iteration 43/70
Collected 100 trajectories. Average reward: 156.65
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.4331
Reward model training epoch 2/5, Loss: 0.4331
Reward model training epoch 3/5, Loss: 0.4331
Reward model training epoch 4/5, Loss: 0.4331
Reward model training epoch 5/5, Loss: 0.4331
Preference prediction accuracy: 0.90
Policy update: Rollout 5/100, Loss: -5.3484
Policy update: Rollout 10/100, Loss: -4.8388
Policy update: Rollout 15/100, Loss: -0.7228
Policy update: Rollout 20/100, Loss: -8.2887
Policy update: Rollout 25/100, Loss: -2.0115
Policy update: Rollout 30/100, Loss: -3.5409
Policy update: Rollout 35/100, Loss: -1.9579
Policy update: Rollout 40/100, Loss: 0.9205
Policy update: Rollout 45/100, Loss: 0.4066
Policy update: Rollout 50/100, Loss: -4.3974
Policy update: Rollout 55/100, Loss: -3.2814
Policy update: Rollout 60/100, Loss: 0.7392
Policy update: Rollout 65/100, Loss: 2.3967
Policy update: Rollout 70/100, Loss: -1.1282
Policy update: Rollout 75/100, Loss: -1.3917
Policy update: Rollout 80/100, Loss: -1.1632
Policy update: Rollout 85/100, Loss: -5.2704
Policy update: Rollout 90/100, Loss: 2.1499
Policy update: Rollout 95/100, Loss: -2.2658
Policy update: Rollout 100/100, Loss: -1.4131
Evaluation: Average reward = 181.80

Iteration 44/70
Collected 100 trajectories. Average reward: 164.53
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.4390
Reward model training epoch 2/5, Loss: 0.4390
Reward model training epoch 3/5, Loss: 0.4390
Reward model training epoch 4/5, Loss: 0.4390
Reward model training epoch 5/5, Loss: 0.4390
Policy update: Rollout 5/100, Loss: 1.0611
Policy update: Rollout 10/100, Loss: -0.7197
Policy update: Rollout 15/100, Loss: -0.3968
Policy update: Rollout 20/100, Loss: -0.8811
Policy update: Rollout 25/100, Loss: -5.7747
Policy update: Rollout 30/100, Loss: 1.4234
Policy update: Rollout 35/100, Loss: -0.5700
Policy update: Rollout 40/100, Loss: -10.8276
Policy update: Rollout 45/100, Loss: -4.4957
Policy update: Rollout 50/100, Loss: -3.6021
Policy update: Rollout 55/100, Loss: 0.3814
Policy update: Rollout 60/100, Loss: -6.4712
Policy update: Rollout 65/100, Loss: 1.2317
Policy update: Rollout 70/100, Loss: -7.6698
Policy update: Rollout 75/100, Loss: -4.3323
Policy update: Rollout 80/100, Loss: -2.0879
Policy update: Rollout 85/100, Loss: -2.0597
Policy update: Rollout 90/100, Loss: -1.8831
Policy update: Rollout 95/100, Loss: -8.6728
Policy update: Rollout 100/100, Loss: -4.2454
Evaluation: Average reward = 145.40

Iteration 45/70
Collected 100 trajectories. Average reward: 163.22
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.3369
Reward model training epoch 2/5, Loss: 0.3369
Reward model training epoch 3/5, Loss: 0.3369
Reward model training epoch 4/5, Loss: 0.3369
Reward model training epoch 5/5, Loss: 0.3369
Preference prediction accuracy: 0.97
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -1.8950
Policy update: Rollout 10/100, Loss: -3.9538
Policy update: Rollout 15/100, Loss: -0.2784
Policy update: Rollout 20/100, Loss: 1.5204
Policy update: Rollout 25/100, Loss: -2.9083
Policy update: Rollout 30/100, Loss: -4.1545
Policy update: Rollout 35/100, Loss: -3.7155
Policy update: Rollout 40/100, Loss: 0.7953
Policy update: Rollout 45/100, Loss: 1.1625
Policy update: Rollout 50/100, Loss: -2.6220
Policy update: Rollout 55/100, Loss: -6.6685
Policy update: Rollout 60/100, Loss: -0.2871
Policy update: Rollout 65/100, Loss: -1.5981
Policy update: Rollout 70/100, Loss: -2.8500
Policy update: Rollout 75/100, Loss: 0.2262
Policy update: Rollout 80/100, Loss: 1.2372
Policy update: Rollout 85/100, Loss: 2.5583
Policy update: Rollout 90/100, Loss: -2.0930
Policy update: Rollout 95/100, Loss: -3.8323
Policy update: Rollout 100/100, Loss: -2.7805
Evaluation: Average reward = 168.40

Iteration 46/70
Collected 100 trajectories. Average reward: 159.87
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.3825
Reward model training epoch 2/5, Loss: 0.3825
Reward model training epoch 3/5, Loss: 0.3825
Reward model training epoch 4/5, Loss: 0.3825
Reward model training epoch 5/5, Loss: 0.3825
Policy update: Rollout 5/100, Loss: 0.9106
Policy update: Rollout 10/100, Loss: -1.9682
Policy update: Rollout 15/100, Loss: -4.7781
Policy update: Rollout 20/100, Loss: 1.0420
Policy update: Rollout 25/100, Loss: 0.0922
Policy update: Rollout 30/100, Loss: -5.8657
Policy update: Rollout 35/100, Loss: -4.7746
Policy update: Rollout 40/100, Loss: -6.1234
Policy update: Rollout 45/100, Loss: -2.8545
Policy update: Rollout 50/100, Loss: -1.6253
Policy update: Rollout 55/100, Loss: -2.3840
Policy update: Rollout 60/100, Loss: 0.2268
Policy update: Rollout 65/100, Loss: -5.4867
Policy update: Rollout 70/100, Loss: -0.1659
Policy update: Rollout 75/100, Loss: -2.7046
Policy update: Rollout 80/100, Loss: -3.3228
Policy update: Rollout 85/100, Loss: -1.5239
Policy update: Rollout 90/100, Loss: -5.2456
Policy update: Rollout 95/100, Loss: -0.9497
Policy update: Rollout 100/100, Loss: -1.6853
Evaluation: Average reward = 147.40

Iteration 47/70
Collected 100 trajectories. Average reward: 162.98
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.4654
Reward model training epoch 2/5, Loss: 0.4654
Reward model training epoch 3/5, Loss: 0.4654
Reward model training epoch 4/5, Loss: 0.4654
Reward model training epoch 5/5, Loss: 0.4654
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -4.6124
Policy update: Rollout 10/100, Loss: -1.5721
Policy update: Rollout 15/100, Loss: -8.3961
Policy update: Rollout 20/100, Loss: 0.4332
Policy update: Rollout 25/100, Loss: -2.3473
Policy update: Rollout 30/100, Loss: -0.0246
Policy update: Rollout 35/100, Loss: -0.6261
Policy update: Rollout 40/100, Loss: -2.4289
Policy update: Rollout 45/100, Loss: -5.8152
Policy update: Rollout 50/100, Loss: -3.1225
Policy update: Rollout 55/100, Loss: -4.7373
Policy update: Rollout 60/100, Loss: -0.0333
Policy update: Rollout 65/100, Loss: -5.3046
Policy update: Rollout 70/100, Loss: -4.3642
Policy update: Rollout 75/100, Loss: -3.3475
Policy update: Rollout 80/100, Loss: 2.2467
Policy update: Rollout 85/100, Loss: -2.0407
Policy update: Rollout 90/100, Loss: -6.2551
Policy update: Rollout 95/100, Loss: -0.4565
Policy update: Rollout 100/100, Loss: -8.7602
Evaluation: Average reward = 165.80

Iteration 48/70
Collected 100 trajectories. Average reward: 165.71
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.3531
Reward model training epoch 2/5, Loss: 0.3531
Reward model training epoch 3/5, Loss: 0.3531
Reward model training epoch 4/5, Loss: 0.3531
Reward model training epoch 5/5, Loss: 0.3531
Policy update: Rollout 5/100, Loss: -9.7856
Policy update: Rollout 10/100, Loss: 0.6219
Policy update: Rollout 15/100, Loss: -3.7918
Policy update: Rollout 20/100, Loss: -1.8159
Policy update: Rollout 25/100, Loss: 0.9733
Policy update: Rollout 30/100, Loss: -16.1126
Policy update: Rollout 35/100, Loss: -2.8345
Policy update: Rollout 40/100, Loss: -2.6076
Policy update: Rollout 45/100, Loss: 1.5120
Policy update: Rollout 50/100, Loss: -0.4508
Policy update: Rollout 55/100, Loss: -3.8343
Policy update: Rollout 60/100, Loss: -2.0493
Policy update: Rollout 65/100, Loss: -0.5318
Policy update: Rollout 70/100, Loss: -10.0112
Policy update: Rollout 75/100, Loss: 0.2118
Policy update: Rollout 80/100, Loss: 1.2791
Policy update: Rollout 85/100, Loss: -10.0272
Policy update: Rollout 90/100, Loss: -0.9502
Policy update: Rollout 95/100, Loss: 2.0796
Policy update: Rollout 100/100, Loss: -7.4950
Evaluation: Average reward = 193.40

Iteration 49/70
Collected 100 trajectories. Average reward: 170.04
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6433
Reward model training epoch 2/5, Loss: 0.6433
Reward model training epoch 3/5, Loss: 0.6432
Reward model training epoch 4/5, Loss: 0.6432
Reward model training epoch 5/5, Loss: 0.6432
Preference prediction accuracy: 0.97
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -4.7764
Policy update: Rollout 10/100, Loss: -1.1717
Policy update: Rollout 15/100, Loss: -0.3581
Policy update: Rollout 20/100, Loss: 2.6644
Policy update: Rollout 25/100, Loss: -8.1448
Policy update: Rollout 30/100, Loss: -0.5573
Policy update: Rollout 35/100, Loss: -7.1280
Policy update: Rollout 40/100, Loss: -2.2958
Policy update: Rollout 45/100, Loss: -6.9960
Policy update: Rollout 50/100, Loss: -2.2153
Policy update: Rollout 55/100, Loss: -3.5130
Policy update: Rollout 60/100, Loss: -0.9268
Policy update: Rollout 65/100, Loss: 0.0330
Policy update: Rollout 70/100, Loss: -12.8987
Policy update: Rollout 75/100, Loss: 1.0217
Policy update: Rollout 80/100, Loss: -2.3500
Policy update: Rollout 85/100, Loss: 1.1143
Policy update: Rollout 90/100, Loss: 0.0165
Policy update: Rollout 95/100, Loss: -2.3379
Policy update: Rollout 100/100, Loss: -0.4364
Evaluation: Average reward = 168.40

Iteration 50/70
Collected 100 trajectories. Average reward: 177.04
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.5507
Reward model training epoch 2/5, Loss: 0.5506
Reward model training epoch 3/5, Loss: 0.5506
Reward model training epoch 4/5, Loss: 0.5506
Reward model training epoch 5/5, Loss: 0.5506
Policy update: Rollout 5/100, Loss: -3.2255
Policy update: Rollout 10/100, Loss: -0.7307
Policy update: Rollout 15/100, Loss: 1.8840
Policy update: Rollout 20/100, Loss: -0.3893
Policy update: Rollout 25/100, Loss: -4.4218
Policy update: Rollout 30/100, Loss: -4.8937
Policy update: Rollout 35/100, Loss: -3.0604
Policy update: Rollout 40/100, Loss: -8.1413
Policy update: Rollout 45/100, Loss: -4.8808
Policy update: Rollout 50/100, Loss: -7.6560
Policy update: Rollout 55/100, Loss: -4.5684
Policy update: Rollout 60/100, Loss: -3.2706
Policy update: Rollout 65/100, Loss: -2.5395
Policy update: Rollout 70/100, Loss: -0.2955
Policy update: Rollout 75/100, Loss: 1.9325
Policy update: Rollout 80/100, Loss: -0.6221
Policy update: Rollout 85/100, Loss: -2.0848
Policy update: Rollout 90/100, Loss: 2.9373
Policy update: Rollout 95/100, Loss: -6.2382
Policy update: Rollout 100/100, Loss: -4.0111
Evaluation: Average reward = 154.00

Iteration 51/70
Collected 100 trajectories. Average reward: 168.70
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.4448
Reward model training epoch 2/5, Loss: 0.4448
Reward model training epoch 3/5, Loss: 0.4448
Reward model training epoch 4/5, Loss: 0.4448
Reward model training epoch 5/5, Loss: 0.4448
Preference prediction accuracy: 0.83
Policy update: Rollout 5/100, Loss: -0.9767
Policy update: Rollout 10/100, Loss: -0.9962
Policy update: Rollout 15/100, Loss: -3.5223
Policy update: Rollout 20/100, Loss: -2.1359
Policy update: Rollout 25/100, Loss: -0.6823
Policy update: Rollout 30/100, Loss: -4.0800
Policy update: Rollout 35/100, Loss: -2.0899
Policy update: Rollout 40/100, Loss: -0.7229
Policy update: Rollout 45/100, Loss: 0.6562
Policy update: Rollout 50/100, Loss: -1.4318
Policy update: Rollout 55/100, Loss: 1.0594
Policy update: Rollout 60/100, Loss: 4.1012
Policy update: Rollout 65/100, Loss: 3.8420
Policy update: Rollout 70/100, Loss: -1.6266
Policy update: Rollout 75/100, Loss: -0.9921
Policy update: Rollout 80/100, Loss: 0.1275
Policy update: Rollout 85/100, Loss: -11.9245
Policy update: Rollout 90/100, Loss: 0.4884
Policy update: Rollout 95/100, Loss: -5.4740
Policy update: Rollout 100/100, Loss: -2.1590
Evaluation: Average reward = 175.60

Iteration 52/70
Collected 100 trajectories. Average reward: 177.20
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6881
Reward model training epoch 2/5, Loss: 0.6881
Reward model training epoch 3/5, Loss: 0.6881
Reward model training epoch 4/5, Loss: 0.6881
Reward model training epoch 5/5, Loss: 0.6881
Policy update: Rollout 5/100, Loss: -6.7137
Policy update: Rollout 10/100, Loss: -6.8486
Policy update: Rollout 15/100, Loss: 0.2733
Policy update: Rollout 20/100, Loss: -0.2244
Policy update: Rollout 25/100, Loss: -4.9959
Policy update: Rollout 30/100, Loss: -0.5092
Policy update: Rollout 35/100, Loss: -1.1111
Policy update: Rollout 40/100, Loss: -1.4473
Policy update: Rollout 45/100, Loss: -3.1608
Policy update: Rollout 50/100, Loss: 0.5819
Policy update: Rollout 55/100, Loss: -3.6388
Policy update: Rollout 60/100, Loss: -2.7154
Policy update: Rollout 65/100, Loss: -0.5389
Policy update: Rollout 70/100, Loss: -1.2548
Policy update: Rollout 75/100, Loss: 0.4446
Policy update: Rollout 80/100, Loss: -2.8441
Policy update: Rollout 85/100, Loss: 2.8860
Policy update: Rollout 90/100, Loss: -1.2351
Policy update: Rollout 95/100, Loss: -6.5803
Policy update: Rollout 100/100, Loss: -1.3580
Evaluation: Average reward = 175.40

Iteration 53/70
Collected 100 trajectories. Average reward: 175.02
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6519
Reward model training epoch 2/5, Loss: 0.6519
Reward model training epoch 3/5, Loss: 0.6519
Reward model training epoch 4/5, Loss: 0.6519
Reward model training epoch 5/5, Loss: 0.6519
Preference prediction accuracy: 0.93
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 7.0046
Policy update: Rollout 10/100, Loss: -1.4285
Policy update: Rollout 15/100, Loss: -1.8652
Policy update: Rollout 20/100, Loss: 1.9470
Policy update: Rollout 25/100, Loss: 2.3218
Policy update: Rollout 30/100, Loss: 1.7858
Policy update: Rollout 35/100, Loss: 2.7171
Policy update: Rollout 40/100, Loss: -0.1191
Policy update: Rollout 45/100, Loss: -7.9503
Policy update: Rollout 50/100, Loss: 1.8304
Policy update: Rollout 55/100, Loss: 2.5588
Policy update: Rollout 60/100, Loss: 3.6979
Policy update: Rollout 65/100, Loss: -6.9629
Policy update: Rollout 70/100, Loss: -3.0108
Policy update: Rollout 75/100, Loss: -6.4614
Policy update: Rollout 80/100, Loss: -5.9291
Policy update: Rollout 85/100, Loss: 1.8829
Policy update: Rollout 90/100, Loss: -4.6679
Policy update: Rollout 95/100, Loss: -8.3393
Policy update: Rollout 100/100, Loss: -8.7098
Evaluation: Average reward = 180.60

Iteration 54/70
Collected 100 trajectories. Average reward: 172.15
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.5757
Reward model training epoch 2/5, Loss: 0.5757
Reward model training epoch 3/5, Loss: 0.5757
Reward model training epoch 4/5, Loss: 0.5757
Reward model training epoch 5/5, Loss: 0.5757
Policy update: Rollout 5/100, Loss: -3.6175
Policy update: Rollout 10/100, Loss: 5.4225
Policy update: Rollout 15/100, Loss: -4.0448
Policy update: Rollout 20/100, Loss: -14.0489
Policy update: Rollout 25/100, Loss: 2.6867
Policy update: Rollout 30/100, Loss: 5.7989
Policy update: Rollout 35/100, Loss: -0.3106
Policy update: Rollout 40/100, Loss: -4.2149
Policy update: Rollout 45/100, Loss: -2.6431
Policy update: Rollout 50/100, Loss: -11.7020
Policy update: Rollout 55/100, Loss: -1.8435
Policy update: Rollout 60/100, Loss: -6.0022
Policy update: Rollout 65/100, Loss: 5.4808
Policy update: Rollout 70/100, Loss: -1.1571
Policy update: Rollout 75/100, Loss: -2.1906
Policy update: Rollout 80/100, Loss: 1.1609
Policy update: Rollout 85/100, Loss: -3.8367
Policy update: Rollout 90/100, Loss: 5.3124
Policy update: Rollout 95/100, Loss: -1.3464
Policy update: Rollout 100/100, Loss: 0.5261
Evaluation: Average reward = 156.40

Iteration 55/70
Collected 100 trajectories. Average reward: 173.62
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6588
Reward model training epoch 2/5, Loss: 0.6588
Reward model training epoch 3/5, Loss: 0.6588
Reward model training epoch 4/5, Loss: 0.6588
Reward model training epoch 5/5, Loss: 0.6588
Preference prediction accuracy: 0.90
Policy update: Rollout 5/100, Loss: -0.7267
Policy update: Rollout 10/100, Loss: -6.5507
Policy update: Rollout 15/100, Loss: -3.7521
Policy update: Rollout 20/100, Loss: -7.0445
Policy update: Rollout 25/100, Loss: -7.2962
Policy update: Rollout 30/100, Loss: -4.8237
Policy update: Rollout 35/100, Loss: 5.9186
Policy update: Rollout 40/100, Loss: -1.2053
Policy update: Rollout 45/100, Loss: -1.8338
Policy update: Rollout 50/100, Loss: 0.0334
Policy update: Rollout 55/100, Loss: -0.8417
Policy update: Rollout 60/100, Loss: -8.2198
Policy update: Rollout 65/100, Loss: -0.0557
Policy update: Rollout 70/100, Loss: 1.1378
Policy update: Rollout 75/100, Loss: -1.5381
Policy update: Rollout 80/100, Loss: -4.4531
Policy update: Rollout 85/100, Loss: -5.0542
Policy update: Rollout 90/100, Loss: -0.5192
Policy update: Rollout 95/100, Loss: 0.7647
Policy update: Rollout 100/100, Loss: -7.9391
Evaluation: Average reward = 169.80

Iteration 56/70
Collected 100 trajectories. Average reward: 181.76
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6932
Reward model training epoch 2/5, Loss: 0.6932
Reward model training epoch 3/5, Loss: 0.6932
Reward model training epoch 4/5, Loss: 0.6932
Reward model training epoch 5/5, Loss: 0.6932
Policy update: Rollout 5/100, Loss: 1.1642
Policy update: Rollout 10/100, Loss: -3.2105
Policy update: Rollout 15/100, Loss: -1.3956
Policy update: Rollout 20/100, Loss: -2.1042
Policy update: Rollout 25/100, Loss: -7.0907
Policy update: Rollout 30/100, Loss: 3.0809
Policy update: Rollout 35/100, Loss: -11.9682
Policy update: Rollout 40/100, Loss: -6.6125
Policy update: Rollout 45/100, Loss: -4.1608
Policy update: Rollout 50/100, Loss: 4.1768
Policy update: Rollout 55/100, Loss: 2.1212
Policy update: Rollout 60/100, Loss: -0.5591
Policy update: Rollout 65/100, Loss: -0.7767
Policy update: Rollout 70/100, Loss: -4.5483
Policy update: Rollout 75/100, Loss: -5.8104
Policy update: Rollout 80/100, Loss: -1.5679
Policy update: Rollout 85/100, Loss: -2.8303
Policy update: Rollout 90/100, Loss: -7.0412
Policy update: Rollout 95/100, Loss: -2.2325
Policy update: Rollout 100/100, Loss: -4.2779
Evaluation: Average reward = 163.00

Iteration 57/70
Collected 100 trajectories. Average reward: 180.59
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 0.93
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -3.4506
Policy update: Rollout 10/100, Loss: -2.4188
Policy update: Rollout 15/100, Loss: -0.9636
Policy update: Rollout 20/100, Loss: -0.7373
Policy update: Rollout 25/100, Loss: -9.1363
Policy update: Rollout 30/100, Loss: -9.8830
Policy update: Rollout 35/100, Loss: 2.3021
Policy update: Rollout 40/100, Loss: 1.2961
Policy update: Rollout 45/100, Loss: -0.2354
Policy update: Rollout 50/100, Loss: -1.8293
Policy update: Rollout 55/100, Loss: -6.1371
Policy update: Rollout 60/100, Loss: -5.1497
Policy update: Rollout 65/100, Loss: -5.8005
Policy update: Rollout 70/100, Loss: -2.5783
Policy update: Rollout 75/100, Loss: 2.8431
Policy update: Rollout 80/100, Loss: 0.6573
Policy update: Rollout 85/100, Loss: -3.1923
Policy update: Rollout 90/100, Loss: -2.7003
Policy update: Rollout 95/100, Loss: -3.8228
Policy update: Rollout 100/100, Loss: 1.6557
Evaluation: Average reward = 200.00

Iteration 58/70
Collected 100 trajectories. Average reward: 183.07
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Policy update: Rollout 5/100, Loss: -2.0909
Policy update: Rollout 10/100, Loss: -2.3752
Policy update: Rollout 15/100, Loss: -9.9998
Policy update: Rollout 20/100, Loss: 0.0772
Policy update: Rollout 25/100, Loss: 2.2229
Policy update: Rollout 30/100, Loss: -0.8249
Policy update: Rollout 35/100, Loss: -0.4043
Policy update: Rollout 40/100, Loss: -4.2656
Policy update: Rollout 45/100, Loss: -12.0420
Policy update: Rollout 50/100, Loss: -4.4955
Policy update: Rollout 55/100, Loss: -0.7979
Policy update: Rollout 60/100, Loss: -4.6352
Policy update: Rollout 65/100, Loss: -0.2804
Policy update: Rollout 70/100, Loss: 3.9897
Policy update: Rollout 75/100, Loss: 0.0410
Policy update: Rollout 80/100, Loss: -13.0457
Policy update: Rollout 85/100, Loss: -5.9100
Policy update: Rollout 90/100, Loss: -9.2603
Policy update: Rollout 95/100, Loss: -5.0736
Policy update: Rollout 100/100, Loss: -1.8577
Evaluation: Average reward = 181.80

Iteration 59/70
Collected 100 trajectories. Average reward: 172.36
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6359
Reward model training epoch 2/5, Loss: 0.6359
Reward model training epoch 3/5, Loss: 0.6359
Reward model training epoch 4/5, Loss: 0.6359
Reward model training epoch 5/5, Loss: 0.6359
Preference prediction accuracy: 0.43
Policy update: Rollout 5/100, Loss: -1.1181
Policy update: Rollout 10/100, Loss: 3.2344
Policy update: Rollout 15/100, Loss: -6.4854
Policy update: Rollout 20/100, Loss: -1.4532
Policy update: Rollout 25/100, Loss: -2.9889
Policy update: Rollout 30/100, Loss: -0.2063
Policy update: Rollout 35/100, Loss: -13.9490
Policy update: Rollout 40/100, Loss: -1.8282
Policy update: Rollout 45/100, Loss: -4.4320
Policy update: Rollout 50/100, Loss: -6.0564
Policy update: Rollout 55/100, Loss: -6.3970
Policy update: Rollout 60/100, Loss: 2.4705
Policy update: Rollout 65/100, Loss: -5.1378
Policy update: Rollout 70/100, Loss: -1.2158
Policy update: Rollout 75/100, Loss: -3.7285
Policy update: Rollout 80/100, Loss: -3.7123
Policy update: Rollout 85/100, Loss: -4.3242
Policy update: Rollout 90/100, Loss: -7.3786
Policy update: Rollout 95/100, Loss: -2.0806
Policy update: Rollout 100/100, Loss: -1.6239
Evaluation: Average reward = 144.80

Iteration 60/70
Collected 100 trajectories. Average reward: 177.49
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6932
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6965
Reward model training epoch 5/5, Loss: 0.6932
Policy update: Rollout 5/100, Loss: -14.2918
Policy update: Rollout 10/100, Loss: -3.3629
Policy update: Rollout 15/100, Loss: 1.1558
Policy update: Rollout 20/100, Loss: -3.5122
Policy update: Rollout 25/100, Loss: 1.2795
Policy update: Rollout 30/100, Loss: 0.0694
Policy update: Rollout 35/100, Loss: -1.0553
Policy update: Rollout 40/100, Loss: -8.6274
Policy update: Rollout 45/100, Loss: -7.1299
Policy update: Rollout 50/100, Loss: -2.0884
Policy update: Rollout 55/100, Loss: -13.3277
Policy update: Rollout 60/100, Loss: -5.0235
Policy update: Rollout 65/100, Loss: -11.5863
Policy update: Rollout 70/100, Loss: -10.7070
Policy update: Rollout 75/100, Loss: -2.5165
Policy update: Rollout 80/100, Loss: -9.9144
Policy update: Rollout 85/100, Loss: -5.0845
Policy update: Rollout 90/100, Loss: -2.3441
Policy update: Rollout 95/100, Loss: 1.4787
Policy update: Rollout 100/100, Loss: -3.2579
Evaluation: Average reward = 184.40

Iteration 61/70
Collected 100 trajectories. Average reward: 185.13
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6906
Reward model training epoch 2/5, Loss: 0.6906
Reward model training epoch 3/5, Loss: 0.6906
Reward model training epoch 4/5, Loss: 0.6906
Reward model training epoch 5/5, Loss: 0.6906
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 3.2270
Policy update: Rollout 10/100, Loss: 2.3719
Policy update: Rollout 15/100, Loss: 0.3764
Policy update: Rollout 20/100, Loss: 2.2450
Policy update: Rollout 25/100, Loss: -4.5638
Policy update: Rollout 30/100, Loss: 5.0661
Policy update: Rollout 35/100, Loss: -3.2577
Policy update: Rollout 40/100, Loss: -1.6498
Policy update: Rollout 45/100, Loss: -8.3451
Policy update: Rollout 50/100, Loss: -0.3955
Policy update: Rollout 55/100, Loss: 3.7166
Policy update: Rollout 60/100, Loss: 4.7426
Policy update: Rollout 65/100, Loss: 6.7592
Policy update: Rollout 70/100, Loss: 2.1418
Policy update: Rollout 75/100, Loss: -2.2825
Policy update: Rollout 80/100, Loss: -4.5929
Policy update: Rollout 85/100, Loss: -9.2821
Policy update: Rollout 90/100, Loss: 1.4084
Policy update: Rollout 95/100, Loss: 1.4813
Policy update: Rollout 100/100, Loss: -12.3845
Evaluation: Average reward = 180.60

Iteration 62/70
Collected 100 trajectories. Average reward: 185.08
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Policy update: Rollout 5/100, Loss: -0.6977
Policy update: Rollout 10/100, Loss: -5.7828
Policy update: Rollout 15/100, Loss: 0.8928
Policy update: Rollout 20/100, Loss: -5.4741
Policy update: Rollout 25/100, Loss: -6.3181
Policy update: Rollout 30/100, Loss: -0.6257
Policy update: Rollout 35/100, Loss: 0.1043
Policy update: Rollout 40/100, Loss: -3.8574
Policy update: Rollout 45/100, Loss: -3.0052
Policy update: Rollout 50/100, Loss: -2.8892
Policy update: Rollout 55/100, Loss: 0.1422
Policy update: Rollout 60/100, Loss: -8.6281
Policy update: Rollout 65/100, Loss: -5.7946
Policy update: Rollout 70/100, Loss: -1.3139
Policy update: Rollout 75/100, Loss: -2.4073
Policy update: Rollout 80/100, Loss: -4.4076
Policy update: Rollout 85/100, Loss: 0.7454
Policy update: Rollout 90/100, Loss: -2.0206
Policy update: Rollout 95/100, Loss: -4.3723
Policy update: Rollout 100/100, Loss: -3.4757
Evaluation: Average reward = 185.40

Iteration 63/70
Collected 100 trajectories. Average reward: 186.73
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: 0.3285
Policy update: Rollout 10/100, Loss: 1.8946
Policy update: Rollout 15/100, Loss: -6.0985
Policy update: Rollout 20/100, Loss: -0.8433
Policy update: Rollout 25/100, Loss: -0.4429
Policy update: Rollout 30/100, Loss: -7.7731
Policy update: Rollout 35/100, Loss: -2.6612
Policy update: Rollout 40/100, Loss: -5.8347
Policy update: Rollout 45/100, Loss: -1.9695
Policy update: Rollout 50/100, Loss: -2.2863
Policy update: Rollout 55/100, Loss: 4.8864
Policy update: Rollout 60/100, Loss: -3.7078
Policy update: Rollout 65/100, Loss: -4.0166
Policy update: Rollout 70/100, Loss: 2.3622
Policy update: Rollout 75/100, Loss: 0.1684
Policy update: Rollout 80/100, Loss: -7.6920
Policy update: Rollout 85/100, Loss: -5.1260
Policy update: Rollout 90/100, Loss: -4.2573
Policy update: Rollout 95/100, Loss: 4.2678
Policy update: Rollout 100/100, Loss: -7.1127
Evaluation: Average reward = 180.80

Iteration 64/70
Collected 100 trajectories. Average reward: 185.70
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6893
Reward model training epoch 2/5, Loss: 0.6893
Reward model training epoch 3/5, Loss: 0.6893
Reward model training epoch 4/5, Loss: 0.6893
Reward model training epoch 5/5, Loss: 0.6893
Policy update: Rollout 5/100, Loss: -1.6162
Policy update: Rollout 10/100, Loss: -4.1700
Policy update: Rollout 15/100, Loss: -4.0011
Policy update: Rollout 20/100, Loss: -3.1765
Policy update: Rollout 25/100, Loss: -1.1526
Policy update: Rollout 30/100, Loss: -4.7998
Policy update: Rollout 35/100, Loss: -9.6700
Policy update: Rollout 40/100, Loss: 1.3280
Policy update: Rollout 45/100, Loss: -18.2132
Policy update: Rollout 50/100, Loss: -2.2242
Policy update: Rollout 55/100, Loss: 0.2974
Policy update: Rollout 60/100, Loss: -2.2165
Policy update: Rollout 65/100, Loss: -4.8199
Policy update: Rollout 70/100, Loss: -0.0755
Policy update: Rollout 75/100, Loss: -5.6943
Policy update: Rollout 80/100, Loss: -10.7547
Policy update: Rollout 85/100, Loss: -1.0928
Policy update: Rollout 90/100, Loss: -0.3591
Policy update: Rollout 95/100, Loss: -5.1563
Policy update: Rollout 100/100, Loss: -7.6634
Evaluation: Average reward = 173.00

Iteration 65/70
Collected 100 trajectories. Average reward: 191.59
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 0.90
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -3.1735
Policy update: Rollout 10/100, Loss: 0.1552
Policy update: Rollout 15/100, Loss: -2.8996
Policy update: Rollout 20/100, Loss: -3.4934
Policy update: Rollout 25/100, Loss: 2.6796
Policy update: Rollout 30/100, Loss: 1.3655
Policy update: Rollout 35/100, Loss: -5.2791
Policy update: Rollout 40/100, Loss: -10.3323
Policy update: Rollout 45/100, Loss: -12.8440
Policy update: Rollout 50/100, Loss: -9.0770
Policy update: Rollout 55/100, Loss: -0.5750
Policy update: Rollout 60/100, Loss: -1.5021
Policy update: Rollout 65/100, Loss: -6.9495
Policy update: Rollout 70/100, Loss: -4.0446
Policy update: Rollout 75/100, Loss: -5.9956
Policy update: Rollout 80/100, Loss: 2.3574
Policy update: Rollout 85/100, Loss: 1.6110
Policy update: Rollout 90/100, Loss: 1.9908
Policy update: Rollout 95/100, Loss: 5.9623
Policy update: Rollout 100/100, Loss: 7.2502
Evaluation: Average reward = 200.00

Iteration 66/70
Collected 100 trajectories. Average reward: 190.81
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Policy update: Rollout 5/100, Loss: 0.7345
Policy update: Rollout 10/100, Loss: -2.2228
Policy update: Rollout 15/100, Loss: -4.5141
Policy update: Rollout 20/100, Loss: -6.9395
Policy update: Rollout 25/100, Loss: -5.5678
Policy update: Rollout 30/100, Loss: 0.9537
Policy update: Rollout 35/100, Loss: -0.9855
Policy update: Rollout 40/100, Loss: -9.7067
Policy update: Rollout 45/100, Loss: -0.3395
Policy update: Rollout 50/100, Loss: -7.9598
Policy update: Rollout 55/100, Loss: -10.0640
Policy update: Rollout 60/100, Loss: -6.9565
Policy update: Rollout 65/100, Loss: -4.4170
Policy update: Rollout 70/100, Loss: -5.2178
Policy update: Rollout 75/100, Loss: -3.0570
Policy update: Rollout 80/100, Loss: -4.4159
Policy update: Rollout 85/100, Loss: -8.3191
Policy update: Rollout 90/100, Loss: 2.4755
Policy update: Rollout 95/100, Loss: -9.9613
Policy update: Rollout 100/100, Loss: -4.2223
Evaluation: Average reward = 165.60

Iteration 67/70
Collected 100 trajectories. Average reward: 183.35
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -13.5003
Policy update: Rollout 10/100, Loss: -11.4030
Policy update: Rollout 15/100, Loss: -14.9884
Policy update: Rollout 20/100, Loss: -4.3694
Policy update: Rollout 25/100, Loss: -4.5148
Policy update: Rollout 30/100, Loss: -8.6225
Policy update: Rollout 35/100, Loss: -0.4520
Policy update: Rollout 40/100, Loss: -8.1024
Policy update: Rollout 45/100, Loss: 4.1877
Policy update: Rollout 50/100, Loss: 3.4849
Policy update: Rollout 55/100, Loss: -1.4097
Policy update: Rollout 60/100, Loss: -4.1566
Policy update: Rollout 65/100, Loss: -7.8289
Policy update: Rollout 70/100, Loss: -6.5830
Policy update: Rollout 75/100, Loss: -0.8126
Policy update: Rollout 80/100, Loss: 0.6347
Policy update: Rollout 85/100, Loss: -0.6467
Policy update: Rollout 90/100, Loss: -4.4983
Policy update: Rollout 95/100, Loss: -7.4106
Policy update: Rollout 100/100, Loss: -4.4410
Evaluation: Average reward = 200.00

Iteration 68/70
Collected 100 trajectories. Average reward: 188.33
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Policy update: Rollout 5/100, Loss: -3.7191
Policy update: Rollout 10/100, Loss: -2.8939
Policy update: Rollout 15/100, Loss: -0.2148
Policy update: Rollout 20/100, Loss: 4.9010
Policy update: Rollout 25/100, Loss: -6.3447
Policy update: Rollout 30/100, Loss: -7.9313
Policy update: Rollout 35/100, Loss: 2.5026
Policy update: Rollout 40/100, Loss: -0.2871
Policy update: Rollout 45/100, Loss: -8.8399
Policy update: Rollout 50/100, Loss: -2.5188
Policy update: Rollout 55/100, Loss: -7.0948
Policy update: Rollout 60/100, Loss: -1.7816
Policy update: Rollout 65/100, Loss: -4.6178
Policy update: Rollout 70/100, Loss: 0.8492
Policy update: Rollout 75/100, Loss: -11.6131
Policy update: Rollout 80/100, Loss: -8.0423
Policy update: Rollout 85/100, Loss: -7.3416
Policy update: Rollout 90/100, Loss: 4.4691
Policy update: Rollout 95/100, Loss: 0.9799
Policy update: Rollout 100/100, Loss: -6.2584
Evaluation: Average reward = 184.00

Iteration 69/70
Collected 100 trajectories. Average reward: 191.23
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -1.3342
Policy update: Rollout 10/100, Loss: -2.4545
Policy update: Rollout 15/100, Loss: -2.6070
Policy update: Rollout 20/100, Loss: -7.9124
Policy update: Rollout 25/100, Loss: -3.7004
Policy update: Rollout 30/100, Loss: -9.9382
Policy update: Rollout 35/100, Loss: -1.1895
Policy update: Rollout 40/100, Loss: -0.6748
Policy update: Rollout 45/100, Loss: -4.0149
Policy update: Rollout 50/100, Loss: 0.7635
Policy update: Rollout 55/100, Loss: 2.9044
Policy update: Rollout 60/100, Loss: -4.9234
Policy update: Rollout 65/100, Loss: -4.1358
Policy update: Rollout 70/100, Loss: -3.2460
Policy update: Rollout 75/100, Loss: -4.6568
Policy update: Rollout 80/100, Loss: -3.4228
Policy update: Rollout 85/100, Loss: -9.1508
Policy update: Rollout 90/100, Loss: -2.2185
Policy update: Rollout 95/100, Loss: 1.6210
Policy update: Rollout 100/100, Loss: -1.8867
Evaluation: Average reward = 195.60

Iteration 70/70
Collected 100 trajectories. Average reward: 192.61
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 0.2915
Policy update: Rollout 10/100, Loss: -11.5428
Policy update: Rollout 15/100, Loss: -0.9892
Policy update: Rollout 20/100, Loss: -1.6077
Policy update: Rollout 25/100, Loss: -0.4357
Policy update: Rollout 30/100, Loss: -4.3183
Policy update: Rollout 35/100, Loss: 1.5744
Policy update: Rollout 40/100, Loss: -5.3583
Policy update: Rollout 45/100, Loss: -11.5502
Policy update: Rollout 50/100, Loss: -1.0855
Policy update: Rollout 55/100, Loss: -0.0409
Policy update: Rollout 60/100, Loss: 2.7962
Policy update: Rollout 65/100, Loss: -3.4356
Policy update: Rollout 70/100, Loss: -3.4873
Policy update: Rollout 75/100, Loss: -10.1367
Policy update: Rollout 80/100, Loss: 1.2055
Policy update: Rollout 85/100, Loss: -9.8856
Policy update: Rollout 90/100, Loss: -3.6281
Policy update: Rollout 95/100, Loss: -9.0766
Policy update: Rollout 100/100, Loss: 6.7888
Evaluation: Average reward = 177.00
Episode 1: Reward = 128.0
Episode 2: Reward = 200.0
Episode 3: Reward = 200.0
Episode 4: Reward = 200.0
Episode 5: Reward = 200.0
Episode 6: Reward = 200.0
Episode 7: Reward = 200.0
Episode 8: Reward = 200.0
Episode 9: Reward = 200.0
Episode 10: Reward = 200.0
Episode 11: Reward = 200.0
Episode 12: Reward = 200.0
Episode 13: Reward = 200.0
Episode 14: Reward = 200.0
Episode 15: Reward = 200.0
Episode 16: Reward = 200.0
Episode 17: Reward = 178.0
Episode 18: Reward = 200.0
Episode 19: Reward = 200.0
Episode 20: Reward = 200.0
Average reward over 20 episodes: 195.30
Final evaluation: Average reward = 195.30
Training complete. Models saved.
Wrote profile results to online_rlhf.py.lprof
Timer unit: 1e-06 s

Total time: 24.1052 s
File: online_rlhf.py
Function: compute_trajectory_reward at line 25

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    25                                               @profile
    26                                               def compute_trajectory_reward(self, trajectory):
    27    212290    1482762.0      7.0      6.2          states = [step[0] for step in trajectory]
    28    212290    1316849.0      6.2      5.5          actions = [step[1] for step in trajectory]
    29                                           
    30    212290    3106612.0     14.6     12.9          states_array = np.array(states)
    31    212290     829866.0      3.9      3.4          actions_array = np.array(actions)
    32                                               
    33    212290     619033.0      2.9      2.6          states_tensor = torch.tensor(states_array, dtype=torch.float32, device=self.device)
    34    212290     482243.0      2.3      2.0          actions_tensor = torch.tensor(actions_array, dtype=torch.long, device=self.device)
    35                                           
    36    212290   15854755.0     74.7     65.8          rewards = self.reward_model(states_tensor, actions_tensor)
    37    212290     413050.0      1.9      1.7          return rewards.sum()

Total time: 67.289 s
File: online_rlhf.py
Function: select_action at line 39

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    39                                               @profile
    40                                               def select_action(self, state):
    41   1957906    6865369.0      3.5     10.2          state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)
    42   1957906   34649320.0     17.7     51.5          probs = self.policy(state_tensor)
    43   1957906   25419427.0     13.0     37.8          action = torch.multinomial(probs, 1).item()
    44   1957906     354867.0      0.2      0.5          return action, probs

Total time: 40.5587 s
File: online_rlhf.py
Function: collect_trajectories at line 46

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    46                                               @profile
    47                                               def collect_trajectories(self, num_trajectories=100, max_steps=200):
    48       194         41.0      0.2      0.0          trajectories = []
    49      8434       1313.0      0.2      0.0          for i in range(num_trajectories):
    50      8240      68104.0      8.3      0.2              state, _ = self.env.reset()
    51      8240       1721.0      0.2      0.0              trajectory = []
    52      8240        802.0      0.1      0.0              total_reward = 0
    53                                           
    54    919330     116137.0      0.1      0.3              for step in range(max_steps):
    55    917106   32427538.0     35.4     80.0                  action, _ = self.select_action(state)
    56    917106    7439404.0      8.1     18.3                  next_state, reward, done, truncated, info = self.env.step(action)
    57    917106     198885.0      0.2      0.5                  trajectory.append((state, action, reward, next_state, done))
    58    917106     121523.0      0.1      0.3                  total_reward += reward
    59    917106      84077.0      0.1      0.2                  state = next_state
    60                                           
    61    917106      96361.0      0.1      0.2                  if done or truncated:
    62      6016        713.0      0.1      0.0                      break
    63                                           
    64      8240       2044.0      0.2      0.0              trajectories.append((trajectory, total_reward))
    65       194         24.0      0.1      0.0          return trajectories

Total time: 144.502 s
File: online_rlhf.py
Function: create_preference_pairs at line 68

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    68                                               @profile
    69                                               def create_preference_pairs(self, trajectories, num_pairs=20, candidtate_multiplier=3):
    70        70         14.0      0.2      0.0          candidate_pairs = []
    71        70         19.0      0.3      0.0          num_candidates = num_pairs * candidtate_multiplier
    72                                                   
    73     63070       9238.0      0.1      0.0          for _ in range(num_candidates):
    74     63000     318343.0      5.1      0.2              idx1, idx2 = random.sample(range(len(trajectories)), 2)
    75     63000      14351.0      0.2      0.0              traj1, reward1 = trajectories[idx1]
    76     63000      10968.0      0.2      0.0              traj2, reward2 = trajectories[idx2]
    77                                           
    78     63000       8222.0      0.1      0.0              if reward1 > reward2:
    79     26203       3014.0      0.1      0.0                  preferred = 1
    80     36797       4448.0      0.1      0.0              elif reward2 > reward1:
    81     25881       2952.0      0.1      0.0                  preferred = 0
    82                                                       else: 
    83     10916      18747.0      1.7      0.0                  preferred = random.randint(0, 1)
    84                                           
    85                                                       
    86     63000  144083432.0   2287.0     99.7              uncertainty = self.estimate_uncertainty(traj1, traj2)
    87     63000      17417.0      0.3      0.0              candidate_pairs.append((traj1, traj2, preferred, uncertainty))
    88                                                       
    89                                                   # Sort by uncertainty and select the top num_pairs
    90        70       8870.0    126.7      0.0          candidate_pairs.sort(key=lambda x: x[3], reverse=True)
    91        70       1923.0     27.5      0.0          selected_pairs = [(traj1, traj2, preferred) for traj1, traj2, preferred, _ in candidate_pairs[:num_pairs]]
    92                                                   
    93        70        432.0      6.2      0.0          print(f"Selected {num_pairs} most uncertain pairs out of {num_candidates} candidates")
    94        70          7.0      0.1      0.0          return selected_pairs

Total time: 43.1469 s
File: online_rlhf.py
Function: train_reward_model at line 107

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   107                                               @profile
   108                                               def train_reward_model(self, preference_pairs, epochs=5, batch_size=32):
   109        70         22.0      0.3      0.0          epoch_losses = []
   110                                                   
   111       420        111.0      0.3      0.0          for epoch in range(epochs):
   112       350         46.0      0.1      0.0              total_loss = 0
   113       350       5585.0     16.0      0.0              indices = np.random.permutation(len(preference_pairs))
   114                                                       
   115      3850       1511.0      0.4      0.0              for i in range(0, len(preference_pairs), batch_size):
   116      3500       4096.0      1.2      0.0                  batch_indices = indices[i:i+batch_size]
   117      3500      21289.0      6.1      0.0                  batch = [preference_pairs[idx] for idx in batch_indices]
   118                                                           
   119      3500     112371.0     32.1      0.3                  self.reward_optimizer.zero_grad()
   120      3500     622371.0    177.8      1.4                  batch_loss = 0
   121                                                           
   122                                                           # Process all pairs in batch before backward pass
   123    108500      29005.0      0.3      0.1                  for traj1, traj2, preferred in batch:
   124    105000   12336065.0    117.5     28.6                      r1 = self.compute_trajectory_reward(traj1)
   125    105000   12555324.0    119.6     29.1                      r2 = self.compute_trajectory_reward(traj2)
   126    105000     212920.0      2.0      0.5                      diff = r1 - r2
   127    105000     172253.0      1.6      0.4                      prob = torch.sigmoid(diff)
   128    105000     290069.0      2.8      0.7                      target = torch.tensor(float(preferred), dtype=torch.float32, device=self.device)
   129    105000    1290624.0     12.3      3.0                      loss = self.reward_loss(prob.unsqueeze(0), target.unsqueeze(0))
   130    105000     479825.0      4.6      1.1                      batch_loss += loss / len(batch)
   131                                                           
   132      3500   14299341.0   4085.5     33.1                  batch_loss.backward()
   133      3500     708903.0    202.5      1.6                  self.reward_optimizer.step()
   134      3500       2986.0      0.9      0.0                  total_loss += batch_loss.item() * len(batch)
   135                                                       
   136       350        142.0      0.4      0.0              avg_loss = total_loss / len(preference_pairs)
   137       350         94.0      0.3      0.0              epoch_losses.append(avg_loss)
   138       350       1945.0      5.6      0.0              print(f"Reward model training epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
   139                                                   
   140        70         22.0      0.3      0.0          return epoch_losses[-1]

Total time: 100.766 s
File: online_rlhf.py
Function: update_policy at line 142

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   142                                               @profile
   143                                               def update_policy(self, num_rollouts=10, gamma=0.99):
   144        70         10.0      0.1      0.0          total_loss = 0
   145                                                   
   146      7070       2127.0      0.3      0.0          for i in range(num_rollouts):
   147      7000      64598.0      9.2      0.1              state, _ = self.env.reset()
   148      7000       1145.0      0.2      0.0              done = False
   149      7000     307266.0     43.9      0.3              log_probs = []
   150      7000     110930.0     15.8      0.1              states = []
   151      7000       3397.0      0.5      0.0              actions = []
   152                                           
   153   1036894     154462.0      0.1      0.2              while not done:
   154   1036894   40951155.0     39.5     40.6                  action, probs = self.select_action(state)
   155   1036894    4201399.0      4.1      4.2                  log_prob = torch.log(probs[0, action])
   156   1036894    9679709.0      9.3      9.6                  next_state, _, done, truncated, _ = self.env.step(action)
   157                                                           
   158   1036894     253266.0      0.2      0.3                  log_probs.append(log_prob)
   159   1036894     169939.0      0.2      0.2                  states.append(state)
   160   1036894     153533.0      0.1      0.2                  actions.append(action)
   161   1036894     112281.0      0.1      0.1                  state = next_state
   162                                           
   163   1036894     134188.0      0.1      0.1                  if truncated or done:
   164      6751        892.0      0.1      0.0                      break
   165                                           
   166      7000     704230.0    100.6      0.7              states_tensor = torch.tensor(states, dtype=torch.float32, device=self.device)
   167      7000      59300.0      8.5      0.1              actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)
   168                                           
   169      7000      27131.0      3.9      0.0              with torch.no_grad():
   170      7000     669909.0     95.7      0.7                  rewards = self.reward_model(states_tensor, actions_tensor)
   171      7000     438474.0     62.6      0.4              discounted_rewards = self.calculate_discounted_rewards(rewards, gamma)
   172      7000    6884817.0    983.5      6.8              policy_loss = sum(-log_prob * R for log_prob, R in zip(log_probs, discounted_rewards))
   173      7000     231293.0     33.0      0.2              self.policy_optimizer.zero_grad()
   174      7000   34589128.0   4941.3     34.3              policy_loss.backward()
   175      7000     846001.0    120.9      0.8              self.policy_optimizer.step()
   176                                                       
   177      7000       3671.0      0.5      0.0              loss_value = policy_loss.item()
   178      7000       1845.0      0.3      0.0              total_loss += loss_value
   179                                                           
   180      7000       2351.0      0.3      0.0              if (i + 1) % 5 == 0:
   181      1400       7228.0      5.2      0.0                  print(f"Policy update: Rollout {i+1}/{num_rollouts}, Loss: {loss_value:.4f}")
   182                                                   
   183        70         28.0      0.4      0.0          return total_loss / num_rollouts

Total time: 143.42 s
File: online_rlhf.py
Function: estimate_uncertainty at line 274

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   274                                               @profile
   275                                               def estimate_uncertainty(self, traj1, traj2, n_samples=10):
   276                                                   # Extract states and actions once before the loop
   277     63000     404771.0      6.4      0.3          states1 = [step[0] for step in traj1]
   278     63000     367093.0      5.8      0.3          actions1 = [step[1] for step in traj1]
   279     63000     404605.0      6.4      0.3          states2 = [step[0] for step in traj2]
   280     63000     351332.0      5.6      0.2          actions2 = [step[1] for step in traj2]
   281                                                   
   282                                                   # Convert to tensors once before the loop
   283     63000    1090930.0     17.3      0.8          states1_tensor = torch.tensor(np.array(states1), dtype=torch.float32, device=self.device)
   284     63000     384251.0      6.1      0.3          actions1_tensor = torch.tensor(np.array(actions1), dtype=torch.long, device=self.device)
   285     63000    1008656.0     16.0      0.7          states2_tensor = torch.tensor(np.array(states2), dtype=torch.float32, device=self.device)
   286     63000     369799.0      5.9      0.3          actions2_tensor = torch.tensor(np.array(actions2), dtype=torch.long, device=self.device)
   287                                                   
   288                                                   # OPTIMIZATION: Batch the MC dropout samples instead of doing them in a loop
   289     63000    1357010.0     21.5      0.9          self.reward_model.enable_dropout()
   290                                                   
   291     63000     130938.0      2.1      0.1          with torch.no_grad():
   292                                                       # Stack the same inputs n_samples times
   293     63000     289276.0      4.6      0.2              states1_repeated = states1_tensor.repeat(n_samples, 1, 1)
   294     63000     177679.0      2.8      0.1              actions1_repeated = actions1_tensor.repeat(n_samples, 1)
   295     63000     237088.0      3.8      0.2              states2_repeated = states2_tensor.repeat(n_samples, 1, 1)
   296     63000     171876.0      2.7      0.1              actions2_repeated = actions2_tensor.repeat(n_samples, 1)
   297                                                       
   298                                                       # Calculate rewards for all samples at once
   299    252000   67394507.0    267.4     47.0              r1_samples = self.reward_model(states1_repeated.view(-1, states1_tensor.size(-1)), 
   300    189000      61922.0      0.3      0.0                                          actions1_repeated.view(-1)).view(n_samples, -1).sum(dim=1)
   301    252000   67129265.0    266.4     46.8              r2_samples = self.reward_model(states2_repeated.view(-1, states2_tensor.size(-1)), 
   302    189000      68467.0      0.4      0.0                                          actions2_repeated.view(-1)).view(n_samples, -1).sum(dim=1)
   303                                                       
   304                                                       # Calculate differences and probabilities
   305     63000     150185.0      2.4      0.1              diff_samples = r1_samples - r2_samples
   306                                                   
   307     63000      63702.0      1.0      0.0          probs = torch.sigmoid(diff_samples)
   308     63000     354317.0      5.6      0.2          uncertainty = probs.var().item()
   309     63000    1443080.0     22.9      1.0          self.reward_model.eval()
   310     63000       9266.0      0.1      0.0          return uncertainty

Total time: 348.662 s
File: online_rlhf.py
Function: train at line 313

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   313                                               @profile
   314                                               def train(self, iterations=20, trajectories_per_iter=200, preference_pairs=500, 
   315                                                         reward_epochs=50, policy_rollouts=20, use_uncertainty=True):
   316         1          0.0      0.0      0.0          iteration_numbers = []
   317         1          0.0      0.0      0.0          eval_rewards = []
   318         1          0.0      0.0      0.0          reward_model_losses = []
   319         1          1.0      1.0      0.0          accuracy_history = []
   320                                           
   321         1          0.0      0.0      0.0          true_rewards_data = []
   322         1          0.0      0.0      0.0          predicted_rewards_data = []
   323                                           
   324         1          0.0      0.0      0.0          uncertainty_comparison_data = []
   325                                                   
   326        71         13.0      0.2      0.0          for iter in range(iterations):
   327        70         55.0      0.8      0.0              print(f"\nIteration {iter+1}/{iterations}")
   328                                                       
   329                                                       # Collect trajectories
   330        70   34423935.0 491770.5      9.9              trajectories = self.collect_trajectories(num_trajectories=trajectories_per_iter)
   331        70       2060.0     29.4      0.0              avg_reward = np.mean([r for _, r in trajectories])
   332        70        307.0      4.4      0.0              print(f"Collected {len(trajectories)} trajectories. Average reward: {avg_reward:.2f}")
   333                                           
   334                                                       # if iter == 0 or iter % 10 == 0 or iter == iterations - 1:
   335                                                       #     random_pairs, uncertain_pairs = self.compare_sampling_strategies(
   336                                                       #         trajectories, num_pairs=min(50, preference_pairs))
   337                                                           
   338                                                       #     # Store data for later analysis
   339                                                       #     random_avg = np.mean([self.estimate_uncertainty(t1, t2) for t1, t2, _ in random_pairs])
   340                                                       #     active_avg = np.mean([self.estimate_uncertainty(t1, t2) for t1, t2, _ in uncertain_pairs])
   341                                                       #     uncertainty_comparison_data.append((iter+1, random_avg, active_avg))
   342                                           
   343        70          5.0      0.1      0.0              if use_uncertainty:
   344        70  144669556.0    2e+06     41.5                  pairs = self.create_preference_pairs(trajectories, num_pairs=preference_pairs)
   345                                                       else:
   346                                                           pairs = []
   347                                                           for _ in range(preference_pairs):
   348                                                               idx1, idx2 = random.sample(range(len(trajectories)), 2)
   349                                                               traj1, reward1 = trajectories[idx1]
   350                                                               traj2, reward2 = trajectories[idx2]
   351                                                               if reward1 > reward2:
   352                                                                   preferred = 1
   353                                                               elif reward2 > reward1:
   354                                                                   preferred = 0
   355                                                               else: 
   356                                                                   preferred = random.randint(0, 1)
   357                                                           pairs.append((traj1, traj2, preferred))
   358                                           
   359        70         90.0      1.3      0.0              print(f"Created {len(pairs)} preference pairs")
   360                                           
   361        70   43317960.0 618828.0     12.4              reward_loss = self.train_reward_model(pairs, epochs=reward_epochs)
   362        70         29.0      0.4      0.0              reward_model_losses.append(reward_loss)
   363                                                       
   364                                                       # Calculate preference prediction accuracy
   365        70         38.0      0.5      0.0              if iter % 2 == 0:  # Calculate every other iteration to save time
   366        35    4245830.0 121309.4      1.2                  accuracy = self.evaluate_preference_accuracy(num_test_pairs=30)
   367        35         19.0      0.5      0.0                  accuracy_history.append(accuracy)
   368                                           
   369        70         44.0      0.6      0.0              if iter % 4 == 0 or iter == iterations - 1:
   370        19    1034001.0  54421.1      0.3                  correlation_trajectories = self.collect_trajectories(num_trajectories=10)
   371                                                           
   372       209        291.0      1.4      0.0                  for trajectory, true_reward in correlation_trajectories:
   373                                                               # Calculate predicted reward
   374       190        435.0      2.3      0.0                      with torch.no_grad():
   375       190      20079.0    105.7      0.0                          predicted_reward = self.compute_trajectory_reward(trajectory).item()
   376                                                               
   377       190         59.0      0.3      0.0                      true_rewards_data.append(true_reward)
   378       190         41.0      0.2      0.0                      predicted_rewards_data.append(predicted_reward)
   379                                                           
   380        19         45.0      2.4      0.0                  print(f"Collected {len(correlation_trajectories)} trajectories for reward correlation")
   381                                           
   382                                                       # Update policy
   383        70  102219308.0    1e+06     29.3              self.update_policy(num_rollouts=policy_rollouts)
   384                                           
   385                                                       # Evaluate current policy
   386        70    1981386.0  28305.5      0.6              eval_trajectories = self.collect_trajectories(num_trajectories=5)
   387        70       2026.0     28.9      0.0              eval_reward = np.mean([r for _, r in eval_trajectories])
   388        70        917.0     13.1      0.0              print(f"Evaluation: Average reward = {eval_reward:.2f}")
   389                                                       
   390                                                       # Store metrics
   391        70         30.0      0.4      0.0              iteration_numbers.append(iter + 1)
   392        70         18.0      0.3      0.0              eval_rewards.append(eval_reward)
   393                                                   
   394         1          5.0      5.0      0.0          self.env.close()
   395                                           
   396                                                       # After training completes, visualize uncertainty evolution
   397         1          0.0      0.0      0.0          if uncertainty_comparison_data:
   398                                                       iterations_compared, random_avgs, active_avgs = zip(*uncertainty_comparison_data)
   399                                                       
   400                                                       plt.figure(figsize=(10, 6))
   401                                                       plt.plot(iterations_compared, random_avgs, 'r-o', label='Random Sampling')
   402                                                       plt.plot(iterations_compared, active_avgs, 'b-o', label='Uncertainty Sampling')
   403                                                       plt.title('Evolution of Uncertainty During Training')
   404                                                       plt.xlabel('Iteration')
   405                                                       plt.ylabel('Average Uncertainty')
   406                                                       plt.legend()
   407                                                       plt.grid(True)
   408                                                       plt.tight_layout()
   409                                                       plt.savefig('uncertainty_evolution.png')
   410                                                       plt.show()
   411                                                   
   412                                                   # Create reward evolution visualization
   413         1    9230207.0    9e+06      2.6          self.visualize_reward_evolution(iteration_numbers, eval_rewards)
   414                                                   
   415                                                   # Create reward model loss visualization
   416         1    2548066.0    3e+06      0.7          self.visualize_reward_model_loss(iteration_numbers, reward_model_losses)
   417                                           
   418                                                   # Create reward correlation visualization
   419         1    2316920.0    2e+06      0.7          self.visualize_reward_correlation(true_rewards_data, predicted_rewards_data)
   420                                                   
   421                                                   # Plot preference prediction accuracy
   422         1      16168.0  16168.0      0.0          plt.figure(figsize=(10, 6))
   423         1      12417.0  12417.0      0.0          plt.plot(range(1, len(accuracy_history)+1), accuracy_history, 'g-o', linewidth=2)
   424         1        145.0    145.0      0.0          plt.title('Preference Prediction Accuracy')
   425         1         40.0     40.0      0.0          plt.xlabel('Evaluation')
   426         1         36.0     36.0      0.0          plt.ylabel('Accuracy')
   427         1        278.0    278.0      0.0          plt.grid(True)
   428         1        463.0    463.0      0.0          plt.ylim(0, 1)
   429         1      24796.0  24796.0      0.0          plt.tight_layout()
   430         1      33210.0  33210.0      0.0          plt.savefig('preference_accuracy.png')
   431         1    2560274.0    3e+06      0.7          plt.show()
   432                                                   
   433         1          3.0      3.0      0.0          return self.policy, self.reward_model

