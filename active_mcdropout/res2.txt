Using device: cpu

Iteration 1/70
Collected 100 trajectories. Average reward: 20.08
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.3592
Reward model training epoch 2/5, Loss: 0.3316
Reward model training epoch 3/5, Loss: 0.3054
Reward model training epoch 4/5, Loss: 0.2829
Reward model training epoch 5/5, Loss: 0.2624
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 0.0126
Policy update: Rollout 10/100, Loss: -0.2261
Policy update: Rollout 15/100, Loss: 0.0963
Policy update: Rollout 20/100, Loss: 0.8083
Policy update: Rollout 25/100, Loss: 0.1287
Policy update: Rollout 30/100, Loss: -0.0809
Policy update: Rollout 35/100, Loss: 0.1851
Policy update: Rollout 40/100, Loss: 0.2937
Policy update: Rollout 45/100, Loss: -0.0371
Policy update: Rollout 50/100, Loss: 0.1712
Policy update: Rollout 55/100, Loss: 0.4362
Policy update: Rollout 60/100, Loss: 0.0939
Policy update: Rollout 65/100, Loss: -0.0986
Policy update: Rollout 70/100, Loss: 0.1682
Policy update: Rollout 75/100, Loss: -0.3561
Policy update: Rollout 80/100, Loss: -0.1963
Policy update: Rollout 85/100, Loss: 0.0935
Policy update: Rollout 90/100, Loss: 0.6487
Policy update: Rollout 95/100, Loss: 0.1247
Policy update: Rollout 100/100, Loss: 0.2060
Evaluation: Average reward = 18.00

Iteration 2/70
Collected 100 trajectories. Average reward: 20.71
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.2315
Reward model training epoch 2/5, Loss: 0.2213
Reward model training epoch 3/5, Loss: 0.2165
Reward model training epoch 4/5, Loss: 0.2138
Reward model training epoch 5/5, Loss: 0.2123
Policy update: Rollout 5/100, Loss: 0.1048
Policy update: Rollout 10/100, Loss: -0.1343
Policy update: Rollout 15/100, Loss: 0.1261
Policy update: Rollout 20/100, Loss: -0.1439
Policy update: Rollout 25/100, Loss: 0.0979
Policy update: Rollout 30/100, Loss: -0.2186
Policy update: Rollout 35/100, Loss: -0.3355
Policy update: Rollout 40/100, Loss: -0.0983
Policy update: Rollout 45/100, Loss: -0.0239
Policy update: Rollout 50/100, Loss: 0.1264
Policy update: Rollout 55/100, Loss: -0.4307
Policy update: Rollout 60/100, Loss: -0.0520
Policy update: Rollout 65/100, Loss: -0.1916
Policy update: Rollout 70/100, Loss: 0.4119
Policy update: Rollout 75/100, Loss: -0.2101
Policy update: Rollout 80/100, Loss: 0.5421
Policy update: Rollout 85/100, Loss: -0.1749
Policy update: Rollout 90/100, Loss: -0.1240
Policy update: Rollout 95/100, Loss: -0.1211
Policy update: Rollout 100/100, Loss: 0.2493
Evaluation: Average reward = 25.20

Iteration 3/70
Collected 100 trajectories. Average reward: 21.68
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.2609
Reward model training epoch 2/5, Loss: 0.2594
Reward model training epoch 3/5, Loss: 0.2588
Reward model training epoch 4/5, Loss: 0.2584
Reward model training epoch 5/5, Loss: 0.2582
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -0.1593
Policy update: Rollout 10/100, Loss: 0.5715
Policy update: Rollout 15/100, Loss: -0.2689
Policy update: Rollout 20/100, Loss: -0.1086
Policy update: Rollout 25/100, Loss: 0.0661
Policy update: Rollout 30/100, Loss: -0.1320
Policy update: Rollout 35/100, Loss: 0.2923
Policy update: Rollout 40/100, Loss: 0.0628
Policy update: Rollout 45/100, Loss: 0.1675
Policy update: Rollout 50/100, Loss: -0.1002
Policy update: Rollout 55/100, Loss: 0.2862
Policy update: Rollout 60/100, Loss: -0.1020
Policy update: Rollout 65/100, Loss: 0.2067
Policy update: Rollout 70/100, Loss: 0.1613
Policy update: Rollout 75/100, Loss: -0.0959
Policy update: Rollout 80/100, Loss: -0.0744
Policy update: Rollout 85/100, Loss: 0.1949
Policy update: Rollout 90/100, Loss: -0.1268
Policy update: Rollout 95/100, Loss: 0.1123
Policy update: Rollout 100/100, Loss: 0.0611
Evaluation: Average reward = 23.80

Iteration 4/70
Collected 100 trajectories. Average reward: 22.32
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1580
Reward model training epoch 2/5, Loss: 0.1578
Reward model training epoch 3/5, Loss: 0.1575
Reward model training epoch 4/5, Loss: 0.1571
Reward model training epoch 5/5, Loss: 0.1565
Policy update: Rollout 5/100, Loss: 0.0692
Policy update: Rollout 10/100, Loss: 0.1575
Policy update: Rollout 15/100, Loss: -0.5263
Policy update: Rollout 20/100, Loss: 0.1721
Policy update: Rollout 25/100, Loss: 0.0832
Policy update: Rollout 30/100, Loss: 0.0694
Policy update: Rollout 35/100, Loss: 0.0172
Policy update: Rollout 40/100, Loss: -0.0824
Policy update: Rollout 45/100, Loss: 0.1265
Policy update: Rollout 50/100, Loss: 0.4747
Policy update: Rollout 55/100, Loss: 0.1803
Policy update: Rollout 60/100, Loss: 0.0122
Policy update: Rollout 65/100, Loss: -0.4854
Policy update: Rollout 70/100, Loss: 0.1267
Policy update: Rollout 75/100, Loss: 0.1378
Policy update: Rollout 80/100, Loss: 0.0074
Policy update: Rollout 85/100, Loss: 0.0135
Policy update: Rollout 90/100, Loss: 0.1208
Policy update: Rollout 95/100, Loss: 0.5115
Policy update: Rollout 100/100, Loss: -0.1913
Evaluation: Average reward = 19.00

Iteration 5/70
Collected 100 trajectories. Average reward: 28.43
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1256
Reward model training epoch 2/5, Loss: 0.1253
Reward model training epoch 3/5, Loss: 0.1249
Reward model training epoch 4/5, Loss: 0.1248
Reward model training epoch 5/5, Loss: 0.1247
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -0.6575
Policy update: Rollout 10/100, Loss: -0.3169
Policy update: Rollout 15/100, Loss: 0.1554
Policy update: Rollout 20/100, Loss: 0.4045
Policy update: Rollout 25/100, Loss: 0.0367
Policy update: Rollout 30/100, Loss: 0.0155
Policy update: Rollout 35/100, Loss: 0.0151
Policy update: Rollout 40/100, Loss: -0.2071
Policy update: Rollout 45/100, Loss: -0.4142
Policy update: Rollout 50/100, Loss: 0.2019
Policy update: Rollout 55/100, Loss: 0.1251
Policy update: Rollout 60/100, Loss: 0.0644
Policy update: Rollout 65/100, Loss: -0.3220
Policy update: Rollout 70/100, Loss: -0.2444
Policy update: Rollout 75/100, Loss: 0.0479
Policy update: Rollout 80/100, Loss: -0.1898
Policy update: Rollout 85/100, Loss: 0.2451
Policy update: Rollout 90/100, Loss: 0.1219
Policy update: Rollout 95/100, Loss: 0.0009
Policy update: Rollout 100/100, Loss: 0.1451
Evaluation: Average reward = 31.60

Iteration 6/70
Collected 100 trajectories. Average reward: 25.41
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1821
Reward model training epoch 2/5, Loss: 0.1820
Reward model training epoch 3/5, Loss: 0.1820
Reward model training epoch 4/5, Loss: 0.1820
Reward model training epoch 5/5, Loss: 0.1820
Policy update: Rollout 5/100, Loss: 0.2115
Policy update: Rollout 10/100, Loss: -0.1632
Policy update: Rollout 15/100, Loss: -0.0415
Policy update: Rollout 20/100, Loss: 0.0903
Policy update: Rollout 25/100, Loss: 0.2455
Policy update: Rollout 30/100, Loss: 0.2076
Policy update: Rollout 35/100, Loss: 0.0111
Policy update: Rollout 40/100, Loss: -0.2576
Policy update: Rollout 45/100, Loss: -0.0549
Policy update: Rollout 50/100, Loss: -0.9205
Policy update: Rollout 55/100, Loss: -0.3716
Policy update: Rollout 60/100, Loss: -0.1945
Policy update: Rollout 65/100, Loss: 0.0525
Policy update: Rollout 70/100, Loss: 0.0073
Policy update: Rollout 75/100, Loss: -0.5337
Policy update: Rollout 80/100, Loss: -0.0585
Policy update: Rollout 85/100, Loss: 0.0716
Policy update: Rollout 90/100, Loss: -0.7867
Policy update: Rollout 95/100, Loss: -0.2230
Policy update: Rollout 100/100, Loss: -0.0108
Evaluation: Average reward = 18.00

Iteration 7/70
Collected 100 trajectories. Average reward: 24.52
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1593
Reward model training epoch 2/5, Loss: 0.1592
Reward model training epoch 3/5, Loss: 0.1592
Reward model training epoch 4/5, Loss: 0.1592
Reward model training epoch 5/5, Loss: 0.1591
Preference prediction accuracy: 0.97
Policy update: Rollout 5/100, Loss: 0.6760
Policy update: Rollout 10/100, Loss: -0.4310
Policy update: Rollout 15/100, Loss: -0.6693
Policy update: Rollout 20/100, Loss: 0.3670
Policy update: Rollout 25/100, Loss: 0.4746
Policy update: Rollout 30/100, Loss: -0.5770
Policy update: Rollout 35/100, Loss: -0.0433
Policy update: Rollout 40/100, Loss: 0.1139
Policy update: Rollout 45/100, Loss: -0.2087
Policy update: Rollout 50/100, Loss: 0.0407
Policy update: Rollout 55/100, Loss: -0.1761
Policy update: Rollout 60/100, Loss: -0.0354
Policy update: Rollout 65/100, Loss: -0.1867
Policy update: Rollout 70/100, Loss: 0.9002
Policy update: Rollout 75/100, Loss: 0.2619
Policy update: Rollout 80/100, Loss: -0.5675
Policy update: Rollout 85/100, Loss: -0.6630
Policy update: Rollout 90/100, Loss: 0.6249
Policy update: Rollout 95/100, Loss: 0.0367
Policy update: Rollout 100/100, Loss: 0.3184
Evaluation: Average reward = 28.20

Iteration 8/70
Collected 100 trajectories. Average reward: 26.38
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1397
Reward model training epoch 2/5, Loss: 0.1396
Reward model training epoch 3/5, Loss: 0.1396
Reward model training epoch 4/5, Loss: 0.1395
Reward model training epoch 5/5, Loss: 0.1395
Policy update: Rollout 5/100, Loss: -0.0665
Policy update: Rollout 10/100, Loss: 0.2532
Policy update: Rollout 15/100, Loss: -0.0828
Policy update: Rollout 20/100, Loss: -1.7812
Policy update: Rollout 25/100, Loss: -0.0588
Policy update: Rollout 30/100, Loss: 0.3786
Policy update: Rollout 35/100, Loss: -1.0524
Policy update: Rollout 40/100, Loss: -0.0707
Policy update: Rollout 45/100, Loss: -0.3955
Policy update: Rollout 50/100, Loss: 0.6725
Policy update: Rollout 55/100, Loss: -0.1510
Policy update: Rollout 60/100, Loss: -0.4213
Policy update: Rollout 65/100, Loss: -0.2135
Policy update: Rollout 70/100, Loss: -0.3427
Policy update: Rollout 75/100, Loss: 0.5551
Policy update: Rollout 80/100, Loss: -0.7053
Policy update: Rollout 85/100, Loss: -0.0700
Policy update: Rollout 90/100, Loss: 0.2554
Policy update: Rollout 95/100, Loss: -0.5898
Policy update: Rollout 100/100, Loss: -0.6663
Evaluation: Average reward = 22.00

Iteration 9/70
Collected 100 trajectories. Average reward: 27.29
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1194
Reward model training epoch 2/5, Loss: 0.1194
Reward model training epoch 3/5, Loss: 0.1194
Reward model training epoch 4/5, Loss: 0.1194
Reward model training epoch 5/5, Loss: 0.1194
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 0.1256
Policy update: Rollout 10/100, Loss: -1.2474
Policy update: Rollout 15/100, Loss: -1.0788
Policy update: Rollout 20/100, Loss: -0.2206
Policy update: Rollout 25/100, Loss: 1.0328
Policy update: Rollout 30/100, Loss: 0.2797
Policy update: Rollout 35/100, Loss: 0.2583
Policy update: Rollout 40/100, Loss: 0.5738
Policy update: Rollout 45/100, Loss: 0.1785
Policy update: Rollout 50/100, Loss: 0.0859
Policy update: Rollout 55/100, Loss: 0.2280
Policy update: Rollout 60/100, Loss: 0.3955
Policy update: Rollout 65/100, Loss: -0.3822
Policy update: Rollout 70/100, Loss: 0.7763
Policy update: Rollout 75/100, Loss: 0.3113
Policy update: Rollout 80/100, Loss: -0.0506
Policy update: Rollout 85/100, Loss: 0.0303
Policy update: Rollout 90/100, Loss: 0.3017
Policy update: Rollout 95/100, Loss: -1.0477
Policy update: Rollout 100/100, Loss: -0.4350
Evaluation: Average reward = 25.60

Iteration 10/70
Collected 100 trajectories. Average reward: 30.94
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1038
Reward model training epoch 2/5, Loss: 0.1038
Reward model training epoch 3/5, Loss: 0.1038
Reward model training epoch 4/5, Loss: 0.1038
Reward model training epoch 5/5, Loss: 0.1038
Policy update: Rollout 5/100, Loss: -0.0101
Policy update: Rollout 10/100, Loss: -1.6248
Policy update: Rollout 15/100, Loss: -0.4799
Policy update: Rollout 20/100, Loss: -0.8107
Policy update: Rollout 25/100, Loss: -1.1469
Policy update: Rollout 30/100, Loss: -0.0663
Policy update: Rollout 35/100, Loss: 0.7999
Policy update: Rollout 40/100, Loss: 0.3756
Policy update: Rollout 45/100, Loss: -0.3231
Policy update: Rollout 50/100, Loss: -2.3511
Policy update: Rollout 55/100, Loss: 0.5368
Policy update: Rollout 60/100, Loss: 0.7695
Policy update: Rollout 65/100, Loss: 0.2348
Policy update: Rollout 70/100, Loss: -0.4075
Policy update: Rollout 75/100, Loss: -0.7812
Policy update: Rollout 80/100, Loss: 0.4609
Policy update: Rollout 85/100, Loss: 0.2890
Policy update: Rollout 90/100, Loss: 0.7812
Policy update: Rollout 95/100, Loss: 0.2807
Policy update: Rollout 100/100, Loss: -0.8258
Evaluation: Average reward = 25.00

Iteration 11/70
Collected 100 trajectories. Average reward: 30.24
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1361
Reward model training epoch 2/5, Loss: 0.1361
Reward model training epoch 3/5, Loss: 0.1361
Reward model training epoch 4/5, Loss: 0.1361
Reward model training epoch 5/5, Loss: 0.1360
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -0.1057
Policy update: Rollout 10/100, Loss: 0.4207
Policy update: Rollout 15/100, Loss: 0.3740
Policy update: Rollout 20/100, Loss: -0.3056
Policy update: Rollout 25/100, Loss: 0.4646
Policy update: Rollout 30/100, Loss: -0.8537
Policy update: Rollout 35/100, Loss: -0.0791
Policy update: Rollout 40/100, Loss: -1.7840
Policy update: Rollout 45/100, Loss: 0.5465
Policy update: Rollout 50/100, Loss: 0.3611
Policy update: Rollout 55/100, Loss: -1.2415
Policy update: Rollout 60/100, Loss: -0.2346
Policy update: Rollout 65/100, Loss: -0.0420
Policy update: Rollout 70/100, Loss: -2.5302
Policy update: Rollout 75/100, Loss: -1.0526
Policy update: Rollout 80/100, Loss: -0.8277
Policy update: Rollout 85/100, Loss: -1.7328
Policy update: Rollout 90/100, Loss: -0.2447
Policy update: Rollout 95/100, Loss: 0.4854
Policy update: Rollout 100/100, Loss: -3.8678
Evaluation: Average reward = 35.80

Iteration 12/70
Collected 100 trajectories. Average reward: 31.33
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1097
Reward model training epoch 2/5, Loss: 0.1097
Reward model training epoch 3/5, Loss: 0.1097
Reward model training epoch 4/5, Loss: 0.1097
Reward model training epoch 5/5, Loss: 0.1097
Policy update: Rollout 5/100, Loss: -0.1943
Policy update: Rollout 10/100, Loss: 0.0461
Policy update: Rollout 15/100, Loss: 0.6915
Policy update: Rollout 20/100, Loss: -0.0765
Policy update: Rollout 25/100, Loss: 0.6699
Policy update: Rollout 30/100, Loss: -1.1287
Policy update: Rollout 35/100, Loss: -1.2602
Policy update: Rollout 40/100, Loss: -0.4379
Policy update: Rollout 45/100, Loss: -0.3127
Policy update: Rollout 50/100, Loss: 0.3374
Policy update: Rollout 55/100, Loss: 1.7762
Policy update: Rollout 60/100, Loss: -1.6529
Policy update: Rollout 65/100, Loss: -1.8194
Policy update: Rollout 70/100, Loss: 0.9497
Policy update: Rollout 75/100, Loss: 0.4908
Policy update: Rollout 80/100, Loss: -0.6314
Policy update: Rollout 85/100, Loss: 1.4837
Policy update: Rollout 90/100, Loss: 0.3213
Policy update: Rollout 95/100, Loss: -1.1061
Policy update: Rollout 100/100, Loss: 0.5736
Evaluation: Average reward = 32.00

Iteration 13/70
Collected 100 trajectories. Average reward: 33.35
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0960
Reward model training epoch 2/5, Loss: 0.0960
Reward model training epoch 3/5, Loss: 0.0960
Reward model training epoch 4/5, Loss: 0.0960
Reward model training epoch 5/5, Loss: 0.0960
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -0.1089
Policy update: Rollout 10/100, Loss: 1.5526
Policy update: Rollout 15/100, Loss: 0.2508
Policy update: Rollout 20/100, Loss: 0.0389
Policy update: Rollout 25/100, Loss: 0.4136
Policy update: Rollout 30/100, Loss: -0.7524
Policy update: Rollout 35/100, Loss: 0.8431
Policy update: Rollout 40/100, Loss: 0.9844
Policy update: Rollout 45/100, Loss: -0.0721
Policy update: Rollout 50/100, Loss: -0.0301
Policy update: Rollout 55/100, Loss: -0.6736
Policy update: Rollout 60/100, Loss: 0.0796
Policy update: Rollout 65/100, Loss: -0.0951
Policy update: Rollout 70/100, Loss: 0.4956
Policy update: Rollout 75/100, Loss: 0.8148
Policy update: Rollout 80/100, Loss: -1.0035
Policy update: Rollout 85/100, Loss: 0.0666
Policy update: Rollout 90/100, Loss: -1.2929
Policy update: Rollout 95/100, Loss: -0.7405
Policy update: Rollout 100/100, Loss: 0.2953
Evaluation: Average reward = 23.00

Iteration 14/70
Collected 100 trajectories. Average reward: 37.21
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0943
Reward model training epoch 2/5, Loss: 0.0943
Reward model training epoch 3/5, Loss: 0.0943
Reward model training epoch 4/5, Loss: 0.0943
Reward model training epoch 5/5, Loss: 0.0943
Policy update: Rollout 5/100, Loss: -0.4944
Policy update: Rollout 10/100, Loss: 0.3002
Policy update: Rollout 15/100, Loss: 0.7549
Policy update: Rollout 20/100, Loss: 0.0488
Policy update: Rollout 25/100, Loss: 0.3276
Policy update: Rollout 30/100, Loss: 0.4088
Policy update: Rollout 35/100, Loss: 0.6787
Policy update: Rollout 40/100, Loss: -2.0257
Policy update: Rollout 45/100, Loss: -3.2164
Policy update: Rollout 50/100, Loss: -0.3146
Policy update: Rollout 55/100, Loss: -1.2909
Policy update: Rollout 60/100, Loss: -0.0055
Policy update: Rollout 65/100, Loss: -1.3998
Policy update: Rollout 70/100, Loss: -0.1465
Policy update: Rollout 75/100, Loss: -0.0986
Policy update: Rollout 80/100, Loss: -0.8725
Policy update: Rollout 85/100, Loss: -1.9794
Policy update: Rollout 90/100, Loss: 0.5844
Policy update: Rollout 95/100, Loss: -1.3525
Policy update: Rollout 100/100, Loss: 0.4721
Evaluation: Average reward = 29.00

Iteration 15/70
Collected 100 trajectories. Average reward: 39.88
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1008
Reward model training epoch 2/5, Loss: 0.1008
Reward model training epoch 3/5, Loss: 0.1008
Reward model training epoch 4/5, Loss: 0.1008
Reward model training epoch 5/5, Loss: 0.1008
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -0.6780
Policy update: Rollout 10/100, Loss: 0.1486
Policy update: Rollout 15/100, Loss: -2.1285
Policy update: Rollout 20/100, Loss: 1.1625
Policy update: Rollout 25/100, Loss: 1.1828
Policy update: Rollout 30/100, Loss: -1.3418
Policy update: Rollout 35/100, Loss: -2.9823
Policy update: Rollout 40/100, Loss: 0.5768
Policy update: Rollout 45/100, Loss: 0.3638
Policy update: Rollout 50/100, Loss: 0.9850
Policy update: Rollout 55/100, Loss: -1.1936
Policy update: Rollout 60/100, Loss: -1.2997
Policy update: Rollout 65/100, Loss: 1.0960
Policy update: Rollout 70/100, Loss: -2.2644
Policy update: Rollout 75/100, Loss: 0.1506
Policy update: Rollout 80/100, Loss: 0.8541
Policy update: Rollout 85/100, Loss: -2.8773
Policy update: Rollout 90/100, Loss: 0.9212
Policy update: Rollout 95/100, Loss: 0.7203
Policy update: Rollout 100/100, Loss: 0.1645
Evaluation: Average reward = 45.20

Iteration 16/70
Collected 100 trajectories. Average reward: 41.84
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0741
Reward model training epoch 2/5, Loss: 0.0741
Reward model training epoch 3/5, Loss: 0.0741
Reward model training epoch 4/5, Loss: 0.0741
Reward model training epoch 5/5, Loss: 0.0741
Policy update: Rollout 5/100, Loss: -0.2833
Policy update: Rollout 10/100, Loss: 0.0748
Policy update: Rollout 15/100, Loss: -1.3641
Policy update: Rollout 20/100, Loss: -1.8611
Policy update: Rollout 25/100, Loss: -0.1411
Policy update: Rollout 30/100, Loss: 0.7435
Policy update: Rollout 35/100, Loss: 1.3955
Policy update: Rollout 40/100, Loss: -2.9419
Policy update: Rollout 45/100, Loss: -0.6308
Policy update: Rollout 50/100, Loss: -1.1096
Policy update: Rollout 55/100, Loss: -1.9415
Policy update: Rollout 60/100, Loss: 0.9091
Policy update: Rollout 65/100, Loss: 0.5607
Policy update: Rollout 70/100, Loss: 0.7646
Policy update: Rollout 75/100, Loss: -1.9310
Policy update: Rollout 80/100, Loss: 0.3019
Policy update: Rollout 85/100, Loss: 0.8752
Policy update: Rollout 90/100, Loss: 0.2288
Policy update: Rollout 95/100, Loss: -0.1392
Policy update: Rollout 100/100, Loss: -3.4639
Evaluation: Average reward = 65.00

Iteration 17/70
Collected 100 trajectories. Average reward: 50.46
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0764
Reward model training epoch 2/5, Loss: 0.0764
Reward model training epoch 3/5, Loss: 0.0764
Reward model training epoch 4/5, Loss: 0.0764
Reward model training epoch 5/5, Loss: 0.0764
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -0.0664
Policy update: Rollout 10/100, Loss: 1.0728
Policy update: Rollout 15/100, Loss: -1.3726
Policy update: Rollout 20/100, Loss: -1.7615
Policy update: Rollout 25/100, Loss: -0.7951
Policy update: Rollout 30/100, Loss: 0.2274
Policy update: Rollout 35/100, Loss: -2.6455
Policy update: Rollout 40/100, Loss: -3.5781
Policy update: Rollout 45/100, Loss: -0.1683
Policy update: Rollout 50/100, Loss: 1.4226
Policy update: Rollout 55/100, Loss: -2.4144
Policy update: Rollout 60/100, Loss: 0.4210
Policy update: Rollout 65/100, Loss: -0.5577
Policy update: Rollout 70/100, Loss: -4.3934
Policy update: Rollout 75/100, Loss: 0.6258
Policy update: Rollout 80/100, Loss: -0.0662
Policy update: Rollout 85/100, Loss: 1.1603
Policy update: Rollout 90/100, Loss: -0.5035
Policy update: Rollout 95/100, Loss: -1.9529
Policy update: Rollout 100/100, Loss: 0.3196
Evaluation: Average reward = 36.40

Iteration 18/70
Collected 100 trajectories. Average reward: 50.57
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0584
Reward model training epoch 2/5, Loss: 0.0584
Reward model training epoch 3/5, Loss: 0.0584
Reward model training epoch 4/5, Loss: 0.0584
Reward model training epoch 5/5, Loss: 0.0584
Policy update: Rollout 5/100, Loss: 0.4655
Policy update: Rollout 10/100, Loss: 0.1886
Policy update: Rollout 15/100, Loss: -0.2452
Policy update: Rollout 20/100, Loss: -0.1194
Policy update: Rollout 25/100, Loss: -4.8968
Policy update: Rollout 30/100, Loss: 1.9893
Policy update: Rollout 35/100, Loss: 0.9819
Policy update: Rollout 40/100, Loss: -0.6480
Policy update: Rollout 45/100, Loss: -0.0180
Policy update: Rollout 50/100, Loss: -0.6136
Policy update: Rollout 55/100, Loss: 0.0061
Policy update: Rollout 60/100, Loss: -6.0753
Policy update: Rollout 65/100, Loss: -1.5129
Policy update: Rollout 70/100, Loss: 0.0570
Policy update: Rollout 75/100, Loss: -2.2491
Policy update: Rollout 80/100, Loss: 0.7004
Policy update: Rollout 85/100, Loss: -2.9043
Policy update: Rollout 90/100, Loss: -1.5061
Policy update: Rollout 95/100, Loss: 1.8020
Policy update: Rollout 100/100, Loss: 0.9597
Evaluation: Average reward = 33.00

Iteration 19/70
Collected 100 trajectories. Average reward: 44.97
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0607
Reward model training epoch 2/5, Loss: 0.0607
Reward model training epoch 3/5, Loss: 0.0607
Reward model training epoch 4/5, Loss: 0.0607
Reward model training epoch 5/5, Loss: 0.0607
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -0.6082
Policy update: Rollout 10/100, Loss: -0.6299
Policy update: Rollout 15/100, Loss: 0.6742
Policy update: Rollout 20/100, Loss: 1.5795
Policy update: Rollout 25/100, Loss: -0.6262
Policy update: Rollout 30/100, Loss: 0.8336
Policy update: Rollout 35/100, Loss: -1.0690
Policy update: Rollout 40/100, Loss: -0.1422
Policy update: Rollout 45/100, Loss: 1.1499
Policy update: Rollout 50/100, Loss: 0.3700
Policy update: Rollout 55/100, Loss: 0.8234
Policy update: Rollout 60/100, Loss: -1.3829
Policy update: Rollout 65/100, Loss: 0.3874
Policy update: Rollout 70/100, Loss: 0.0888
Policy update: Rollout 75/100, Loss: 0.7296
Policy update: Rollout 80/100, Loss: -0.0562
Policy update: Rollout 85/100, Loss: -3.3757
Policy update: Rollout 90/100, Loss: 0.5687
Policy update: Rollout 95/100, Loss: 1.1517
Policy update: Rollout 100/100, Loss: -2.9528
Evaluation: Average reward = 37.20

Iteration 20/70
Collected 100 trajectories. Average reward: 53.17
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0684
Reward model training epoch 2/5, Loss: 0.0684
Reward model training epoch 3/5, Loss: 0.0684
Reward model training epoch 4/5, Loss: 0.0684
Reward model training epoch 5/5, Loss: 0.0684
Policy update: Rollout 5/100, Loss: 0.1980
Policy update: Rollout 10/100, Loss: -0.1167
Policy update: Rollout 15/100, Loss: 0.4327
Policy update: Rollout 20/100, Loss: -2.0996
Policy update: Rollout 25/100, Loss: -0.0865
Policy update: Rollout 30/100, Loss: -2.1492
Policy update: Rollout 35/100, Loss: -0.7915
Policy update: Rollout 40/100, Loss: 0.5426
Policy update: Rollout 45/100, Loss: 1.8568
Policy update: Rollout 50/100, Loss: 1.4944
Policy update: Rollout 55/100, Loss: 0.8045
Policy update: Rollout 60/100, Loss: 0.4971
Policy update: Rollout 65/100, Loss: 0.0240
Policy update: Rollout 70/100, Loss: 0.7001
Policy update: Rollout 75/100, Loss: -4.1922
Policy update: Rollout 80/100, Loss: -0.4960
Policy update: Rollout 85/100, Loss: 0.4319
Policy update: Rollout 90/100, Loss: 0.6734
Policy update: Rollout 95/100, Loss: 1.5901
Policy update: Rollout 100/100, Loss: -5.5161
Evaluation: Average reward = 58.00

Iteration 21/70
Collected 100 trajectories. Average reward: 51.99
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0702
Reward model training epoch 2/5, Loss: 0.0702
Reward model training epoch 3/5, Loss: 0.0702
Reward model training epoch 4/5, Loss: 0.0702
Reward model training epoch 5/5, Loss: 0.0702
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -2.1679
Policy update: Rollout 10/100, Loss: -3.2612
Policy update: Rollout 15/100, Loss: 1.3419
Policy update: Rollout 20/100, Loss: 0.0244
Policy update: Rollout 25/100, Loss: -0.5119
Policy update: Rollout 30/100, Loss: -1.5059
Policy update: Rollout 35/100, Loss: 0.7902
Policy update: Rollout 40/100, Loss: -4.6072
Policy update: Rollout 45/100, Loss: -0.6546
Policy update: Rollout 50/100, Loss: -0.1636
Policy update: Rollout 55/100, Loss: 0.9624
Policy update: Rollout 60/100, Loss: -1.5096
Policy update: Rollout 65/100, Loss: 0.8731
Policy update: Rollout 70/100, Loss: -0.5410
Policy update: Rollout 75/100, Loss: 0.4636
Policy update: Rollout 80/100, Loss: -5.1333
Policy update: Rollout 85/100, Loss: -3.0029
Policy update: Rollout 90/100, Loss: 1.0657
Policy update: Rollout 95/100, Loss: 1.3125
Policy update: Rollout 100/100, Loss: -6.4916
Evaluation: Average reward = 48.60

Iteration 22/70
Collected 100 trajectories. Average reward: 54.58
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0480
Reward model training epoch 2/5, Loss: 0.0480
Reward model training epoch 3/5, Loss: 0.0480
Reward model training epoch 4/5, Loss: 0.0480
Reward model training epoch 5/5, Loss: 0.0480
Policy update: Rollout 5/100, Loss: 1.7870
Policy update: Rollout 10/100, Loss: 0.0783
Policy update: Rollout 15/100, Loss: 0.0061
Policy update: Rollout 20/100, Loss: -4.2820
Policy update: Rollout 25/100, Loss: 1.1939
Policy update: Rollout 30/100, Loss: -0.2323
Policy update: Rollout 35/100, Loss: 1.2978
Policy update: Rollout 40/100, Loss: -0.8136
Policy update: Rollout 45/100, Loss: -0.0629
Policy update: Rollout 50/100, Loss: -2.7517
Policy update: Rollout 55/100, Loss: -3.5039
Policy update: Rollout 60/100, Loss: -2.7911
Policy update: Rollout 65/100, Loss: 1.6269
Policy update: Rollout 70/100, Loss: 0.1471
Policy update: Rollout 75/100, Loss: -2.3409
Policy update: Rollout 80/100, Loss: -1.6939
Policy update: Rollout 85/100, Loss: -0.0420
Policy update: Rollout 90/100, Loss: 0.6659
Policy update: Rollout 95/100, Loss: -0.9471
Policy update: Rollout 100/100, Loss: -0.2377
Evaluation: Average reward = 52.20

Iteration 23/70
Collected 100 trajectories. Average reward: 64.31
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0500
Reward model training epoch 2/5, Loss: 0.0500
Reward model training epoch 3/5, Loss: 0.0500
Reward model training epoch 4/5, Loss: 0.0500
Reward model training epoch 5/5, Loss: 0.0500
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: 1.4613
Policy update: Rollout 10/100, Loss: -2.0049
Policy update: Rollout 15/100, Loss: -0.7546
Policy update: Rollout 20/100, Loss: -2.9242
Policy update: Rollout 25/100, Loss: -0.9533
Policy update: Rollout 30/100, Loss: 0.0345
Policy update: Rollout 35/100, Loss: 1.4156
Policy update: Rollout 40/100, Loss: -0.4109
Policy update: Rollout 45/100, Loss: -1.7988
Policy update: Rollout 50/100, Loss: 1.3458
Policy update: Rollout 55/100, Loss: 0.5716
Policy update: Rollout 60/100, Loss: -3.8765
Policy update: Rollout 65/100, Loss: -0.4220
Policy update: Rollout 70/100, Loss: -2.4970
Policy update: Rollout 75/100, Loss: -0.2994
Policy update: Rollout 80/100, Loss: -2.5522
Policy update: Rollout 85/100, Loss: 0.9296
Policy update: Rollout 90/100, Loss: -1.1811
Policy update: Rollout 95/100, Loss: -0.3896
Policy update: Rollout 100/100, Loss: 1.2082
Evaluation: Average reward = 93.00

Iteration 24/70
Collected 100 trajectories. Average reward: 72.31
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0384
Reward model training epoch 2/5, Loss: 0.0384
Reward model training epoch 3/5, Loss: 0.0384
Reward model training epoch 4/5, Loss: 0.0384
Reward model training epoch 5/5, Loss: 0.0384
Policy update: Rollout 5/100, Loss: -1.7138
Policy update: Rollout 10/100, Loss: -5.1239
Policy update: Rollout 15/100, Loss: -0.0340
Policy update: Rollout 20/100, Loss: 1.1805
Policy update: Rollout 25/100, Loss: 1.3337
Policy update: Rollout 30/100, Loss: -1.7310
Policy update: Rollout 35/100, Loss: -1.3078
Policy update: Rollout 40/100, Loss: -1.7806
Policy update: Rollout 45/100, Loss: -7.0134
Policy update: Rollout 50/100, Loss: 0.7514
Policy update: Rollout 55/100, Loss: 1.6458
Policy update: Rollout 60/100, Loss: 0.5662
Policy update: Rollout 65/100, Loss: -2.8558
Policy update: Rollout 70/100, Loss: -1.4335
Policy update: Rollout 75/100, Loss: -0.9419
Policy update: Rollout 80/100, Loss: 0.5918
Policy update: Rollout 85/100, Loss: 1.6447
Policy update: Rollout 90/100, Loss: 1.1299
Policy update: Rollout 95/100, Loss: 0.6239
Policy update: Rollout 100/100, Loss: -2.3835
Evaluation: Average reward = 105.80

Iteration 25/70
Collected 100 trajectories. Average reward: 83.40
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0284
Reward model training epoch 2/5, Loss: 0.0284
Reward model training epoch 3/5, Loss: 0.0284
Reward model training epoch 4/5, Loss: 0.0284
Reward model training epoch 5/5, Loss: 0.0284
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 0.2611
Policy update: Rollout 10/100, Loss: -1.3311
Policy update: Rollout 15/100, Loss: -3.1929
Policy update: Rollout 20/100, Loss: -0.7493
Policy update: Rollout 25/100, Loss: -0.5715
Policy update: Rollout 30/100, Loss: 1.2892
Policy update: Rollout 35/100, Loss: -0.6688
Policy update: Rollout 40/100, Loss: 0.6307
Policy update: Rollout 45/100, Loss: -0.2731
Policy update: Rollout 50/100, Loss: 1.0453
Policy update: Rollout 55/100, Loss: -5.6262
Policy update: Rollout 60/100, Loss: -1.2380
Policy update: Rollout 65/100, Loss: -1.1103
Policy update: Rollout 70/100, Loss: -1.8222
Policy update: Rollout 75/100, Loss: 0.6086
Policy update: Rollout 80/100, Loss: 0.0552
Policy update: Rollout 85/100, Loss: 1.0840
Policy update: Rollout 90/100, Loss: -0.8980
Policy update: Rollout 95/100, Loss: 1.0708
Policy update: Rollout 100/100, Loss: -0.6598
Evaluation: Average reward = 68.00

Iteration 26/70
Collected 100 trajectories. Average reward: 75.25
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0331
Reward model training epoch 2/5, Loss: 0.0331
Reward model training epoch 3/5, Loss: 0.0331
Reward model training epoch 4/5, Loss: 0.0331
Reward model training epoch 5/5, Loss: 0.0331
Policy update: Rollout 5/100, Loss: -0.2867
Policy update: Rollout 10/100, Loss: 0.6753
Policy update: Rollout 15/100, Loss: 1.8119
Policy update: Rollout 20/100, Loss: -2.9975
Policy update: Rollout 25/100, Loss: -0.7533
Policy update: Rollout 30/100, Loss: -2.4686
Policy update: Rollout 35/100, Loss: -0.9024
Policy update: Rollout 40/100, Loss: -2.5100
Policy update: Rollout 45/100, Loss: 0.3152
Policy update: Rollout 50/100, Loss: 0.3898
Policy update: Rollout 55/100, Loss: -4.0959
Policy update: Rollout 60/100, Loss: -0.6699
Policy update: Rollout 65/100, Loss: 0.2826
Policy update: Rollout 70/100, Loss: 0.7577
Policy update: Rollout 75/100, Loss: -0.9924
Policy update: Rollout 80/100, Loss: -2.0843
Policy update: Rollout 85/100, Loss: -3.1391
Policy update: Rollout 90/100, Loss: -4.0980
Policy update: Rollout 95/100, Loss: 0.3965
Policy update: Rollout 100/100, Loss: -1.5372
Evaluation: Average reward = 91.60

Iteration 27/70
Collected 100 trajectories. Average reward: 83.31
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0394
Reward model training epoch 2/5, Loss: 0.0394
Reward model training epoch 3/5, Loss: 0.0394
Reward model training epoch 4/5, Loss: 0.0394
Reward model training epoch 5/5, Loss: 0.0394
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -1.2101
Policy update: Rollout 10/100, Loss: 1.1214
Policy update: Rollout 15/100, Loss: -2.9712
Policy update: Rollout 20/100, Loss: 0.5530
Policy update: Rollout 25/100, Loss: 1.2735
Policy update: Rollout 30/100, Loss: -0.3861
Policy update: Rollout 35/100, Loss: -0.6016
Policy update: Rollout 40/100, Loss: -2.5215
Policy update: Rollout 45/100, Loss: -4.1410
Policy update: Rollout 50/100, Loss: -0.9707
Policy update: Rollout 55/100, Loss: -0.3024
Policy update: Rollout 60/100, Loss: -0.6736
Policy update: Rollout 65/100, Loss: -4.8219
Policy update: Rollout 70/100, Loss: 0.3730
Policy update: Rollout 75/100, Loss: -0.6039
Policy update: Rollout 80/100, Loss: -0.1871
Policy update: Rollout 85/100, Loss: 0.0470
Policy update: Rollout 90/100, Loss: 0.4833
Policy update: Rollout 95/100, Loss: -2.6718
Policy update: Rollout 100/100, Loss: -1.3014
Evaluation: Average reward = 79.60

Iteration 28/70
Collected 100 trajectories. Average reward: 111.10
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0490
Reward model training epoch 2/5, Loss: 0.0490
Reward model training epoch 3/5, Loss: 0.0489
Reward model training epoch 4/5, Loss: 0.0489
Reward model training epoch 5/5, Loss: 0.0489
Policy update: Rollout 5/100, Loss: -5.6504
Policy update: Rollout 10/100, Loss: 0.4447
Policy update: Rollout 15/100, Loss: -1.7066
Policy update: Rollout 20/100, Loss: -0.0530
Policy update: Rollout 25/100, Loss: -0.4551
Policy update: Rollout 30/100, Loss: -3.7986
Policy update: Rollout 35/100, Loss: -2.4062
Policy update: Rollout 40/100, Loss: -1.7994
Policy update: Rollout 45/100, Loss: 0.4989
Policy update: Rollout 50/100, Loss: -1.0718
Policy update: Rollout 55/100, Loss: -0.5195
Policy update: Rollout 60/100, Loss: -0.7560
Policy update: Rollout 65/100, Loss: -2.8615
Policy update: Rollout 70/100, Loss: -1.1833
Policy update: Rollout 75/100, Loss: 1.3971
Policy update: Rollout 80/100, Loss: 1.8399
Policy update: Rollout 85/100, Loss: 2.3367
Policy update: Rollout 90/100, Loss: 2.4997
Policy update: Rollout 95/100, Loss: -1.2763
Policy update: Rollout 100/100, Loss: -1.2441
Evaluation: Average reward = 85.40

Iteration 29/70
Collected 100 trajectories. Average reward: 105.62
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0329
Reward model training epoch 2/5, Loss: 0.0329
Reward model training epoch 3/5, Loss: 0.0329
Reward model training epoch 4/5, Loss: 0.0329
Reward model training epoch 5/5, Loss: 0.0329
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -1.6186
Policy update: Rollout 10/100, Loss: -6.0369
Policy update: Rollout 15/100, Loss: -0.1263
Policy update: Rollout 20/100, Loss: -9.2229
Policy update: Rollout 25/100, Loss: 0.2901
Policy update: Rollout 30/100, Loss: -6.4087
Policy update: Rollout 35/100, Loss: 0.9141
Policy update: Rollout 40/100, Loss: -4.9668
Policy update: Rollout 45/100, Loss: -2.3544
Policy update: Rollout 50/100, Loss: -2.8649
Policy update: Rollout 55/100, Loss: -1.6757
Policy update: Rollout 60/100, Loss: 0.4451
Policy update: Rollout 65/100, Loss: 1.8908
Policy update: Rollout 70/100, Loss: 1.5776
Policy update: Rollout 75/100, Loss: 0.5565
Policy update: Rollout 80/100, Loss: -8.7086
Policy update: Rollout 85/100, Loss: 0.0455
Policy update: Rollout 90/100, Loss: -2.8222
Policy update: Rollout 95/100, Loss: -3.6446
Policy update: Rollout 100/100, Loss: -1.1957
Evaluation: Average reward = 123.60

Iteration 30/70
Collected 100 trajectories. Average reward: 126.12
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0916
Reward model training epoch 2/5, Loss: 0.0916
Reward model training epoch 3/5, Loss: 0.0916
Reward model training epoch 4/5, Loss: 0.0916
Reward model training epoch 5/5, Loss: 0.0916
Policy update: Rollout 5/100, Loss: -0.6743
Policy update: Rollout 10/100, Loss: 1.0454
Policy update: Rollout 15/100, Loss: -4.2209
Policy update: Rollout 20/100, Loss: 0.0235
Policy update: Rollout 25/100, Loss: 0.2090
Policy update: Rollout 30/100, Loss: -0.2246
Policy update: Rollout 35/100, Loss: 0.5075
Policy update: Rollout 40/100, Loss: -7.0712
Policy update: Rollout 45/100, Loss: 3.5331
Policy update: Rollout 50/100, Loss: -3.1496
Policy update: Rollout 55/100, Loss: -1.7052
Policy update: Rollout 60/100, Loss: -6.6917
Policy update: Rollout 65/100, Loss: -0.0337
Policy update: Rollout 70/100, Loss: -2.2693
Policy update: Rollout 75/100, Loss: -0.6081
Policy update: Rollout 80/100, Loss: -6.9004
Policy update: Rollout 85/100, Loss: -2.8033
Policy update: Rollout 90/100, Loss: -0.1548
Policy update: Rollout 95/100, Loss: 1.3208
Policy update: Rollout 100/100, Loss: 0.3466
Evaluation: Average reward = 159.00

Iteration 31/70
Collected 100 trajectories. Average reward: 116.25
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.0304
Reward model training epoch 2/5, Loss: 0.0304
Reward model training epoch 3/5, Loss: 0.0304
Reward model training epoch 4/5, Loss: 0.0304
Reward model training epoch 5/5, Loss: 0.0304
Preference prediction accuracy: 1.00
Policy update: Rollout 5/100, Loss: -1.6774
Policy update: Rollout 10/100, Loss: -7.9196
Policy update: Rollout 15/100, Loss: -1.9368
Policy update: Rollout 20/100, Loss: 3.1932
Policy update: Rollout 25/100, Loss: -9.8390
Policy update: Rollout 30/100, Loss: 3.0732
Policy update: Rollout 35/100, Loss: -3.0105
Policy update: Rollout 40/100, Loss: -0.4798
Policy update: Rollout 45/100, Loss: 0.3174
Policy update: Rollout 50/100, Loss: -1.8464
Policy update: Rollout 55/100, Loss: -2.2608
Policy update: Rollout 60/100, Loss: -3.1941
Policy update: Rollout 65/100, Loss: -1.0353
Policy update: Rollout 70/100, Loss: 2.9484
Policy update: Rollout 75/100, Loss: 0.7841
Policy update: Rollout 80/100, Loss: -7.9179
Policy update: Rollout 85/100, Loss: -4.7750
Policy update: Rollout 90/100, Loss: -4.4814
Policy update: Rollout 95/100, Loss: -3.4052
Policy update: Rollout 100/100, Loss: -1.6588
Evaluation: Average reward = 129.40

Iteration 32/70
Collected 100 trajectories. Average reward: 139.10
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1644
Reward model training epoch 2/5, Loss: 0.1644
Reward model training epoch 3/5, Loss: 0.1644
Reward model training epoch 4/5, Loss: 0.1644
Reward model training epoch 5/5, Loss: 0.1644
Policy update: Rollout 5/100, Loss: -0.5585
Policy update: Rollout 10/100, Loss: -5.0394
Policy update: Rollout 15/100, Loss: -3.0785
Policy update: Rollout 20/100, Loss: -1.8149
Policy update: Rollout 25/100, Loss: -3.6511
Policy update: Rollout 30/100, Loss: -5.4776
Policy update: Rollout 35/100, Loss: -3.6026
Policy update: Rollout 40/100, Loss: 0.7498
Policy update: Rollout 45/100, Loss: -2.8800
Policy update: Rollout 50/100, Loss: -1.4824
Policy update: Rollout 55/100, Loss: 1.2947
Policy update: Rollout 60/100, Loss: -1.7614
Policy update: Rollout 65/100, Loss: -1.9271
Policy update: Rollout 70/100, Loss: -1.8786
Policy update: Rollout 75/100, Loss: -0.9278
Policy update: Rollout 80/100, Loss: 0.5032
Policy update: Rollout 85/100, Loss: 0.9155
Policy update: Rollout 90/100, Loss: -1.5993
Policy update: Rollout 95/100, Loss: -4.0983
Policy update: Rollout 100/100, Loss: -1.7058
Evaluation: Average reward = 92.20

Iteration 33/70
Collected 100 trajectories. Average reward: 131.34
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1136
Reward model training epoch 2/5, Loss: 0.1136
Reward model training epoch 3/5, Loss: 0.1136
Reward model training epoch 4/5, Loss: 0.1136
Reward model training epoch 5/5, Loss: 0.1136
Preference prediction accuracy: 1.00
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 0.4553
Policy update: Rollout 10/100, Loss: -1.7932
Policy update: Rollout 15/100, Loss: -0.1164
Policy update: Rollout 20/100, Loss: -6.0147
Policy update: Rollout 25/100, Loss: 0.2501
Policy update: Rollout 30/100, Loss: -0.9667
Policy update: Rollout 35/100, Loss: -8.2835
Policy update: Rollout 40/100, Loss: -2.4386
Policy update: Rollout 45/100, Loss: 3.4984
Policy update: Rollout 50/100, Loss: -0.6692
Policy update: Rollout 55/100, Loss: -1.5529
Policy update: Rollout 60/100, Loss: -1.5225
Policy update: Rollout 65/100, Loss: -0.0567
Policy update: Rollout 70/100, Loss: -9.2215
Policy update: Rollout 75/100, Loss: 0.2349
Policy update: Rollout 80/100, Loss: -2.1604
Policy update: Rollout 85/100, Loss: -1.3347
Policy update: Rollout 90/100, Loss: -0.0476
Policy update: Rollout 95/100, Loss: -5.8057
Policy update: Rollout 100/100, Loss: -1.2174
Evaluation: Average reward = 174.20

Iteration 34/70
Collected 100 trajectories. Average reward: 139.54
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1829
Reward model training epoch 2/5, Loss: 0.1829
Reward model training epoch 3/5, Loss: 0.1829
Reward model training epoch 4/5, Loss: 0.1829
Reward model training epoch 5/5, Loss: 0.1829
Policy update: Rollout 5/100, Loss: -3.4515
Policy update: Rollout 10/100, Loss: -4.9482
Policy update: Rollout 15/100, Loss: -8.6119
Policy update: Rollout 20/100, Loss: -4.5252
Policy update: Rollout 25/100, Loss: -0.9985
Policy update: Rollout 30/100, Loss: -3.0435
Policy update: Rollout 35/100, Loss: -4.5280
Policy update: Rollout 40/100, Loss: 1.8189
Policy update: Rollout 45/100, Loss: -1.4032
Policy update: Rollout 50/100, Loss: -3.2876
Policy update: Rollout 55/100, Loss: -2.3233
Policy update: Rollout 60/100, Loss: -2.5716
Policy update: Rollout 65/100, Loss: 0.2056
Policy update: Rollout 70/100, Loss: -2.7518
Policy update: Rollout 75/100, Loss: 3.1709
Policy update: Rollout 80/100, Loss: -9.4020
Policy update: Rollout 85/100, Loss: 1.9630
Policy update: Rollout 90/100, Loss: -5.8464
Policy update: Rollout 95/100, Loss: -2.1532
Policy update: Rollout 100/100, Loss: -1.3837
Evaluation: Average reward = 196.00

Iteration 35/70
Collected 100 trajectories. Average reward: 151.57
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1652
Reward model training epoch 2/5, Loss: 0.1652
Reward model training epoch 3/5, Loss: 0.1652
Reward model training epoch 4/5, Loss: 0.1652
Reward model training epoch 5/5, Loss: 0.1652
Preference prediction accuracy: 0.97
Policy update: Rollout 5/100, Loss: -2.6262
Policy update: Rollout 10/100, Loss: -0.0099
Policy update: Rollout 15/100, Loss: -0.9707
Policy update: Rollout 20/100, Loss: 0.9457
Policy update: Rollout 25/100, Loss: -6.7112
Policy update: Rollout 30/100, Loss: -2.4495
Policy update: Rollout 35/100, Loss: -0.7595
Policy update: Rollout 40/100, Loss: -6.4295
Policy update: Rollout 45/100, Loss: 0.0173
Policy update: Rollout 50/100, Loss: -2.3756
Policy update: Rollout 55/100, Loss: -3.4894
Policy update: Rollout 60/100, Loss: -8.5172
Policy update: Rollout 65/100, Loss: -7.1723
Policy update: Rollout 70/100, Loss: -0.2234
Policy update: Rollout 75/100, Loss: -2.6986
Policy update: Rollout 80/100, Loss: -3.6386
Policy update: Rollout 85/100, Loss: -3.2113
Policy update: Rollout 90/100, Loss: -6.8420
Policy update: Rollout 95/100, Loss: -0.4324
Policy update: Rollout 100/100, Loss: -2.0814
Evaluation: Average reward = 132.80

Iteration 36/70
Collected 100 trajectories. Average reward: 153.16
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.2934
Reward model training epoch 2/5, Loss: 0.2934
Reward model training epoch 3/5, Loss: 0.2934
Reward model training epoch 4/5, Loss: 0.2934
Reward model training epoch 5/5, Loss: 0.2934
Policy update: Rollout 5/100, Loss: -3.5178
Policy update: Rollout 10/100, Loss: -0.0400
Policy update: Rollout 15/100, Loss: 0.4355
Policy update: Rollout 20/100, Loss: -0.1670
Policy update: Rollout 25/100, Loss: -5.5708
Policy update: Rollout 30/100, Loss: -2.4195
Policy update: Rollout 35/100, Loss: -3.0583
Policy update: Rollout 40/100, Loss: -0.6025
Policy update: Rollout 45/100, Loss: 0.0367
Policy update: Rollout 50/100, Loss: 2.5383
Policy update: Rollout 55/100, Loss: -1.3712
Policy update: Rollout 60/100, Loss: 4.1709
Policy update: Rollout 65/100, Loss: -1.4964
Policy update: Rollout 70/100, Loss: 1.0100
Policy update: Rollout 75/100, Loss: -2.3142
Policy update: Rollout 80/100, Loss: -2.2248
Policy update: Rollout 85/100, Loss: -0.5082
Policy update: Rollout 90/100, Loss: -3.1157
Policy update: Rollout 95/100, Loss: 0.9987
Policy update: Rollout 100/100, Loss: 0.8688
Evaluation: Average reward = 106.40

Iteration 37/70
Collected 100 trajectories. Average reward: 151.48
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.2611
Reward model training epoch 2/5, Loss: 0.2611
Reward model training epoch 3/5, Loss: 0.2611
Reward model training epoch 4/5, Loss: 0.2611
Reward model training epoch 5/5, Loss: 0.2611
Preference prediction accuracy: 0.97
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -4.2503
Policy update: Rollout 10/100, Loss: 0.5501
Policy update: Rollout 15/100, Loss: -11.9819
Policy update: Rollout 20/100, Loss: -6.1279
Policy update: Rollout 25/100, Loss: -6.3317
Policy update: Rollout 30/100, Loss: -1.2277
Policy update: Rollout 35/100, Loss: -4.4350
Policy update: Rollout 40/100, Loss: -7.8283
Policy update: Rollout 45/100, Loss: -2.1679
Policy update: Rollout 50/100, Loss: -3.0923
Policy update: Rollout 55/100, Loss: -0.1378
Policy update: Rollout 60/100, Loss: 0.2071
Policy update: Rollout 65/100, Loss: 1.7566
Policy update: Rollout 70/100, Loss: -3.7420
Policy update: Rollout 75/100, Loss: -3.2350
Policy update: Rollout 80/100, Loss: -0.9353
Policy update: Rollout 85/100, Loss: 1.9774
Policy update: Rollout 90/100, Loss: 0.4485
Policy update: Rollout 95/100, Loss: 0.7275
Policy update: Rollout 100/100, Loss: -2.4876
Evaluation: Average reward = 133.00

Iteration 38/70
Collected 100 trajectories. Average reward: 151.97
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.1758
Reward model training epoch 2/5, Loss: 0.1757
Reward model training epoch 3/5, Loss: 0.1757
Reward model training epoch 4/5, Loss: 0.1757
Reward model training epoch 5/5, Loss: 0.1751
Policy update: Rollout 5/100, Loss: -4.7495
Policy update: Rollout 10/100, Loss: -1.4201
Policy update: Rollout 15/100, Loss: -7.1499
Policy update: Rollout 20/100, Loss: 0.4996
Policy update: Rollout 25/100, Loss: -1.5189
Policy update: Rollout 30/100, Loss: -6.1920
Policy update: Rollout 35/100, Loss: -2.8766
Policy update: Rollout 40/100, Loss: -3.7126
Policy update: Rollout 45/100, Loss: -3.5831
Policy update: Rollout 50/100, Loss: 2.7096
Policy update: Rollout 55/100, Loss: -0.4333
Policy update: Rollout 60/100, Loss: 0.3530
Policy update: Rollout 65/100, Loss: -5.0359
Policy update: Rollout 70/100, Loss: -0.1674
Policy update: Rollout 75/100, Loss: -3.0801
Policy update: Rollout 80/100, Loss: 3.4280
Policy update: Rollout 85/100, Loss: -1.5147
Policy update: Rollout 90/100, Loss: 1.1673
Policy update: Rollout 95/100, Loss: -1.4590
Policy update: Rollout 100/100, Loss: -4.0857
Evaluation: Average reward = 133.60

Iteration 39/70
Collected 100 trajectories. Average reward: 157.86
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.3342
Reward model training epoch 2/5, Loss: 0.3307
Reward model training epoch 3/5, Loss: 0.3307
Reward model training epoch 4/5, Loss: 0.3307
Reward model training epoch 5/5, Loss: 0.3307
Preference prediction accuracy: 0.87
Policy update: Rollout 5/100, Loss: -2.3117
Policy update: Rollout 10/100, Loss: -2.5047
Policy update: Rollout 15/100, Loss: 1.0751
Policy update: Rollout 20/100, Loss: 2.9015
Policy update: Rollout 25/100, Loss: -1.3860
Policy update: Rollout 30/100, Loss: -3.8296
Policy update: Rollout 35/100, Loss: -0.9031
Policy update: Rollout 40/100, Loss: -2.9181
Policy update: Rollout 45/100, Loss: 0.6861
Policy update: Rollout 50/100, Loss: -2.2151
Policy update: Rollout 55/100, Loss: -0.6570
Policy update: Rollout 60/100, Loss: 2.7317
Policy update: Rollout 65/100, Loss: -6.1188
Policy update: Rollout 70/100, Loss: -5.9716
Policy update: Rollout 75/100, Loss: -6.9081
Policy update: Rollout 80/100, Loss: -1.4625
Policy update: Rollout 85/100, Loss: -9.6266
Policy update: Rollout 90/100, Loss: -2.8601
Policy update: Rollout 95/100, Loss: -2.2179
Policy update: Rollout 100/100, Loss: -4.9773
Evaluation: Average reward = 152.20

Iteration 40/70
Collected 100 trajectories. Average reward: 166.62
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.5727
Reward model training epoch 2/5, Loss: 0.5727
Reward model training epoch 3/5, Loss: 0.5727
Reward model training epoch 4/5, Loss: 0.5727
Reward model training epoch 5/5, Loss: 0.5727
Policy update: Rollout 5/100, Loss: -2.9263
Policy update: Rollout 10/100, Loss: 0.4117
Policy update: Rollout 15/100, Loss: -0.5207
Policy update: Rollout 20/100, Loss: -1.5123
Policy update: Rollout 25/100, Loss: -2.8738
Policy update: Rollout 30/100, Loss: 1.8050
Policy update: Rollout 35/100, Loss: -8.2862
Policy update: Rollout 40/100, Loss: -4.5703
Policy update: Rollout 45/100, Loss: 3.0740
Policy update: Rollout 50/100, Loss: -3.3526
Policy update: Rollout 55/100, Loss: -5.5572
Policy update: Rollout 60/100, Loss: -2.5463
Policy update: Rollout 65/100, Loss: -1.3890
Policy update: Rollout 70/100, Loss: 2.7261
Policy update: Rollout 75/100, Loss: -3.5618
Policy update: Rollout 80/100, Loss: -2.2693
Policy update: Rollout 85/100, Loss: -7.3396
Policy update: Rollout 90/100, Loss: -0.6542
Policy update: Rollout 95/100, Loss: -3.1400
Policy update: Rollout 100/100, Loss: -3.5060
Evaluation: Average reward = 155.60

Iteration 41/70
Collected 100 trajectories. Average reward: 174.93
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 0.97
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 0.3936
Policy update: Rollout 10/100, Loss: 0.9997
Policy update: Rollout 15/100, Loss: -4.3623
Policy update: Rollout 20/100, Loss: -2.9214
Policy update: Rollout 25/100, Loss: 0.5224
Policy update: Rollout 30/100, Loss: -0.8326
Policy update: Rollout 35/100, Loss: 0.1981
Policy update: Rollout 40/100, Loss: -1.6943
Policy update: Rollout 45/100, Loss: -0.4899
Policy update: Rollout 50/100, Loss: -3.1315
Policy update: Rollout 55/100, Loss: -1.7444
Policy update: Rollout 60/100, Loss: -0.7524
Policy update: Rollout 65/100, Loss: -2.3203
Policy update: Rollout 70/100, Loss: -3.1007
Policy update: Rollout 75/100, Loss: -2.3977
Policy update: Rollout 80/100, Loss: -0.9008
Policy update: Rollout 85/100, Loss: -1.5870
Policy update: Rollout 90/100, Loss: 3.7706
Policy update: Rollout 95/100, Loss: -3.4001
Policy update: Rollout 100/100, Loss: 2.8859
Evaluation: Average reward = 183.60

Iteration 42/70
Collected 100 trajectories. Average reward: 175.42
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6123
Reward model training epoch 2/5, Loss: 0.6123
Reward model training epoch 3/5, Loss: 0.6123
Reward model training epoch 4/5, Loss: 0.6123
Reward model training epoch 5/5, Loss: 0.6123
Policy update: Rollout 5/100, Loss: -2.8326
Policy update: Rollout 10/100, Loss: -3.3016
Policy update: Rollout 15/100, Loss: -1.0585
Policy update: Rollout 20/100, Loss: -1.0660
Policy update: Rollout 25/100, Loss: 3.6552
Policy update: Rollout 30/100, Loss: -10.5848
Policy update: Rollout 35/100, Loss: -11.8800
Policy update: Rollout 40/100, Loss: 0.1857
Policy update: Rollout 45/100, Loss: -3.8500
Policy update: Rollout 50/100, Loss: 4.6700
Policy update: Rollout 55/100, Loss: -2.3058
Policy update: Rollout 60/100, Loss: -4.1617
Policy update: Rollout 65/100, Loss: -7.2457
Policy update: Rollout 70/100, Loss: 4.1268
Policy update: Rollout 75/100, Loss: -1.7800
Policy update: Rollout 80/100, Loss: -4.0953
Policy update: Rollout 85/100, Loss: -3.2813
Policy update: Rollout 90/100, Loss: -2.0441
Policy update: Rollout 95/100, Loss: -2.3572
Policy update: Rollout 100/100, Loss: -2.0776
Evaluation: Average reward = 165.40

Iteration 43/70
Collected 100 trajectories. Average reward: 173.55
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6919
Reward model training epoch 2/5, Loss: 0.6919
Reward model training epoch 3/5, Loss: 0.6919
Reward model training epoch 4/5, Loss: 0.6919
Reward model training epoch 5/5, Loss: 0.6919
Preference prediction accuracy: 0.97
Policy update: Rollout 5/100, Loss: 2.0294
Policy update: Rollout 10/100, Loss: -0.2183
Policy update: Rollout 15/100, Loss: 2.1435
Policy update: Rollout 20/100, Loss: 0.7837
Policy update: Rollout 25/100, Loss: 3.5415
Policy update: Rollout 30/100, Loss: -4.4389
Policy update: Rollout 35/100, Loss: -1.6583
Policy update: Rollout 40/100, Loss: -8.0742
Policy update: Rollout 45/100, Loss: -0.4132
Policy update: Rollout 50/100, Loss: -3.0405
Policy update: Rollout 55/100, Loss: -6.9845
Policy update: Rollout 60/100, Loss: -3.9952
Policy update: Rollout 65/100, Loss: -8.3161
Policy update: Rollout 70/100, Loss: 2.2796
Policy update: Rollout 75/100, Loss: -4.2221
Policy update: Rollout 80/100, Loss: -1.5997
Policy update: Rollout 85/100, Loss: -9.0653
Policy update: Rollout 90/100, Loss: 0.0984
Policy update: Rollout 95/100, Loss: -3.4050
Policy update: Rollout 100/100, Loss: 2.6871
Evaluation: Average reward = 160.40

Iteration 44/70
Collected 100 trajectories. Average reward: 184.33
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6855
Reward model training epoch 2/5, Loss: 0.6855
Reward model training epoch 3/5, Loss: 0.6855
Reward model training epoch 4/5, Loss: 0.6855
Reward model training epoch 5/5, Loss: 0.6855
Policy update: Rollout 5/100, Loss: -4.1691
Policy update: Rollout 10/100, Loss: 0.4523
Policy update: Rollout 15/100, Loss: -0.6238
Policy update: Rollout 20/100, Loss: -1.5366
Policy update: Rollout 25/100, Loss: 1.7483
Policy update: Rollout 30/100, Loss: 2.5219
Policy update: Rollout 35/100, Loss: -4.5952
Policy update: Rollout 40/100, Loss: -12.9874
Policy update: Rollout 45/100, Loss: -2.0882
Policy update: Rollout 50/100, Loss: -7.8696
Policy update: Rollout 55/100, Loss: 0.1394
Policy update: Rollout 60/100, Loss: 1.2803
Policy update: Rollout 65/100, Loss: 4.3752
Policy update: Rollout 70/100, Loss: -12.2328
Policy update: Rollout 75/100, Loss: -0.6849
Policy update: Rollout 80/100, Loss: -8.1571
Policy update: Rollout 85/100, Loss: -5.0726
Policy update: Rollout 90/100, Loss: -7.8133
Policy update: Rollout 95/100, Loss: 0.5660
Policy update: Rollout 100/100, Loss: -0.1381
Evaluation: Average reward = 186.40

Iteration 45/70
Collected 100 trajectories. Average reward: 176.29
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 0.83
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -4.2065
Policy update: Rollout 10/100, Loss: -0.4388
Policy update: Rollout 15/100, Loss: 0.1132
Policy update: Rollout 20/100, Loss: -2.5048
Policy update: Rollout 25/100, Loss: 1.0511
Policy update: Rollout 30/100, Loss: -4.0773
Policy update: Rollout 35/100, Loss: -4.4290
Policy update: Rollout 40/100, Loss: -1.3678
Policy update: Rollout 45/100, Loss: -2.7947
Policy update: Rollout 50/100, Loss: 0.3196
Policy update: Rollout 55/100, Loss: -8.6308
Policy update: Rollout 60/100, Loss: -4.0627
Policy update: Rollout 65/100, Loss: -3.5097
Policy update: Rollout 70/100, Loss: -8.2804
Policy update: Rollout 75/100, Loss: -8.6267
Policy update: Rollout 80/100, Loss: -8.2708
Policy update: Rollout 85/100, Loss: -2.0983
Policy update: Rollout 90/100, Loss: 3.4507
Policy update: Rollout 95/100, Loss: 1.4843
Policy update: Rollout 100/100, Loss: 0.6442
Evaluation: Average reward = 185.20

Iteration 46/70
Collected 100 trajectories. Average reward: 170.21
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6881
Reward model training epoch 2/5, Loss: 0.6881
Reward model training epoch 3/5, Loss: 0.6881
Reward model training epoch 4/5, Loss: 0.6881
Reward model training epoch 5/5, Loss: 0.6881
Policy update: Rollout 5/100, Loss: 1.3006
Policy update: Rollout 10/100, Loss: -3.5746
Policy update: Rollout 15/100, Loss: -6.1136
Policy update: Rollout 20/100, Loss: 0.8048
Policy update: Rollout 25/100, Loss: -2.3969
Policy update: Rollout 30/100, Loss: -1.3591
Policy update: Rollout 35/100, Loss: -5.0624
Policy update: Rollout 40/100, Loss: -8.2178
Policy update: Rollout 45/100, Loss: -2.1563
Policy update: Rollout 50/100, Loss: -3.6287
Policy update: Rollout 55/100, Loss: -1.9461
Policy update: Rollout 60/100, Loss: -4.8021
Policy update: Rollout 65/100, Loss: -5.1390
Policy update: Rollout 70/100, Loss: -3.9779
Policy update: Rollout 75/100, Loss: -0.5508
Policy update: Rollout 80/100, Loss: 2.3558
Policy update: Rollout 85/100, Loss: -1.6300
Policy update: Rollout 90/100, Loss: -1.6155
Policy update: Rollout 95/100, Loss: -3.6643
Policy update: Rollout 100/100, Loss: 1.0051
Evaluation: Average reward = 200.00

Iteration 47/70
Collected 100 trajectories. Average reward: 173.10
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6653
Reward model training epoch 2/5, Loss: 0.6653
Reward model training epoch 3/5, Loss: 0.6653
Reward model training epoch 4/5, Loss: 0.6653
Reward model training epoch 5/5, Loss: 0.6653
Preference prediction accuracy: 0.77
Policy update: Rollout 5/100, Loss: -4.5427
Policy update: Rollout 10/100, Loss: -3.3100
Policy update: Rollout 15/100, Loss: -7.8620
Policy update: Rollout 20/100, Loss: -2.4221
Policy update: Rollout 25/100, Loss: -3.2489
Policy update: Rollout 30/100, Loss: -2.8816
Policy update: Rollout 35/100, Loss: 3.4216
Policy update: Rollout 40/100, Loss: 3.0003
Policy update: Rollout 45/100, Loss: -8.6925
Policy update: Rollout 50/100, Loss: -9.6308
Policy update: Rollout 55/100, Loss: -5.3419
Policy update: Rollout 60/100, Loss: -1.6691
Policy update: Rollout 65/100, Loss: -6.0547
Policy update: Rollout 70/100, Loss: 1.3673
Policy update: Rollout 75/100, Loss: 3.2755
Policy update: Rollout 80/100, Loss: 2.9898
Policy update: Rollout 85/100, Loss: -7.2539
Policy update: Rollout 90/100, Loss: -4.7360
Policy update: Rollout 95/100, Loss: -2.1652
Policy update: Rollout 100/100, Loss: 1.0543
Evaluation: Average reward = 196.80

Iteration 48/70
Collected 100 trajectories. Average reward: 184.17
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6919
Reward model training epoch 2/5, Loss: 0.6919
Reward model training epoch 3/5, Loss: 0.6919
Reward model training epoch 4/5, Loss: 0.6919
Reward model training epoch 5/5, Loss: 0.6919
Policy update: Rollout 5/100, Loss: -5.5392
Policy update: Rollout 10/100, Loss: 1.7496
Policy update: Rollout 15/100, Loss: -3.6414
Policy update: Rollout 20/100, Loss: -2.2540
Policy update: Rollout 25/100, Loss: -1.2921
Policy update: Rollout 30/100, Loss: -9.9385
Policy update: Rollout 35/100, Loss: -6.7533
Policy update: Rollout 40/100, Loss: 0.9337
Policy update: Rollout 45/100, Loss: -8.7751
Policy update: Rollout 50/100, Loss: 1.7800
Policy update: Rollout 55/100, Loss: 0.5936
Policy update: Rollout 60/100, Loss: -4.5975
Policy update: Rollout 65/100, Loss: -0.9606
Policy update: Rollout 70/100, Loss: 1.8763
Policy update: Rollout 75/100, Loss: -2.1510
Policy update: Rollout 80/100, Loss: -3.6967
Policy update: Rollout 85/100, Loss: -0.1814
Policy update: Rollout 90/100, Loss: 2.9555
Policy update: Rollout 95/100, Loss: -5.0230
Policy update: Rollout 100/100, Loss: -5.4974
Evaluation: Average reward = 200.00

Iteration 49/70
Collected 100 trajectories. Average reward: 181.13
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6868
Reward model training epoch 2/5, Loss: 0.6868
Reward model training epoch 3/5, Loss: 0.6868
Reward model training epoch 4/5, Loss: 0.6868
Reward model training epoch 5/5, Loss: 0.6868
Preference prediction accuracy: 0.97
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -3.5381
Policy update: Rollout 10/100, Loss: -10.5400
Policy update: Rollout 15/100, Loss: -5.6358
Policy update: Rollout 20/100, Loss: -4.2638
Policy update: Rollout 25/100, Loss: -1.9410
Policy update: Rollout 30/100, Loss: 1.2890
Policy update: Rollout 35/100, Loss: -8.6079
Policy update: Rollout 40/100, Loss: -3.1863
Policy update: Rollout 45/100, Loss: -3.3084
Policy update: Rollout 50/100, Loss: -1.7745
Policy update: Rollout 55/100, Loss: 1.8955
Policy update: Rollout 60/100, Loss: -4.3339
Policy update: Rollout 65/100, Loss: -6.1119
Policy update: Rollout 70/100, Loss: 2.0818
Policy update: Rollout 75/100, Loss: 2.7849
Policy update: Rollout 80/100, Loss: -8.6003
Policy update: Rollout 85/100, Loss: -11.3584
Policy update: Rollout 90/100, Loss: -2.8203
Policy update: Rollout 95/100, Loss: 1.6477
Policy update: Rollout 100/100, Loss: 0.1380
Evaluation: Average reward = 190.20

Iteration 50/70
Collected 100 trajectories. Average reward: 184.45
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Policy update: Rollout 5/100, Loss: -0.7834
Policy update: Rollout 10/100, Loss: 1.0247
Policy update: Rollout 15/100, Loss: -4.7573
Policy update: Rollout 20/100, Loss: 5.6787
Policy update: Rollout 25/100, Loss: 4.6037
Policy update: Rollout 30/100, Loss: -3.1134
Policy update: Rollout 35/100, Loss: -2.4215
Policy update: Rollout 40/100, Loss: -10.2307
Policy update: Rollout 45/100, Loss: -1.4148
Policy update: Rollout 50/100, Loss: -6.2184
Policy update: Rollout 55/100, Loss: 0.3121
Policy update: Rollout 60/100, Loss: -7.7899
Policy update: Rollout 65/100, Loss: -1.8992
Policy update: Rollout 70/100, Loss: -0.4489
Policy update: Rollout 75/100, Loss: 2.6253
Policy update: Rollout 80/100, Loss: 4.5340
Policy update: Rollout 85/100, Loss: -6.2387
Policy update: Rollout 90/100, Loss: -2.8383
Policy update: Rollout 95/100, Loss: -11.9402
Policy update: Rollout 100/100, Loss: -15.7032
Evaluation: Average reward = 195.60

Iteration 51/70
Collected 100 trajectories. Average reward: 186.61
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6792
Reward model training epoch 2/5, Loss: 0.6792
Reward model training epoch 3/5, Loss: 0.6792
Reward model training epoch 4/5, Loss: 0.6792
Reward model training epoch 5/5, Loss: 0.6792
Preference prediction accuracy: 0.73
Policy update: Rollout 5/100, Loss: -9.3888
Policy update: Rollout 10/100, Loss: -5.7383
Policy update: Rollout 15/100, Loss: -4.4290
Policy update: Rollout 20/100, Loss: -1.2992
Policy update: Rollout 25/100, Loss: -1.1108
Policy update: Rollout 30/100, Loss: -1.7363
Policy update: Rollout 35/100, Loss: -2.2610
Policy update: Rollout 40/100, Loss: -14.9043
Policy update: Rollout 45/100, Loss: 5.8735
Policy update: Rollout 50/100, Loss: 1.4412
Policy update: Rollout 55/100, Loss: -2.4450
Policy update: Rollout 60/100, Loss: -0.4862
Policy update: Rollout 65/100, Loss: -1.2667
Policy update: Rollout 70/100, Loss: -2.4710
Policy update: Rollout 75/100, Loss: -7.4048
Policy update: Rollout 80/100, Loss: -1.1470
Policy update: Rollout 85/100, Loss: -3.9196
Policy update: Rollout 90/100, Loss: 1.1959
Policy update: Rollout 95/100, Loss: -2.3586
Policy update: Rollout 100/100, Loss: -9.1174
Evaluation: Average reward = 189.80

Iteration 52/70
Collected 100 trajectories. Average reward: 183.56
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6932
Reward model training epoch 2/5, Loss: 0.6932
Reward model training epoch 3/5, Loss: 0.6932
Reward model training epoch 4/5, Loss: 0.6932
Reward model training epoch 5/5, Loss: 0.6932
Policy update: Rollout 5/100, Loss: -4.0021
Policy update: Rollout 10/100, Loss: -0.5622
Policy update: Rollout 15/100, Loss: -2.8082
Policy update: Rollout 20/100, Loss: -10.8332
Policy update: Rollout 25/100, Loss: -3.1997
Policy update: Rollout 30/100, Loss: -9.2260
Policy update: Rollout 35/100, Loss: -4.6551
Policy update: Rollout 40/100, Loss: -8.8829
Policy update: Rollout 45/100, Loss: -1.4517
Policy update: Rollout 50/100, Loss: -2.7504
Policy update: Rollout 55/100, Loss: -1.0493
Policy update: Rollout 60/100, Loss: -2.5626
Policy update: Rollout 65/100, Loss: 0.2290
Policy update: Rollout 70/100, Loss: -1.6466
Policy update: Rollout 75/100, Loss: -0.0387
Policy update: Rollout 80/100, Loss: -4.1534
Policy update: Rollout 85/100, Loss: -11.1423
Policy update: Rollout 90/100, Loss: 2.3497
Policy update: Rollout 95/100, Loss: 2.1979
Policy update: Rollout 100/100, Loss: -15.1398
Evaluation: Average reward = 195.40

Iteration 53/70
Collected 100 trajectories. Average reward: 187.81
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6919
Reward model training epoch 2/5, Loss: 0.6919
Reward model training epoch 3/5, Loss: 0.6919
Reward model training epoch 4/5, Loss: 0.6919
Reward model training epoch 5/5, Loss: 0.6919
Preference prediction accuracy: 0.80
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 2.7668
Policy update: Rollout 10/100, Loss: -15.9971
Policy update: Rollout 15/100, Loss: -2.4115
Policy update: Rollout 20/100, Loss: 2.3476
Policy update: Rollout 25/100, Loss: 0.4868
Policy update: Rollout 30/100, Loss: -5.4397
Policy update: Rollout 35/100, Loss: -2.3948
Policy update: Rollout 40/100, Loss: 1.6222
Policy update: Rollout 45/100, Loss: 3.8233
Policy update: Rollout 50/100, Loss: 3.8121
Policy update: Rollout 55/100, Loss: -3.8133
Policy update: Rollout 60/100, Loss: -4.9545
Policy update: Rollout 65/100, Loss: 7.0115
Policy update: Rollout 70/100, Loss: 5.0672
Policy update: Rollout 75/100, Loss: -2.8344
Policy update: Rollout 80/100, Loss: -6.7031
Policy update: Rollout 85/100, Loss: -0.3847
Policy update: Rollout 90/100, Loss: 4.5679
Policy update: Rollout 95/100, Loss: -0.1451
Policy update: Rollout 100/100, Loss: -6.8394
Evaluation: Average reward = 175.00

Iteration 54/70
Collected 100 trajectories. Average reward: 192.38
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Policy update: Rollout 5/100, Loss: -8.2586
Policy update: Rollout 10/100, Loss: -4.0188
Policy update: Rollout 15/100, Loss: -3.1999
Policy update: Rollout 20/100, Loss: -5.1104
Policy update: Rollout 25/100, Loss: -3.5569
Policy update: Rollout 30/100, Loss: -5.5629
Policy update: Rollout 35/100, Loss: 3.8748
Policy update: Rollout 40/100, Loss: -9.2974
Policy update: Rollout 45/100, Loss: -8.3680
Policy update: Rollout 50/100, Loss: 2.1070
Policy update: Rollout 55/100, Loss: -5.1803
Policy update: Rollout 60/100, Loss: -7.6093
Policy update: Rollout 65/100, Loss: -5.8001
Policy update: Rollout 70/100, Loss: -9.6960
Policy update: Rollout 75/100, Loss: 0.3363
Policy update: Rollout 80/100, Loss: -0.9686
Policy update: Rollout 85/100, Loss: -8.0359
Policy update: Rollout 90/100, Loss: -19.6609
Policy update: Rollout 95/100, Loss: -1.8559
Policy update: Rollout 100/100, Loss: 2.9300
Evaluation: Average reward = 196.00

Iteration 55/70
Collected 100 trajectories. Average reward: 191.40
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 0.67
Policy update: Rollout 5/100, Loss: -5.8385
Policy update: Rollout 10/100, Loss: -0.9788
Policy update: Rollout 15/100, Loss: -2.4701
Policy update: Rollout 20/100, Loss: -5.1159
Policy update: Rollout 25/100, Loss: -4.9409
Policy update: Rollout 30/100, Loss: -9.2429
Policy update: Rollout 35/100, Loss: -11.3083
Policy update: Rollout 40/100, Loss: -1.4036
Policy update: Rollout 45/100, Loss: 1.3150
Policy update: Rollout 50/100, Loss: 2.2704
Policy update: Rollout 55/100, Loss: -0.8849
Policy update: Rollout 60/100, Loss: 3.0565
Policy update: Rollout 65/100, Loss: -6.8209
Policy update: Rollout 70/100, Loss: -10.7221
Policy update: Rollout 75/100, Loss: -0.0905
Policy update: Rollout 80/100, Loss: -13.3160
Policy update: Rollout 85/100, Loss: -5.3488
Policy update: Rollout 90/100, Loss: -5.3809
Policy update: Rollout 95/100, Loss: -3.2533
Policy update: Rollout 100/100, Loss: -4.8134
Evaluation: Average reward = 200.00

Iteration 56/70
Collected 100 trajectories. Average reward: 188.03
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Policy update: Rollout 5/100, Loss: -9.1620
Policy update: Rollout 10/100, Loss: -1.2972
Policy update: Rollout 15/100, Loss: -7.3280
Policy update: Rollout 20/100, Loss: -3.9959
Policy update: Rollout 25/100, Loss: -6.2968
Policy update: Rollout 30/100, Loss: -11.1757
Policy update: Rollout 35/100, Loss: 3.3749
Policy update: Rollout 40/100, Loss: -1.5390
Policy update: Rollout 45/100, Loss: -5.7063
Policy update: Rollout 50/100, Loss: -18.0411
Policy update: Rollout 55/100, Loss: -0.8772
Policy update: Rollout 60/100, Loss: -2.0922
Policy update: Rollout 65/100, Loss: 3.4823
Policy update: Rollout 70/100, Loss: -4.4148
Policy update: Rollout 75/100, Loss: -2.4354
Policy update: Rollout 80/100, Loss: -6.2350
Policy update: Rollout 85/100, Loss: -2.0547
Policy update: Rollout 90/100, Loss: -2.6644
Policy update: Rollout 95/100, Loss: -3.7401
Policy update: Rollout 100/100, Loss: 0.7747
Evaluation: Average reward = 178.00

Iteration 57/70
Collected 100 trajectories. Average reward: 190.47
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 0.77
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: 1.9027
Policy update: Rollout 10/100, Loss: 1.3711
Policy update: Rollout 15/100, Loss: -5.2366
Policy update: Rollout 20/100, Loss: -4.3037
Policy update: Rollout 25/100, Loss: -2.0704
Policy update: Rollout 30/100, Loss: -9.8456
Policy update: Rollout 35/100, Loss: 0.9328
Policy update: Rollout 40/100, Loss: 0.7251
Policy update: Rollout 45/100, Loss: 0.9765
Policy update: Rollout 50/100, Loss: 8.1198
Policy update: Rollout 55/100, Loss: -2.7968
Policy update: Rollout 60/100, Loss: -10.8711
Policy update: Rollout 65/100, Loss: -2.8289
Policy update: Rollout 70/100, Loss: 2.9913
Policy update: Rollout 75/100, Loss: -0.5007
Policy update: Rollout 80/100, Loss: -0.2755
Policy update: Rollout 85/100, Loss: 3.1176
Policy update: Rollout 90/100, Loss: -6.9921
Policy update: Rollout 95/100, Loss: -6.3659
Policy update: Rollout 100/100, Loss: -7.0803
Evaluation: Average reward = 199.60

Iteration 58/70
Collected 100 trajectories. Average reward: 191.31
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6919
Reward model training epoch 2/5, Loss: 0.6919
Reward model training epoch 3/5, Loss: 0.6919
Reward model training epoch 4/5, Loss: 0.6919
Reward model training epoch 5/5, Loss: 0.6919
Policy update: Rollout 5/100, Loss: -3.0235
Policy update: Rollout 10/100, Loss: -2.7671
Policy update: Rollout 15/100, Loss: 1.5257
Policy update: Rollout 20/100, Loss: -6.2705
Policy update: Rollout 25/100, Loss: -13.2913
Policy update: Rollout 30/100, Loss: -3.7637
Policy update: Rollout 35/100, Loss: -1.6565
Policy update: Rollout 40/100, Loss: 3.2592
Policy update: Rollout 45/100, Loss: 0.1846
Policy update: Rollout 50/100, Loss: -6.8461
Policy update: Rollout 55/100, Loss: -7.2995
Policy update: Rollout 60/100, Loss: -6.1477
Policy update: Rollout 65/100, Loss: -12.1748
Policy update: Rollout 70/100, Loss: -1.8768
Policy update: Rollout 75/100, Loss: -3.5031
Policy update: Rollout 80/100, Loss: -0.4878
Policy update: Rollout 85/100, Loss: -5.1073
Policy update: Rollout 90/100, Loss: 0.3411
Policy update: Rollout 95/100, Loss: -9.0625
Policy update: Rollout 100/100, Loss: -4.5953
Evaluation: Average reward = 184.20

Iteration 59/70
Collected 100 trajectories. Average reward: 192.66
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6906
Reward model training epoch 2/5, Loss: 0.6906
Reward model training epoch 3/5, Loss: 0.6906
Reward model training epoch 4/5, Loss: 0.6906
Reward model training epoch 5/5, Loss: 0.6906
Preference prediction accuracy: 0.63
Policy update: Rollout 5/100, Loss: -5.0123
Policy update: Rollout 10/100, Loss: -7.8713
Policy update: Rollout 15/100, Loss: 0.1841
Policy update: Rollout 20/100, Loss: -1.6415
Policy update: Rollout 25/100, Loss: -8.3815
Policy update: Rollout 30/100, Loss: -4.3794
Policy update: Rollout 35/100, Loss: -8.7815
Policy update: Rollout 40/100, Loss: 1.3981
Policy update: Rollout 45/100, Loss: -4.5868
Policy update: Rollout 50/100, Loss: -10.2880
Policy update: Rollout 55/100, Loss: -4.6855
Policy update: Rollout 60/100, Loss: -8.8676
Policy update: Rollout 65/100, Loss: -2.0751
Policy update: Rollout 70/100, Loss: -1.5680
Policy update: Rollout 75/100, Loss: 2.9913
Policy update: Rollout 80/100, Loss: -0.2596
Policy update: Rollout 85/100, Loss: -3.8220
Policy update: Rollout 90/100, Loss: -0.7466
Policy update: Rollout 95/100, Loss: -9.7098
Policy update: Rollout 100/100, Loss: 3.9190
Evaluation: Average reward = 175.60

Iteration 60/70
Collected 100 trajectories. Average reward: 192.88
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Policy update: Rollout 5/100, Loss: 2.1966
Policy update: Rollout 10/100, Loss: 4.7819
Policy update: Rollout 15/100, Loss: -0.9340
Policy update: Rollout 20/100, Loss: -5.7222
Policy update: Rollout 25/100, Loss: -4.1101
Policy update: Rollout 30/100, Loss: -5.8731
Policy update: Rollout 35/100, Loss: -0.7298
Policy update: Rollout 40/100, Loss: 1.8662
Policy update: Rollout 45/100, Loss: 2.0151
Policy update: Rollout 50/100, Loss: -9.0407
Policy update: Rollout 55/100, Loss: -7.5002
Policy update: Rollout 60/100, Loss: -1.1033
Policy update: Rollout 65/100, Loss: -7.9019
Policy update: Rollout 70/100, Loss: -3.6734
Policy update: Rollout 75/100, Loss: -1.7273
Policy update: Rollout 80/100, Loss: -0.1822
Policy update: Rollout 85/100, Loss: 1.9736
Policy update: Rollout 90/100, Loss: -11.9450
Policy update: Rollout 95/100, Loss: -3.2243
Policy update: Rollout 100/100, Loss: 3.5912
Evaluation: Average reward = 185.80

Iteration 61/70
Collected 100 trajectories. Average reward: 191.02
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 0.70
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -2.6483
Policy update: Rollout 10/100, Loss: -1.0990
Policy update: Rollout 15/100, Loss: 1.6175
Policy update: Rollout 20/100, Loss: -5.2583
Policy update: Rollout 25/100, Loss: -2.1859
Policy update: Rollout 30/100, Loss: -7.3618
Policy update: Rollout 35/100, Loss: -7.4869
Policy update: Rollout 40/100, Loss: -5.5942
Policy update: Rollout 45/100, Loss: -6.5986
Policy update: Rollout 50/100, Loss: 0.2692
Policy update: Rollout 55/100, Loss: -4.5565
Policy update: Rollout 60/100, Loss: -3.9242
Policy update: Rollout 65/100, Loss: -4.0545
Policy update: Rollout 70/100, Loss: 2.1531
Policy update: Rollout 75/100, Loss: -1.3547
Policy update: Rollout 80/100, Loss: -1.4418
Policy update: Rollout 85/100, Loss: -0.8847
Policy update: Rollout 90/100, Loss: -7.2210
Policy update: Rollout 95/100, Loss: 2.1077
Policy update: Rollout 100/100, Loss: -4.2711
Evaluation: Average reward = 194.40

Iteration 62/70
Collected 100 trajectories. Average reward: 195.30
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Policy update: Rollout 5/100, Loss: -5.6918
Policy update: Rollout 10/100, Loss: -0.9420
Policy update: Rollout 15/100, Loss: -4.6609
Policy update: Rollout 20/100, Loss: -4.3205
Policy update: Rollout 25/100, Loss: -12.3666
Policy update: Rollout 30/100, Loss: -0.9977
Policy update: Rollout 35/100, Loss: 2.4358
Policy update: Rollout 40/100, Loss: 0.9245
Policy update: Rollout 45/100, Loss: -12.8561
Policy update: Rollout 50/100, Loss: 3.0006
Policy update: Rollout 55/100, Loss: 1.8365
Policy update: Rollout 60/100, Loss: 1.7213
Policy update: Rollout 65/100, Loss: -0.8716
Policy update: Rollout 70/100, Loss: 4.3329
Policy update: Rollout 75/100, Loss: -0.3627
Policy update: Rollout 80/100, Loss: -4.6073
Policy update: Rollout 85/100, Loss: 3.4634
Policy update: Rollout 90/100, Loss: -7.5184
Policy update: Rollout 95/100, Loss: -6.6735
Policy update: Rollout 100/100, Loss: -1.8889
Evaluation: Average reward = 154.40

Iteration 63/70
Collected 100 trajectories. Average reward: 187.09
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 0.53
Policy update: Rollout 5/100, Loss: 0.8171
Policy update: Rollout 10/100, Loss: 8.7278
Policy update: Rollout 15/100, Loss: -9.0555
Policy update: Rollout 20/100, Loss: 3.1429
Policy update: Rollout 25/100, Loss: -2.8371
Policy update: Rollout 30/100, Loss: -5.0502
Policy update: Rollout 35/100, Loss: -0.1522
Policy update: Rollout 40/100, Loss: -3.2035
Policy update: Rollout 45/100, Loss: -13.5396
Policy update: Rollout 50/100, Loss: 1.7340
Policy update: Rollout 55/100, Loss: -5.8350
Policy update: Rollout 60/100, Loss: 1.7423
Policy update: Rollout 65/100, Loss: 2.4679
Policy update: Rollout 70/100, Loss: 0.6665
Policy update: Rollout 75/100, Loss: -8.8923
Policy update: Rollout 80/100, Loss: -0.7486
Policy update: Rollout 85/100, Loss: -7.7691
Policy update: Rollout 90/100, Loss: 4.8110
Policy update: Rollout 95/100, Loss: 3.7217
Policy update: Rollout 100/100, Loss: -4.4968
Evaluation: Average reward = 187.20

Iteration 64/70
Collected 100 trajectories. Average reward: 196.45
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Policy update: Rollout 5/100, Loss: -4.7791
Policy update: Rollout 10/100, Loss: -13.4600
Policy update: Rollout 15/100, Loss: -7.9176
Policy update: Rollout 20/100, Loss: -1.6963
Policy update: Rollout 25/100, Loss: -4.2564
Policy update: Rollout 30/100, Loss: -6.3535
Policy update: Rollout 35/100, Loss: -0.0303
Policy update: Rollout 40/100, Loss: -6.0696
Policy update: Rollout 45/100, Loss: -2.7428
Policy update: Rollout 50/100, Loss: -2.6660
Policy update: Rollout 55/100, Loss: -2.3073
Policy update: Rollout 60/100, Loss: 1.2885
Policy update: Rollout 65/100, Loss: 2.6572
Policy update: Rollout 70/100, Loss: 3.2069
Policy update: Rollout 75/100, Loss: -6.7196
Policy update: Rollout 80/100, Loss: -13.2040
Policy update: Rollout 85/100, Loss: 1.6538
Policy update: Rollout 90/100, Loss: -1.8439
Policy update: Rollout 95/100, Loss: -4.0524
Policy update: Rollout 100/100, Loss: -1.9146
Evaluation: Average reward = 200.00

Iteration 65/70
Collected 100 trajectories. Average reward: 192.14
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 0.73
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -0.8684
Policy update: Rollout 10/100, Loss: 1.9447
Policy update: Rollout 15/100, Loss: -14.0119
Policy update: Rollout 20/100, Loss: -8.5631
Policy update: Rollout 25/100, Loss: -0.5533
Policy update: Rollout 30/100, Loss: -0.7640
Policy update: Rollout 35/100, Loss: 2.0393
Policy update: Rollout 40/100, Loss: -5.6391
Policy update: Rollout 45/100, Loss: -1.4529
Policy update: Rollout 50/100, Loss: 6.8043
Policy update: Rollout 55/100, Loss: 3.2900
Policy update: Rollout 60/100, Loss: 4.1285
Policy update: Rollout 65/100, Loss: 0.6910
Policy update: Rollout 70/100, Loss: -4.0711
Policy update: Rollout 75/100, Loss: 3.6807
Policy update: Rollout 80/100, Loss: -12.4167
Policy update: Rollout 85/100, Loss: -12.3260
Policy update: Rollout 90/100, Loss: -9.5235
Policy update: Rollout 95/100, Loss: -7.2758
Policy update: Rollout 100/100, Loss: -3.4323
Evaluation: Average reward = 200.00

Iteration 66/70
Collected 100 trajectories. Average reward: 196.58
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Policy update: Rollout 5/100, Loss: -6.5675
Policy update: Rollout 10/100, Loss: 3.1468
Policy update: Rollout 15/100, Loss: -3.9150
Policy update: Rollout 20/100, Loss: -8.4546
Policy update: Rollout 25/100, Loss: -4.5626
Policy update: Rollout 30/100, Loss: -5.2794
Policy update: Rollout 35/100, Loss: -0.6545
Policy update: Rollout 40/100, Loss: -2.7123
Policy update: Rollout 45/100, Loss: -21.0557
Policy update: Rollout 50/100, Loss: -9.3225
Policy update: Rollout 55/100, Loss: 0.8874
Policy update: Rollout 60/100, Loss: -18.2264
Policy update: Rollout 65/100, Loss: -3.6503
Policy update: Rollout 70/100, Loss: -3.2367
Policy update: Rollout 75/100, Loss: 1.7815
Policy update: Rollout 80/100, Loss: 5.0129
Policy update: Rollout 85/100, Loss: -2.5634
Policy update: Rollout 90/100, Loss: -8.9742
Policy update: Rollout 95/100, Loss: -0.5102
Policy update: Rollout 100/100, Loss: -6.3264
Evaluation: Average reward = 200.00

Iteration 67/70
Collected 100 trajectories. Average reward: 198.12
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6906
Reward model training epoch 2/5, Loss: 0.6906
Reward model training epoch 3/5, Loss: 0.6906
Reward model training epoch 4/5, Loss: 0.6906
Reward model training epoch 5/5, Loss: 0.6906
Preference prediction accuracy: 0.57
Policy update: Rollout 5/100, Loss: -6.5970
Policy update: Rollout 10/100, Loss: 3.9740
Policy update: Rollout 15/100, Loss: 4.9828
Policy update: Rollout 20/100, Loss: -2.2414
Policy update: Rollout 25/100, Loss: -1.7288
Policy update: Rollout 30/100, Loss: 1.9174
Policy update: Rollout 35/100, Loss: -1.7462
Policy update: Rollout 40/100, Loss: -9.5145
Policy update: Rollout 45/100, Loss: -5.3689
Policy update: Rollout 50/100, Loss: 3.7968
Policy update: Rollout 55/100, Loss: -0.7172
Policy update: Rollout 60/100, Loss: 1.3505
Policy update: Rollout 65/100, Loss: 5.3618
Policy update: Rollout 70/100, Loss: -7.0269
Policy update: Rollout 75/100, Loss: -1.9869
Policy update: Rollout 80/100, Loss: -0.8539
Policy update: Rollout 85/100, Loss: -1.5657
Policy update: Rollout 90/100, Loss: 4.8125
Policy update: Rollout 95/100, Loss: 0.9647
Policy update: Rollout 100/100, Loss: -12.1428
Evaluation: Average reward = 184.80

Iteration 68/70
Collected 100 trajectories. Average reward: 196.09
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Policy update: Rollout 5/100, Loss: -5.5597
Policy update: Rollout 10/100, Loss: 1.3128
Policy update: Rollout 15/100, Loss: -11.0895
Policy update: Rollout 20/100, Loss: -0.8794
Policy update: Rollout 25/100, Loss: -15.8255
Policy update: Rollout 30/100, Loss: -4.3585
Policy update: Rollout 35/100, Loss: -0.2836
Policy update: Rollout 40/100, Loss: -3.9879
Policy update: Rollout 45/100, Loss: -15.5249
Policy update: Rollout 50/100, Loss: 0.7739
Policy update: Rollout 55/100, Loss: 1.2092
Policy update: Rollout 60/100, Loss: -7.7441
Policy update: Rollout 65/100, Loss: 2.7942
Policy update: Rollout 70/100, Loss: -1.3044
Policy update: Rollout 75/100, Loss: -3.9811
Policy update: Rollout 80/100, Loss: -9.8742
Policy update: Rollout 85/100, Loss: -0.3801
Policy update: Rollout 90/100, Loss: -3.2003
Policy update: Rollout 95/100, Loss: 3.2490
Policy update: Rollout 100/100, Loss: 2.1346
Evaluation: Average reward = 200.00

Iteration 69/70
Collected 100 trajectories. Average reward: 197.87
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Preference prediction accuracy: 0.73
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -0.3780
Policy update: Rollout 10/100, Loss: -4.0265
Policy update: Rollout 15/100, Loss: -7.9246
Policy update: Rollout 20/100, Loss: -6.2120
Policy update: Rollout 25/100, Loss: -16.0355
Policy update: Rollout 30/100, Loss: -5.9689
Policy update: Rollout 35/100, Loss: -4.2343
Policy update: Rollout 40/100, Loss: 9.6923
Policy update: Rollout 45/100, Loss: -8.1625
Policy update: Rollout 50/100, Loss: -8.0968
Policy update: Rollout 55/100, Loss: -2.2631
Policy update: Rollout 60/100, Loss: -1.9210
Policy update: Rollout 65/100, Loss: -3.1905
Policy update: Rollout 70/100, Loss: -7.3270
Policy update: Rollout 75/100, Loss: 2.9745
Policy update: Rollout 80/100, Loss: -11.6703
Policy update: Rollout 85/100, Loss: -0.8398
Policy update: Rollout 90/100, Loss: -3.8646
Policy update: Rollout 95/100, Loss: 2.0040
Policy update: Rollout 100/100, Loss: -8.0205
Evaluation: Average reward = 200.00

Iteration 70/70
Collected 100 trajectories. Average reward: 194.42
Selected 300 most uncertain pairs out of 900 candidates
Created 300 preference pairs
Reward model training epoch 1/5, Loss: 0.6931
Reward model training epoch 2/5, Loss: 0.6931
Reward model training epoch 3/5, Loss: 0.6931
Reward model training epoch 4/5, Loss: 0.6931
Reward model training epoch 5/5, Loss: 0.6931
Collected 10 trajectories for reward correlation
Policy update: Rollout 5/100, Loss: -3.5383
Policy update: Rollout 10/100, Loss: -5.6506
Policy update: Rollout 15/100, Loss: -8.1394
Policy update: Rollout 20/100, Loss: -14.6184
Policy update: Rollout 25/100, Loss: -7.8175
Policy update: Rollout 30/100, Loss: -17.2408
Policy update: Rollout 35/100, Loss: 0.1034
Policy update: Rollout 40/100, Loss: 0.4800
Policy update: Rollout 45/100, Loss: -11.1933
Policy update: Rollout 50/100, Loss: 1.7421
Policy update: Rollout 55/100, Loss: -2.9705
Policy update: Rollout 60/100, Loss: 0.3550
Policy update: Rollout 65/100, Loss: -3.8385
Policy update: Rollout 70/100, Loss: 3.9341
Policy update: Rollout 75/100, Loss: -2.3892
Policy update: Rollout 80/100, Loss: 5.3941
Policy update: Rollout 85/100, Loss: -5.8456
Policy update: Rollout 90/100, Loss: 2.5091
Policy update: Rollout 95/100, Loss: -2.0155
Policy update: Rollout 100/100, Loss: 7.6683
Evaluation: Average reward = 185.60
Episode 1: Reward = 200.0
Episode 2: Reward = 200.0
Episode 3: Reward = 200.0
Episode 4: Reward = 200.0
Episode 5: Reward = 200.0
Episode 6: Reward = 200.0
Episode 7: Reward = 200.0
Episode 8: Reward = 200.0
Episode 9: Reward = 200.0
Episode 10: Reward = 200.0
Episode 11: Reward = 200.0
Episode 12: Reward = 187.0
Episode 13: Reward = 200.0
Episode 14: Reward = 200.0
Episode 15: Reward = 200.0
Episode 16: Reward = 200.0
Episode 17: Reward = 200.0
Episode 18: Reward = 200.0
Episode 19: Reward = 200.0
Episode 20: Reward = 200.0
Average reward over 20 episodes: 199.35
Final evaluation: Average reward = 199.35
Training complete. Models saved.
Wrote profile results to online_rlhf.py.lprof
Timer unit: 1e-06 s

Total time: 44.174 s
File: online_rlhf.py
Function: compute_trajectory_reward at line 25

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    25                                               @profile
    26                                               def compute_trajectory_reward(self, trajectory):
    27                                                   # OPTIMIZATION: Convert directly to tensor without going through numpy
    28    424580   22150968.0     52.2     50.1          states = torch.tensor([step[0] for step in trajectory], 
    29    212290      43684.0      0.2      0.1                              dtype=torch.float32, device=self.device)
    30    424580    3163506.0      7.5      7.2          actions = torch.tensor([step[1] for step in trajectory], 
    31    212290      43265.0      0.2      0.1                              dtype=torch.long, device=self.device)
    32                                                   
    33    212290   18307280.0     86.2     41.4          rewards = self.reward_model(states, actions)
    34    212290     465271.0      2.2      1.1          return rewards.sum()

Total time: 85.8126 s
File: online_rlhf.py
Function: select_action at line 36

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    36                                               @profile
    37                                               def select_action(self, state):
    38   2303069    8642772.0      3.8     10.1          state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)
    39   2303069   44433723.0     19.3     51.8          probs = self.policy(state_tensor)
    40   2303069   32304594.0     14.0     37.6          action = torch.multinomial(probs, 1).item()
    41   2303069     431535.0      0.2      0.5          return action, probs

Total time: 45.9784 s
File: online_rlhf.py
Function: collect_trajectories at line 43

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    43                                               @profile
    44                                               def collect_trajectories(self, num_trajectories=100, max_steps=200):
    45       194         35.0      0.2      0.0          trajectories = []
    46      8434       1552.0      0.2      0.0          for i in range(num_trajectories):
    47      8240      70466.0      8.6      0.2              state, _ = self.env.reset()
    48      8240       1797.0      0.2      0.0              trajectory = []
    49      8240        806.0      0.1      0.0              total_reward = 0
    50                                           
    51   1012488     139531.0      0.1      0.3              for step in range(max_steps):
    52   1009345   36852449.0     36.5     80.2                  action, _ = self.select_action(state)
    53   1009345    8321225.0      8.2     18.1                  next_state, reward, done, truncated, info = self.env.step(action)
    54   1009345     229820.0      0.2      0.5                  trajectory.append((state, action, reward, next_state, done))
    55   1009345     154292.0      0.2      0.3                  total_reward += reward
    56   1009345      98930.0      0.1      0.2                  state = next_state
    57                                           
    58   1009345     104839.0      0.1      0.2                  if done or truncated:
    59      5097        675.0      0.1      0.0                      break
    60                                           
    61      8240       1999.0      0.2      0.0              trajectories.append((trajectory, total_reward))
    62       194         28.0      0.1      0.0          return trajectories

Total time: 175.752 s
File: online_rlhf.py
Function: create_preference_pairs at line 65

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
    65                                               @profile
    66                                               def create_preference_pairs(self, trajectories, num_pairs=20, candidtate_multiplier=3):
    67        70          9.0      0.1      0.0          candidate_pairs = []
    68        70         15.0      0.2      0.0          num_candidates = num_pairs * candidtate_multiplier
    69                                                   
    70     63070      10958.0      0.2      0.0          for _ in range(num_candidates):
    71     63000     338386.0      5.4      0.2              idx1, idx2 = random.sample(range(len(trajectories)), 2)
    72     63000      17789.0      0.3      0.0              traj1, reward1 = trajectories[idx1]
    73     63000      13521.0      0.2      0.0              traj2, reward2 = trajectories[idx2]
    74                                           
    75     63000       9000.0      0.1      0.0              if reward1 > reward2:
    76     22485       2770.0      0.1      0.0                  preferred = 1
    77     40515       5470.0      0.1      0.0              elif reward2 > reward1:
    78     22455       2697.0      0.1      0.0                  preferred = 0
    79                                                       else: 
    80     18060      34592.0      1.9      0.0                  preferred = random.randint(0, 1)
    81                                           
    82                                                       
    83     63000  175283136.0   2782.3     99.7              uncertainty = self.estimate_uncertainty(traj1, traj2)
    84     63000      20852.0      0.3      0.0              candidate_pairs.append((traj1, traj2, preferred, uncertainty))
    85                                                       
    86                                                   # Sort by uncertainty and select the top num_pairs
    87        70      10108.0    144.4      0.0          candidate_pairs.sort(key=lambda x: x[3], reverse=True)
    88        70       2286.0     32.7      0.0          selected_pairs = [(traj1, traj2, preferred) for traj1, traj2, preferred, _ in candidate_pairs[:num_pairs]]
    89                                                   
    90        70        412.0      5.9      0.0          print(f"Selected {num_pairs} most uncertain pairs out of {num_candidates} candidates")
    91        70         13.0      0.2      0.0          return selected_pairs

Total time: 65.6749 s
File: online_rlhf.py
Function: train_reward_model at line 104

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   104                                               @profile
   105                                               def train_reward_model(self, preference_pairs, epochs=5, batch_size=32):
   106        70         21.0      0.3      0.0          epoch_losses = []
   107                                                   
   108       420        106.0      0.3      0.0          for epoch in range(epochs):
   109       350         46.0      0.1      0.0              total_loss = 0
   110       350       7058.0     20.2      0.0              indices = np.random.permutation(len(preference_pairs))
   111                                                       
   112      3850       1955.0      0.5      0.0              for i in range(0, len(preference_pairs), batch_size):
   113      3500       6515.0      1.9      0.0                  batch_indices = indices[i:i+batch_size]
   114      3500      24202.0      6.9      0.0                  batch = [preference_pairs[idx] for idx in batch_indices]
   115                                                           
   116      3500     123564.0     35.3      0.2                  self.reward_optimizer.zero_grad()
   117      3500     695643.0    198.8      1.1                  batch_loss = 0
   118                                                           
   119                                                           # Process all pairs in batch before backward pass
   120    108500      34882.0      0.3      0.1                  for traj1, traj2, preferred in batch:
   121    105000   22233596.0    211.7     33.9                      r1 = self.compute_trajectory_reward(traj1)
   122    105000   22537516.0    214.6     34.3                      r2 = self.compute_trajectory_reward(traj2)
   123    105000     239595.0      2.3      0.4                      diff = r1 - r2
   124    105000     194265.0      1.9      0.3                      prob = torch.sigmoid(diff)
   125    105000     304608.0      2.9      0.5                      target = torch.tensor(float(preferred), dtype=torch.float32, device=self.device)
   126    105000    1393839.0     13.3      2.1                      loss = self.reward_loss(prob.unsqueeze(0), target.unsqueeze(0))
   127    105000     541709.0      5.2      0.8                      batch_loss += loss / len(batch)
   128                                                           
   129      3500   16521666.0   4720.5     25.2                  batch_loss.backward()
   130      3500     808203.0    230.9      1.2                  self.reward_optimizer.step()
   131      3500       3500.0      1.0      0.0                  total_loss += batch_loss.item() * len(batch)
   132                                                       
   133       350        163.0      0.5      0.0              avg_loss = total_loss / len(preference_pairs)
   134       350        102.0      0.3      0.0              epoch_losses.append(avg_loss)
   135       350       2162.0      6.2      0.0              print(f"Reward model training epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
   136                                                   
   137        70         21.0      0.3      0.0          return epoch_losses[-1]

Total time: 139.629 s
File: online_rlhf.py
Function: update_policy at line 139

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   139                                               @profile
   140                                               def update_policy(self, num_rollouts=10, gamma=0.99):
   141        70         12.0      0.2      0.0          total_loss = 0
   142                                                   
   143      7070       2432.0      0.3      0.0          for i in range(num_rollouts):
   144      7000      84020.0     12.0      0.1              state, _ = self.env.reset()
   145      7000       1215.0      0.2      0.0              done = False
   146      7000     455181.0     65.0      0.3              log_probs = []
   147      7000     158131.0     22.6      0.1              states = []
   148      7000       4351.0      0.6      0.0              actions = []
   149                                           
   150   1289737     210497.0      0.2      0.2              while not done:
   151   1289737   56526074.0     43.8     40.5                  action, probs = self.select_action(state)
   152   1289737    5750256.0      4.5      4.1                  log_prob = torch.log(probs[0, action])
   153   1289737   13144762.0     10.2      9.4                  next_state, _, done, truncated, _ = self.env.step(action)
   154                                                           
   155   1289737     356894.0      0.3      0.3                  log_probs.append(log_prob)
   156   1289737     227442.0      0.2      0.2                  states.append(state)
   157   1289737     231587.0      0.2      0.2                  actions.append(action)
   158   1289737     151601.0      0.1      0.1                  state = next_state
   159                                           
   160   1289737     182081.0      0.1      0.1                  if truncated or done:
   161      6345        891.0      0.1      0.0                      break
   162                                           
   163      7000     998304.0    142.6      0.7              states_tensor = torch.tensor(states, dtype=torch.float32, device=self.device)
   164      7000      77062.0     11.0      0.1              actions_tensor = torch.tensor(actions, dtype=torch.long, device=self.device)
   165                                           
   166      7000      32351.0      4.6      0.0              with torch.no_grad():
   167      7000     770891.0    110.1      0.6                  rewards = self.reward_model(states_tensor, actions_tensor)
   168      7000     577494.0     82.5      0.4              discounted_rewards = self.calculate_discounted_rewards(rewards, gamma)
   169      7000    9681101.0   1383.0      6.9              policy_loss = sum(-log_prob * R for log_prob, R in zip(log_probs, discounted_rewards))
   170      7000     276773.0     39.5      0.2              self.policy_optimizer.zero_grad()
   171      7000   48739866.0   6962.8     34.9              policy_loss.backward()
   172      7000     970100.0    138.6      0.7              self.policy_optimizer.step()
   173                                                       
   174      7000       4298.0      0.6      0.0              loss_value = policy_loss.item()
   175      7000       2036.0      0.3      0.0              total_loss += loss_value
   176                                                           
   177      7000       2710.0      0.4      0.0              if (i + 1) % 5 == 0:
   178      1400       8583.0      6.1      0.0                  print(f"Policy update: Rollout {i+1}/{num_rollouts}, Loss: {loss_value:.4f}")
   179                                                   
   180        70         28.0      0.4      0.0          return total_loss / num_rollouts

Total time: 174.517 s
File: online_rlhf.py
Function: estimate_uncertainty at line 271

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   271                                               @profile
   272                                               def estimate_uncertainty(self, traj1, traj2, n_samples=10):
   273                                                   # Extract states and actions once before the loop
   274     63000     508043.0      8.1      0.3          states1 = [step[0] for step in traj1]
   275     63000     437085.0      6.9      0.3          actions1 = [step[1] for step in traj1]
   276     63000     488104.0      7.7      0.3          states2 = [step[0] for step in traj2]
   277     63000     439520.0      7.0      0.3          actions2 = [step[1] for step in traj2]
   278                                                   
   279                                                   # Convert to tensors once before the loop
   280     63000    1368531.0     21.7      0.8          states1_tensor = torch.tensor(np.array(states1), dtype=torch.float32, device=self.device)
   281     63000     456929.0      7.3      0.3          actions1_tensor = torch.tensor(np.array(actions1), dtype=torch.long, device=self.device)
   282     63000    1252191.0     19.9      0.7          states2_tensor = torch.tensor(np.array(states2), dtype=torch.float32, device=self.device)
   283     63000     438657.0      7.0      0.3          actions2_tensor = torch.tensor(np.array(actions2), dtype=torch.long, device=self.device)
   284                                                   
   285                                                   # OPTIMIZATION: Batch the MC dropout samples instead of doing them in a loop
   286     63000    1389896.0     22.1      0.8          self.reward_model.enable_dropout()
   287                                                   
   288     63000     156819.0      2.5      0.1          with torch.no_grad():
   289                                                       # Stack the same inputs n_samples times
   290     63000     336945.0      5.3      0.2              states1_repeated = states1_tensor.repeat(n_samples, 1, 1)
   291     63000     204161.0      3.2      0.1              actions1_repeated = actions1_tensor.repeat(n_samples, 1)
   292     63000     270737.0      4.3      0.2              states2_repeated = states2_tensor.repeat(n_samples, 1, 1)
   293     63000     196963.0      3.1      0.1              actions2_repeated = actions2_tensor.repeat(n_samples, 1)
   294                                                       
   295                                                       # Calculate rewards for all samples at once
   296    252000   82321593.0    326.7     47.2              r1_samples = self.reward_model(states1_repeated.view(-1, states1_tensor.size(-1)), 
   297    189000      70280.0      0.4      0.0                                          actions1_repeated.view(-1)).view(n_samples, -1).sum(dim=1)
   298    252000   81998302.0    325.4     47.0              r2_samples = self.reward_model(states2_repeated.view(-1, states2_tensor.size(-1)), 
   299    189000      74695.0      0.4      0.0                                          actions2_repeated.view(-1)).view(n_samples, -1).sum(dim=1)
   300                                                       
   301                                                       # Calculate differences and probabilities
   302     63000     176434.0      2.8      0.1              diff_samples = r1_samples - r2_samples
   303                                                   
   304     63000      71444.0      1.1      0.0          probs = torch.sigmoid(diff_samples)
   305     63000     404248.0      6.4      0.2          uncertainty = probs.var().item()
   306     63000    1445254.0     22.9      0.8          self.reward_model.eval()
   307     63000      10040.0      0.2      0.0          return uncertainty

Total time: 441.503 s
File: online_rlhf.py
Function: train at line 310

Line #      Hits         Time  Per Hit   % Time  Line Contents
==============================================================
   310                                               @profile
   311                                               def train(self, iterations=20, trajectories_per_iter=200, preference_pairs=500, 
   312                                                         reward_epochs=50, policy_rollouts=20, use_uncertainty=True):
   313         1          0.0      0.0      0.0          iteration_numbers = []
   314         1          0.0      0.0      0.0          eval_rewards = []
   315         1          0.0      0.0      0.0          reward_model_losses = []
   316         1          0.0      0.0      0.0          accuracy_history = []
   317                                           
   318         1          0.0      0.0      0.0          true_rewards_data = []
   319         1          0.0      0.0      0.0          predicted_rewards_data = []
   320                                           
   321         1          0.0      0.0      0.0          uncertainty_comparison_data = []
   322                                                   
   323        71         22.0      0.3      0.0          for iter in range(iterations):
   324        70         71.0      1.0      0.0              print(f"\nIteration {iter+1}/{iterations}")
   325                                                       
   326                                                       # Collect trajectories
   327        70   38462760.0 549468.0      8.7              trajectories = self.collect_trajectories(num_trajectories=trajectories_per_iter)
   328        70       2367.0     33.8      0.0              avg_reward = np.mean([r for _, r in trajectories])
   329        70        467.0      6.7      0.0              print(f"Collected {len(trajectories)} trajectories. Average reward: {avg_reward:.2f}")
   330                                           
   331                                                       # if iter == 0 or iter % 10 == 0 or iter == iterations - 1:
   332                                                       #     random_pairs, uncertain_pairs = self.compare_sampling_strategies(
   333                                                       #         trajectories, num_pairs=min(50, preference_pairs))
   334                                                           
   335                                                       #     # Store data for later analysis
   336                                                       #     random_avg = np.mean([self.estimate_uncertainty(t1, t2) for t1, t2, _ in random_pairs])
   337                                                       #     active_avg = np.mean([self.estimate_uncertainty(t1, t2) for t1, t2, _ in uncertain_pairs])
   338                                                       #     uncertainty_comparison_data.append((iter+1, random_avg, active_avg))
   339                                           
   340        70          9.0      0.1      0.0              if use_uncertainty:
   341        70  175955221.0    3e+06     39.9                  pairs = self.create_preference_pairs(trajectories, num_pairs=preference_pairs)
   342                                                       else:
   343                                                           pairs = []
   344                                                           for _ in range(preference_pairs):
   345                                                               idx1, idx2 = random.sample(range(len(trajectories)), 2)
   346                                                               traj1, reward1 = trajectories[idx1]
   347                                                               traj2, reward2 = trajectories[idx2]
   348                                                               if reward1 > reward2:
   349                                                                   preferred = 1
   350                                                               elif reward2 > reward1:
   351                                                                   preferred = 0
   352                                                               else: 
   353                                                                   preferred = random.randint(0, 1)
   354                                                           pairs.append((traj1, traj2, preferred))
   355                                           
   356        70        279.0      4.0      0.0              print(f"Created {len(pairs)} preference pairs")
   357                                           
   358        70   65871255.0 941017.9     14.9              reward_loss = self.train_reward_model(pairs, epochs=reward_epochs)
   359        70         27.0      0.4      0.0              reward_model_losses.append(reward_loss)
   360                                                       
   361                                                       # Calculate preference prediction accuracy
   362        70         37.0      0.5      0.0              if iter % 2 == 0:  # Calculate every other iteration to save time
   363        35    5317967.0 151941.9      1.2                  accuracy = self.evaluate_preference_accuracy(num_test_pairs=30)
   364        35         11.0      0.3      0.0                  accuracy_history.append(accuracy)
   365                                           
   366        70         38.0      0.5      0.0              if iter % 4 == 0 or iter == iterations - 1:
   367        19    1196148.0  62955.2      0.3                  correlation_trajectories = self.collect_trajectories(num_trajectories=10)
   368                                                           
   369       209        318.0      1.5      0.0                  for trajectory, true_reward in correlation_trajectories:
   370                                                               # Calculate predicted reward
   371       190        464.0      2.4      0.0                      with torch.no_grad():
   372       190      33978.0    178.8      0.0                          predicted_reward = self.compute_trajectory_reward(trajectory).item()
   373                                                               
   374       190         61.0      0.3      0.0                      true_rewards_data.append(true_reward)
   375       190         36.0      0.2      0.0                      predicted_rewards_data.append(predicted_reward)
   376                                                           
   377        19         51.0      2.7      0.0                  print(f"Collected {len(correlation_trajectories)} trajectories for reward correlation")
   378                                           
   379                                                       # Update policy
   380        70  141736332.0    2e+06     32.1              self.update_policy(num_rollouts=policy_rollouts)
   381                                           
   382                                                       # Evaluate current policy
   383        70    2443919.0  34913.1      0.6              eval_trajectories = self.collect_trajectories(num_trajectories=5)
   384        70       2300.0     32.9      0.0              eval_reward = np.mean([r for _, r in eval_trajectories])
   385        70        263.0      3.8      0.0              print(f"Evaluation: Average reward = {eval_reward:.2f}")
   386                                                       
   387                                                       # Store metrics
   388        70         26.0      0.4      0.0              iteration_numbers.append(iter + 1)
   389        70         14.0      0.2      0.0              eval_rewards.append(eval_reward)
   390                                                   
   391         1          4.0      4.0      0.0          self.env.close()
   392                                           
   393                                                       # After training completes, visualize uncertainty evolution
   394         1          0.0      0.0      0.0          if uncertainty_comparison_data:
   395                                                       iterations_compared, random_avgs, active_avgs = zip(*uncertainty_comparison_data)
   396                                                       
   397                                                       plt.figure(figsize=(10, 6))
   398                                                       plt.plot(iterations_compared, random_avgs, 'r-o', label='Random Sampling')
   399                                                       plt.plot(iterations_compared, active_avgs, 'b-o', label='Uncertainty Sampling')
   400                                                       plt.title('Evolution of Uncertainty During Training')
   401                                                       plt.xlabel('Iteration')
   402                                                       plt.ylabel('Average Uncertainty')
   403                                                       plt.legend()
   404                                                       plt.grid(True)
   405                                                       plt.tight_layout()
   406                                                       plt.savefig('uncertainty_evolution.png')
   407                                                       plt.show()
   408                                                   
   409                                                   # Create reward evolution visualization
   410         1    3994147.0    4e+06      0.9          self.visualize_reward_evolution(iteration_numbers, eval_rewards)
   411                                                   
   412                                                   # Create reward model loss visualization
   413         1    2133429.0    2e+06      0.5          self.visualize_reward_model_loss(iteration_numbers, reward_model_losses)
   414                                           
   415                                                   # Create reward correlation visualization
   416         1    2050432.0    2e+06      0.5          self.visualize_reward_correlation(true_rewards_data, predicted_rewards_data)
   417                                                   
   418                                                   # Plot preference prediction accuracy
   419         1      16441.0  16441.0      0.0          plt.figure(figsize=(10, 6))
   420         1      10738.0  10738.0      0.0          plt.plot(range(1, len(accuracy_history)+1), accuracy_history, 'g-o', linewidth=2)
   421         1        142.0    142.0      0.0          plt.title('Preference Prediction Accuracy')
   422         1         41.0     41.0      0.0          plt.xlabel('Evaluation')
   423         1         34.0     34.0      0.0          plt.ylabel('Accuracy')
   424         1        274.0    274.0      0.0          plt.grid(True)
   425         1        444.0    444.0      0.0          plt.ylim(0, 1)
   426         1      25842.0  25842.0      0.0          plt.tight_layout()
   427         1      33164.0  33164.0      0.0          plt.savefig('preference_accuracy.png')
   428         1    2213704.0    2e+06      0.5          plt.show()
   429                                                   
   430         1          7.0      7.0      0.0          return self.policy, self.reward_model

